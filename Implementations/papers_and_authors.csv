paper_id,title,year,authors,abstract,venue,venue_id
arXiv:1706.03762,Attention is All you Need,2017,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, I. Polosukhin","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",Neural Information Processing Systems,d9720b90-d60b-48bc-9df8-87a30b9a60dd
arXiv:1409.0473,Neural Machine Translation by Jointly Learning to Align and Translate,2014,"Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
arXiv:1312.5602,Playing Atari with Deep Reinforcement Learning,2013,"Volodymyr Mnih, K. Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A. Riedmiller","We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
032274e57f7d8b456bd255fe76b909b2c1d7458e,A Deep Reinforced Model for Abstractive Summarization,2017,"Romain Paulus, Caiming Xiong, R. Socher","Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit ""exposure bias"" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
43428880d75b3a14257c3ee9bda054e61eb869c0,Convolutional Sequence to Sequence Learning,2017,"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann Dauphin","The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",International Conference on Machine Learning,fc0a208c-acb7-47dc-a0d4-af8190e21d29
4550a4c714920ef57d19878e31c9ebae37b049b2,Massive Exploration of Neural Machine Translation Architectures,2017,"D. Britz, Anna Goldie, Minh-Thang Luong, Quoc V. Le","Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.",Conference on Empirical Methods in Natural Language Processing,41bf9ed3-85b3-4c90-b015-150e31690253
204a4a70428f3938d2c538a4d74c7ae0416306d8,A Structured Self-attentive Sentence Embedding,2017,"Zhouhan Lin, Minwei Feng, C. D. Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio","This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
79baf48bd560060549998d7b61751286de062e2a,Factorization tricks for LSTM networks,2017,"Oleksii Kuchaiev, Boris Ginsburg","We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is ""matrix factorization by design"" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
13d9323a8716131911bfda048a40e2cde1a76a46,Structured Attention Networks,2017,"Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush","Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
510e26733aaff585d65701b9f1be7ca9d5afc586,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,2017,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, J. Dean","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
98445f4172659ec5e891e031d8202c102135c644,Neural Machine Translation in Linear Time,2016,"Nal Kalchbrenner, L. Espeholt, K. Simonyan, Aäron van den Oord, Alex Graves, K. Kavukcuoglu","We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
735d547fc75e0772d2a78c46a1cc5fad7da1474c,Can Active Memory Replace Attention?,2016,"Lukasz Kaiser, Samy Bengio","Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.",Neural Information Processing Systems,d9720b90-d60b-48bc-9df8-87a30b9a60dd
5b6ec746d309b165f9f9def873a2375b6fb40f3d,Xception: Deep Learning with Depthwise Separable Convolutions,2016,François Chollet,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",Computer Vision and Pattern Recognition,768b87bb-8a18-4d9c-a161-4d483c776bcf
c6850869aa5e78a107c378d2e8bfa39633158c0c,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,2016,"Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, Wei Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, O. Vinyals, G. Corrado, Macduff Hughes, J. Dean","Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
63e39cdf1ad884da6bc69096bb3413b5b1100559,Using the Output Embedding to Improve Language Models,2016,"Ofir Press, Lior Wolf","We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.",Conference of the European Chapter of the Association for Computational Linguistics,8de18c35-6785-4e54-99f2-21ee961302c6
97fb4e3d45bb098e27e0071448b6152217bd35a5,Layer Normalization,2016,"Jimmy Ba, J. Kiros, Geoffrey E. Hinton","Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
b60abe57bc195616063be10638c6437358c81d1e,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,2016,"Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, W. Xu","Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT’14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task.",Transactions of the Association for Computational Linguistics,e0dbf116-86aa-418d-859f-a49952d7e44a
2f2d8f8072e5cc9b296fad551f65f183bdbff7aa,Exploring the Limits of Language Modeling,2016,"R. Józefowicz, O. Vinyals, M. Schuster, Noam Shazeer, Yonghui Wu","In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
7345843e87c81e24e42264859b214d26042f8d51,Recurrent Neural Network Grammars,2016,"Chris Dyer, A. Kuncoro, Miguel Ballesteros, Noah A. Smith","We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.",North American Chapter of the Association for Computational Linguistics,01103732-3808-4930-b8e4-7e9e68d5c68d
13fe71da009484f240c46f14d9330e932f8de210,Long Short-Term Memory-Networks for Machine Reading,2016,"Jianpeng Cheng, Li Dong, Mirella Lapata","In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.",Conference on Empirical Methods in Natural Language Processing,41bf9ed3-85b3-4c90-b015-150e31690253
2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep Residual Learning for Image Recognition,2015,"Kaiming He, X. Zhang, Shaoqing Ren, Jian Sun","Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",Computer Vision and Pattern Recognition,768b87bb-8a18-4d9c-a161-4d483c776bcf
23ffaa0fe06eae05817f527a47ac3291077f9e58,Rethinking the Inception Architecture for Computer Vision,2015,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Z. Wojna","Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",Computer Vision and Pattern Recognition,768b87bb-8a18-4d9c-a161-4d483c776bcf
5e4eb58d5b47ac1c73f4cf189497170e75ae6237,Neural GPUs Learn Algorithms,2015,"Lukasz Kaiser, I. Sutskever","Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded.  We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run.  An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers.  To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
d76c07211479e233f7c6a6f32d5346c983c5598f,Multi-task Sequence to Sequence Learning,2015,"Minh-Thang Luong, Quoc V. Le, I. Sutskever, O. Vinyals, Lukasz Kaiser","Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
1518039b5001f1836565215eb047526b3ac7f462,Neural Machine Translation of Rare Words with Subword Units,2015,"Rico Sennrich, B. Haddow, Alexandra Birch","Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",Annual Meeting of the Association for Computational Linguistics,1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44
93499a7c7f699b6630a86fad964536f9423bb6d0,Effective Approaches to Attention-based Neural Machine Translation,2015,"Thang Luong, Hieu Pham, Christopher D. Manning","An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",Conference on Empirical Methods in Natural Language Processing,41bf9ed3-85b3-4c90-b015-150e31690253
4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e,End-To-End Memory Networks,2015,"Sainbayar Sukhbaatar, Arthur Szlam, J. Weston, R. Fergus",,Neural Information Processing Systems,d9720b90-d60b-48bc-9df8-87a30b9a60dd
47570e7f63e296f224a0e7f9a0d08b0de3cbaf40,Grammar as a Foreign Language,2014,"O. Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, I. Sutskever, Geoffrey E. Hinton","Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.",Neural Information Processing Systems,d9720b90-d60b-48bc-9df8-87a30b9a60dd
a6cb366736791bcccc5c8639de5a8f9636bf87e8,Adam: A Method for Stochastic Optimization,2014,"Diederik P. Kingma, Jimmy Ba","We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
ac3ee98020251797c2b401e1389461df88e52e62,Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling,2014,"Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, Yoshua Bengio","In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
cea967b59209c6be22829699f05b8b1ac4dc092d,Sequence to Sequence Learning with Neural Networks,2014,"I. Sutskever, O. Vinyals, Quoc V. Le","Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",Neural Information Processing Systems,d9720b90-d60b-48bc-9df8-87a30b9a60dd
fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,2014,"Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
0b544dfe355a5070b60986319a3f51fb45d1348e,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,2014,"Kyunghyun Cho, B. V. Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio","In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",Conference on Empirical Methods in Natural Language Processing,41bf9ed3-85b3-4c90-b015-150e31690253
6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17,Generating Sequences With Recurrent Neural Networks,2013,Alex Graves,"This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",arXiv.org,1901e811-ee72-4b20-8f7e-de08cd395a10
174bbdb96252454cbb40a9c4e53335996235a008,Fast and Accurate Shift-Reduce Constituent Parsing,2013,"Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, Jingbo Zhu",,Annual Meeting of the Association for Computational Linguistics,1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44
5bfd8d40bc071fffaf93685a46974b122ee4239d,Self-Training PCFG Grammars with Latent Annotations Across Languages,2009,"Zhongqiang Huang, M. Harper","We investigate the effectiveness of self-training PCFG grammars with latent annotations (PCFG-LA) for parsing languages with different amounts of labeled training data. Compared to Charniak's lexicalized parser, the PCFG-LA parser was more effectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and benefited more from self-training. We show for the first time that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. Our approach achieves state-of-the-art parsing accuracies for a single parser on both English (91.5%) and Chinese (85.2%).",Conference on Empirical Methods in Natural Language Processing,41bf9ed3-85b3-4c90-b015-150e31690253
f52de7242e574b70410ca6fb70b79c811919fc00,"Learning Accurate, Compact, and Interpretable Tree Annotation",2006,"Slav Petrov, Leon Barrett, R. Thibaux, D. Klein","We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",Annual Meeting of the Association for Computational Linguistics,1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44
78a9513e70f596077179101f6cb6eadc51602039,Effective Self-Training for Parsing,2006,"David McClosky, Eugene Charniak, Mark Johnson","We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.",North American Chapter of the Association for Computational Linguistics,01103732-3808-4930-b8e4-7e9e68d5c68d
2e9d221c206e9503ceb452302d68d10e293f2a10,Long Short-Term Memory,1997,"Sepp Hochreiter, J. Schmidhuber",,Neural Computation,69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3
0b44fcbeea9415d400c5f5789d6b892b6f98daff,Building a Large Annotated Corpus of English: The Penn Treebank,1993,"Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz",,International Conference on Computational Logic,30a8645d-22d4-42e2-b3f6-304bf4ce3a02
34f25a8704614163c4095b3ee2fc969b60de4698,Dropout: a simple way to prevent neural networks from overfitting,2014,"Nitish Srivastava, Geoffrey E. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov",,Journal of machine learning research,c22e7c36-3bfa-43e1-bb7b-edccdea2a780
2e5f2b57f4c476dd69dc22ccdf547e48f40a994c,Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies,2001,"Sepp Hochreiter, Yoshua Bengio",,Unknown,
d44d6f735257376f69d253cf48a2ee4d42597ae8,A sample-efficient transfer learning framework for industrial remaining useful life prediction leveraging large language models,2026,"Yan Chen, Cheng Liu",,Reliability Engineering & System Safety,432120e1-fd7c-4da0-8c59-eb4c1723bcbd
713bbd661434d22fa8c76cbb5b624e22323cbf17,Multi-level semantics-aware and multi-granularity knowledge-infused model for emotional support conversation,2026,"Xin Zheng, Yajun Du, Xiaolin Qin",,Information Fusion,06afdd0b-0d85-413f-af8a-c3045c12c561
d9009b7085414b7b02ebf3cb0a3d6463bc1135a1,Prediction of remaining useful life and reliability study of aero-engines based on adaptive attention dual-path networks,2026,"Junfa Li, Youchao Sun, Hao Liu, Yulong Li, Hao Wang",,Reliability Engineering & System Safety,432120e1-fd7c-4da0-8c59-eb4c1723bcbd
9dd7e0f27bd03f9a45da208e9b5d4b0ddaa12b3d,"Multimodal recommender systems: A survey of representation, modeling, and optimization",2026,"Lin Pan, Zhiqiang Pan, Fei Cai, Honghui Chen",,Information Fusion,06afdd0b-0d85-413f-af8a-c3045c12c561
f14a3df8fff5984135c559b4a18121e1829f69d6,EMSA-Net: Efficient multi-attention boosting lightweight multi-scale CNN for bone age assessment,2026,"Dongyang Wang, Yinjie Wang, Xuyang Wu, Fangnan Lin",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
9065c01db1ddd48c7e36e757ef82ed727e208325,TD2-LTS: Transformer based group-channel interaction for medical image segmentation,2026,"Mingge Xia, Jinlin Ma",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
fbf243de61cd054c95abae59fe95b0e02c95750b,Whole slide image redundancy reduction for efficient pathological diagnosis,2026,"Shijie Li, Zhineng Chen, Feng-Jung Chen, Xieping Gao",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
d424519b8a689d118384d271514dc3ffe1d20076,DCM-Net: A novel dual-branch CNN-Mamba cross-layer feature fusion network for medical image segmentation,2026,"Yanling Liu, Hongmin Deng, Jinghao Fu",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
c927285b278522b1b761a967d5b575a6948efbb6,OSFTL-PL: Online source-free transfer learning with pseudo-labels for privacy-preserving EEG-based BCI,2026,"Siwei Liu, Keyan Chen, Jia Zhang, Jinyi Long, Hanrui Wu",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
4c7550c2169f0f653a0ba00bfbca15aa40b54ef8,"Demography-Aware Personalized Federated Learning for fair, private, and efficient clinical risk prediction",2026,"Puyang Zhao, Zhiyi Yue, Nan Mi, Huiyun Zhang",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
614e1e8df210e975369e6a2d6cdb906e950b99e3,WTP-MILA-UNet: Mamba-Inspired linear attention with wavelet and pointwise convolution for liver MRI segmentation in Budd-Chiari Syndrome,2026,"Haitao Ge, Dongqi Han, Yuan Lin, Weikang Li, Shengqiang Zhou, Hui Wang, Yu-Le Wang, Yifeng Pan, Peng Xu, Jie Zhao",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
a7491693e33ce277150a6edf345f9f0f9ba77dae,ScaleProto-DETR: An automatic detection model for pediatric bronchoalveolar lavage fluid cells,2026,"Weiwei Jiang, Xiuxian Wang, Xianpeng Liu, Jie Chen, Huiling Wu, Hujun Wu, Fang Jin, Xiaofen Tao, Lei Wu",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
713e251a42dd01ee631c0f1aea31703bc1d8381b,SACE-Net: Scale-adaptive and context-enriched network for medical image classification,2026,"Jieying Tu, Jianqiang Du, Jianfeng Xu, Di Gai",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
f096610c6cc0e7e8642b881f0859e1ce8a56c771,Integrating text and medical images for segmentation using interpretable graph neural network,2026,"Shurong Chai, Rahul Kumar Jain, Shaocong Mo, Jiaqing Liu, Shiyu Teng, T. Tateyama, Lanfen Lin, Yen-Wei Chen",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
6e431715ee0705c4e04eec3d0afe9cff5f1c6589,Multimodal emotion recognition based on temporal-spatio bidirectional dilated causal convolution multi-head attention network,2026,"Jingjie Yan, Peiyuan Li, Guangkun Shi, Xiaoyang Zhou, Ying Liu",,Information Sciences,e46002a1-d7a6-4681-aae9-36bc3a6a1f93
53cd451dfc797e8ce061e6ec997df8552e66d58a,Sleep stage classification with multi-resolution temporal modeling and adaptive feature fusion,2026,"Jie Shi, Ziqi Hao, Haiyang Yue, Zihao Chen, Xiaoguang Liu, Xiuling Liu, Jun Li",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
5699336c21012b45709a2c701b0eab0e72b24b7a,A unified deep network for thin- and dense-slice reconstruction: Improving through-plane resolution in clinical MRI,2026,"Rong Zhang, Lingtong Zhang, Jiaen Wang, Xuhe Huangfu, Kecheng Yuan, B. Qiu",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
0591fffaf7a3877bf1a7a45f8b30f804db679f24,Gated transformer network for multivariate security patch identification with mixture-of-experts,2026,"Jiajun Tong, Zhixiao Wang, Xiaobin Rui",,Information and Software Technology,64ae7767-9467-4e70-9e31-e4185f9f16ff
8208e7ba0bb94030e72dd34ba40f838d2be13615,Unmanned delivery aerial vehicles fault detection method based on enhanced spatiotemporal feature fusion framework and multi-head attention mechanism classifier,2026,"Yixin Zhang, Chao Yang, Wenjie Liu, Tianqi Qie, Weida Wang, Hongcai Li",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
86e01b7966c0b62835902bbf4c0037689e818a7b,STKGformer: A spatio-temporal knowledge graph enhanced transformer for passenger flow prediction,2026,"Zhe Ji, Zuoan Hu, Shixi Chang, Wenhao Zhang",,Information Sciences,e46002a1-d7a6-4681-aae9-36bc3a6a1f93
c00905d5f2bbbc6141b4d26bd6c7bc11c170b5e1,A comparative review and benchmark for deep learning based digital image correlation method,2026,"Lianpo Wang, Han Wu",,Optics and lasers in engineering,c6394061-6f8e-452e-8da8-37f0267cb282
998557fb318abd5e5dafa0080e5b23b562e3c2a7,Enhancing heterogeneous information networks through student interactions for knowledge tracing,2026,"Jie Gao, Luchuan Ma, Liqing Qiu",,Information Sciences,e46002a1-d7a6-4681-aae9-36bc3a6a1f93
d4d53ceb942e08cb85fe322d564af12b57cbea12,Engineformer: A digital twin model for predicting aero-engine performance and degradation,2026,"Dasheng Xiao, Aiyang Yu, Shuo Song, Hong Xiao, Zhanxue Wang",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
8ef6cbeb4f374defc6870b1516d4aec6a336b9c5,VAD-GHNet: A Variational Attention Decomposition GH-LSTM Network for signal reconstruction toward TBM performance advance prediction,2026,"Shilong Pang, Linchong Huang, Yixian Wang, Hang Lin, Yu Chen",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
8ed7580c0c6e3117b3aa400c13c5cad92a38b00d,MCPT-CAF-BiGRU: A multi-scale CNN and ProbSparse-Masked Transformer model with cross-attention fusion and BiGRU for hourly wind speed forecasting,2026,"Jinsheng Fan, Guo-An Yu, Mingmeng Zhao, Hucheng Zong",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
5e411461f6d64ae012364c4fa5ae5723c34ebe07,CSVD-AES: Cross-project software vulnerability detection based on active learning with metric fusion,2026,"Zhidan Yuan, Xiang Chen, Juan Zhang, Weiming Zeng",,Information and Software Technology,64ae7767-9467-4e70-9e31-e4185f9f16ff
ef9ed6474184660b033c52455ce4d1f55653559c,Discriminatory order assignment and payment-setting of on-demand food-delivery platforms: A multi-action and multi-agent reinforcement learning framework,2026,"Zijian Zhao, Sen Li",,Transportation Research Part E: Logistics and Transportation Review,
9323391553ec945ca68691db7e5d7bb5f3e8ffa1,Deep reinforcement learning for the vehicle routing problem with route balancing,2026,"Jianhua Xiao, Detian Kong, Zhiguang Cao, Jingyi Zhao",,Transportation Research Part E: Logistics and Transportation Review,
f69292cdec03bc29030bca16e270879c9a5575ed,KVC-Q: A high-fidelity and dynamic KV Cache quantization framework for long-context large language models,2026,"Yusen Wu, Ruiqin Lin, Jiarong Que, Qixiang Zeng, Hongsen Zhang",,Journal of systems architecture,d6974de6-2e30-4295-bdb9-af21f7988e22
126952c145dbeb96118b8a4cd3db8495d63a6c24,Orchestrating optimization passes of machine learning compiler for reducing memory footprints of computation graphs,2026,"Qianwei Yu, Pengbo Nie, Zihan Wang, Chengcheng Wan, Ziyi Lin, He Jiang, Jianjun Zhao, Lei Qiao, Le Chen, Yuting Chen",,Journal of systems architecture,d6974de6-2e30-4295-bdb9-af21f7988e22
9ae0ea9c9cbbda967cc7a3d864638de0d71ec7d3,Reducing contextual noise in review-based recommendation via aspect term extraction and attention modeling,2026,"Heena Lim, Xinzhe Li, Seonu Park, Qinglong Li, Jaekyeong Kim",,Information Sciences,e46002a1-d7a6-4681-aae9-36bc3a6a1f93
8d949ec59560c59c588265777562f7b1b0296373,Edge-to-cloud computing and intelligence for IoT-based Structural Health Monitoring: A comprehensive review,2026,"Shuaiwen Cui, Yugang Fu, Hao Fu, Wei Shen",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
1683dbc72f976128655bfcfb5aacfac6f46666c4,A self-supervised exploratory guided push and grasp approach to enhance robotic manipulation in clutter,2026,"Chiat-Pin Tay, Shijun Yan, Chong Chen, Wei Qi Toh, Miaolong Yuan, Dongkyu Choi",,Robotics and Autonomous Systems,1ee2b231-a72e-4ac7-bef7-72361085f323
50d5e6842b53d61fa2d652287c023d1e88132526,TSDR-SFE: A prediction model for dam crack width based on two-stage decomposition–reconstruction and spatiotemporal feature extraction,2026,"Yixiang Fang, Chongshi Gu, Yangtao Li, Yiming Wang, Taiqi Lu, Lei Shen, Xiao Sun, Mingyuan Zhu, Fuqiang Zhou, Sitao Fu, Hao Gu",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
c570f65db84dc553a72576be0b9bd0930e498892,An inspection path planning approach based on hierarchical reinforcement learning,2026,"Kuanhong Yan, Daxin Liu, Zhenyu Liu, Jianrong Tan",,Journal of manufacturing systems,dc56537e-0e06-44dd-8fe9-50131609f0c2
9c18209390191f34c522525b79c79eabf387bcb4,A frequency-domain decomposition and TCN-GTAF fusion framework for GNSS sequence forecasting,2026,"Congxin Wei, Zidong Quan, Yaxin Su, Haikuo Pang, Lei Wang, S. Shuhidan, Safwan Mahmood Al-Selwi, Mohd Fadzil Hassan",,Information Sciences,e46002a1-d7a6-4681-aae9-36bc3a6a1f93
9f23afcbf53184e56921fb30832e6d0aec038e9d,Hybrid attention-guided RRT*: Learning spatial sampling priors for accelerated path planning,2026,"Asmaa Loulou, Mustafa Unel",,Robotics and Autonomous Systems,1ee2b231-a72e-4ac7-bef7-72361085f323
bb5700bcfbf448f5860b940554e92f4f850eeb6f,VulATMHD: Joint adaptive triplet mining and hybrid distillation for type-aware vulnerability classification,2026,"Xuanye Wang, Lu Lu",,Information and Software Technology,64ae7767-9467-4e70-9e31-e4185f9f16ff
01ab1c3d8450d378a9543836bdf6de5ee58fb1db,"Rainfall intensity estimation at night using deep learning and urban surveillance cameras in Jiangsu Province, China",2026,"Xing Wang, Haiqin Chen, Ang Zhou, Ye Chen",,Journal of Hydrology: Regional Studies,
1e7abaf31762fccb4ea8d5d9ec0b68df3a800315,DiffusionEngine: Diffusion model is scalable data engine for object detection,2026,"Manlin Zhang, Jie Wu, Yuxi Ren, Jiahong Yang, Ming Li, Andy J. Ma",,Pattern Recognition,266f640f-003e-453b-ab76-57e4053252f8
b83c83ac1729596c51329d6411e4aa5bf5bd4cac,What drives attention sinks? A study of massive activations and rotational positional encoding in large vision-language models,2026,"Xiaofeng Zhang, Yuanchao Zhu, Chaochen Gu, Jiawei Cao, Hao Cheng, Kaijie Wu",,Information Processing & Management,37f5b9b7-f828-4ae1-a174-45b538cbd4e4
2739cb10123641f84cbdb7afbc170c408f12953f,Advancing cross-domain emergency classification with multi-view adversarial learning,2026,"Yuhan Xie, Chen Lyu, Zheng Qu, Chunmei Liu",,Information Processing & Management,37f5b9b7-f828-4ae1-a174-45b538cbd4e4
a47cfc05ed7d2328056bb5c74cd85f1633206f4d,Learning generalizable representations with adversarial domain adaptation for snoring-based sleep apnea detection,2026,"Heng Li, Yun Lu, Yukun Qian, Lianyu Zhou, Mingjiang Wang, Hanrong Cheng",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
8c2c54d545da3422e89af275cb8ceb6fa1f029ae,Bridging the safety-specific language model gap: Domain-adaptive pretraining of transformer-based models across several industrial sectors for occupational safety applications,2026,"Abid Ali Khan Danish, Snehamoy Chatterjee",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
2e7764de286a9ddf22db10864b6f5098acb6b792,End-to-end predictions of trabecular bone structural and mechanical properties from resolution adaptive CT imaging,2026,"Peixuan Ge, P. Wong, Shibo Li, Shuwei Zhang, Lihai Zhang, Qiong Wang, Baoliang Zhao, Ying Hu",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
e18258f37c98c447527598fa063b93df8b81c00d,Bidirectional GPT,2026,"Chuanliu Fan, Zicheng Ma, Wenjie Du, Nan Yu, Jun Zhang, Y. Gao, Ziqiang Cao, Guohong Fu",,Information Processing & Management,37f5b9b7-f828-4ae1-a174-45b538cbd4e4
73448c3802f1ad962fc9c8808f7416ada6afa812,From local to global: A hierarchical spatial-frequency Bi-patch Mamba network for EEG-based emotion recognition,2026,"Xinyu Li, Jiajin Huang, Jianhui Chen, Haiyan Zhou",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
2cd099f7a0f6e7a0f5bb83e2300f363bd8881934,A transfer condition-focused model for battery capacity forecast,2026,"Zhuoyi Qian, Zhen Chen, E. Pan",,Reliability Engineering & System Safety,432120e1-fd7c-4da0-8c59-eb4c1723bcbd
7ca3ad189698c766be882744d8477b6841a9e5f3,A continuously coupled attention neural network for MSI status classification in whole slide images,2026,"Zhongqiang Pan, Xiangyu Li, Liangliang Liu",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
01402f3a3ab666e1c96cbea7459893a8b6ca48e2,PVMNet: A navel orange defect detection algorithm based on Mamba structure,2026,"You-Wen Huang, Wei Wang",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
e86e003236e3fd8ccab4108df9d14af3e05a66bd,Multimodal large models for smart agriculture: Frameworks and applications,2026,"Jun Liu, Xuewei Wang",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
ce3972810eac1705d6ca673e53d7a216874f3159,TAC-DASR: Temporal-aggregated Conformer with data augmentation for dysarthric automatic speech recognition,2026,"Yuexia Song, Hao Yuan, Xuanfeng Duan, Qingchuan Tao, Yanmei Yu",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
4376346c4c5bb04aac8bde20526107f8b0b7e6a6,Enhancing person-job fit through multi-temporal career trajectory modeling,2026,"Junmei Feng, Jiarui Yang, Shuchun Li, Qiguang Miao, Yue Xi, Zhaoqiang Xia",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
581ccc0baa9cfbf30fe2f926d18bfd0c11803d58,Federated cross-domain CTR prediction with triple-view contrastive learning and LLM augmentation,2026,"Jiangcheng Qin, Xueyuan Zhang, Baisong Liu, Jiangbo Qian, Yangyang Wang",,Information Processing & Management,37f5b9b7-f828-4ae1-a174-45b538cbd4e4
1e2bbcfc334c5ade7a42c0894b103e0b63207fb3,Enhancing large language model for fake news video detection via cross-modal retrieval,2026,"Linfeng Han, Xiaoming Zhang, Tianbo Wang, Yun Liu, Zhiqiang Dong",,Information Processing & Management,37f5b9b7-f828-4ae1-a174-45b538cbd4e4
c102d198c0b39602bbd55c085527509195273b57,CFWA-Net: Cross-scale fusion and wavelet aggregation network for polyp segmentation,2026,"Zeye Xu, Jian Ji, Falin Wang, Kaili Lu, Teng Sun, Junkun Li",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
a2b682f435f9ff163bc494f8c1c22151e36f0ff6,WPMMA-NIS: wavelet pooling and major-minor axis distance monomerization for nuclei instance segmentation in breast whole slide image,2026,"Xiuxiao Yuan, Xunping Wang, Jingping Yuan, Hongjun Sha",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
b4990816047121c6b31c98342dbb100811c60ec1,An ultra-compact efficient silicon power beam splitter based on large language model by inverse design,2026,"Jiahao Li, Chenxu Wang, Qi Zhou, Junyang Tang, Ming Luo, Lin Wu, Jin Tao, Tianye Huang, Hanbing Li, Xiang Li, Ying Qiu",,Optics Communications,ea02590d-89a5-4608-950c-d9cfff018d19
ae25f11408a3b34e72b041990d9561c8b9ea5c49,Emerging perspectives on embodied intelligence in future smart manufacturing,2026,"Dan Xia, Pengpeng Xu, Guangjie Han, Jinfang Jiang",,Journal of Industrial Information Integration,c192003b-48dd-45d7-b737-5ff93540aca3
6ac744ce2023d7e10fac6742c83601bde1b53dad,Error source identification in metrology digital twin systems using machine learning,2026,"Gengxiang Chen, Charyar-Mehdi Souzani, Nabil Anwer",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
5dbd6a057ebc98cf0493b6e06867b36bb5378699,Uncertainty-Aware Bayesian Time Series framework for probabilistic imputation,2026,"Zi-Xuan Zeng, Yanke Tan, E. Deng, Yiqing Ni, Qi-Lin Zhang, Bin Yang",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
f3efb404c42ca9fdda7380bb915dcd7e4aa07029,Fine-grained segmentation of high-resolution crack images based on transformer and rendering techniques,2026,"Honghu Chu, Weiwei Chen, Lu Deng",,Results in Engineering,9bb0fb7c-8704-4e65-a386-12c41a683412
5cf01b013dc0aaaad5dfb848a8b5d229f2f060b6,RAG-HIDS: A multi-relational graph-based hierarchical intrusion detection system for in-vehicle networks,2026,"Hai Lin, Xi Yu, Zhihong Chen, Yue Cao",,Ad hoc networks,3597523f-277c-4b10-9ac7-b68f01aef995
363f1f22b0ac81920ada1be9228e3f225bf290cd,PTC-diffusion: A partitioned modeling with diffusion-based uncertainty evaluation for blasting silt displacement method reconstruction,2026,"Wei Qin, Yuqi Pan, Xin Ye, Da Yang, Yanke Tan, Haoran Ouyang, Hui Zhong",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
e3993112854f59b3966b165d8775559b213c0874,Clinically informed imputation of medical data using parallel diffusion models,2026,"Shuaixun Wang, Xueer Zhang, Sharon Jewell, Martyn Boutelle",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
06edfafebfeec505d7ed8875ac6049202137474b,A multimodal trajectory prediction framework with multi-scale spatio-temporal interaction and multi-stage adaptive decoding for autonomous vehicles,2026,"Jing Huang, Zhipeng He, Ting Liu, Lin Hu",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
64f14cf3880f036c0e396b491b0538a2792ccec7,Factors influencing language teachers’ judgements and decision-making about the use of generative AI tools: A systematic review,2026,Sima Eedelouei,,Teaching and Teacher Education : An International Journal of Research and Studies,1a784aa3-4e5b-484b-a743-85ae1f00a2c4
a5813577fc5e3a311d841c6c468185fca8c83b51,Efficient crowd anomaly detection using C3D-LSTM networks with enhanced attention mechanisms,2026,"Sarah Altowairqi, Suhuai Luo, Peter Greer, Shan Chen",,Array,a7e4161c-9216-40df-a896-ed51a04aab3a
c7631771d1d5efd0420b28884fd93b4f5bfd5137,Enhancing skin cancer diagnosis using late discrete wavelet transform and new swarm-based optimizers,2026,"Ramin Mousa, Saeed Chamani, Mohammad Morsali, Mohammad Kazzazi, Parsa Hatami, Soroush Sarabi",,Machine Learning with Applications,24440165-229a-4b91-a09d-2d61ce71d94e
e9cf0f895306192affa63c08327a6ad64346f2ac,Clinical evaluation of GenAI adaptive cancer therapy,2026,Y. Derbal,,EngMedicine,
4ff7d903baa2a45c34458eb8b8f53faa0f5dcc1e,Hold me tight: Towards the detection of the flow experience using a commercial video game controller,2026,"Lazaros Michailidis, Jesus Lucas Barcias",,Computers in Human Behavior Reports,503d9ce4-9d19-4f0a-9a58-2ae60822350d
1b3c6bc5e1b3930bfc00f1258f55017c0424611d,Real-time plastic waste segmentation for sustainable resource recovery in construction,2026,"Iman Ranjbar, Yiannis Ventikos, M. Arashpour",,"Resources, Conservation and Recycling",3cb501bc-0ca7-47c4-8fa2-e2dd90f55b67
a055d0b5097f507dfa780bb0640f80a6831a7a12,Deep learning detection and analysis of eddies in the East Greenland marginal ice zone from Sentinel-1 SAR imagery,2026,"Fei Jiang, Xiaofeng Li, Yingjie Liu, Yibin Ren",,Remote Sensing of Environment,2544009c-f3cc-45f2-b79b-aff3d09cfc34
40216d6881af24d51e5e6f96dcaec5d5e4854f5b,SSEDF: A shared-private semantic enhanced dynamic fusion network for multimodal sentiment analysis,2026,"Wanjie Zhang, Yajun Du, Hongyang Wang, Jia Liu, Xianyong Li",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
1a79598e63069d3f218f44bccf22507feabe6a21,"Temporal attention multi-resolution fusion of satellite image time-series, applied to Landsat-8/9 and Sentinel-2: all bands, any time, at best spatial resolution",2026,"Julien Michel, Jordi Inglada",,Remote Sensing of Environment,2544009c-f3cc-45f2-b79b-aff3d09cfc34
38cb74dde545e82ac9ec11e31f6d7c38b19954da,Synergistic importance of memory and spatial neighbourhood effects in modelling net ecosystem productivity,2026,"Jian Liu, Tao Zhou, Jingyu Zeng, Jingzhou Zhang, Xuemei Wu, Yajie Zhang, Qi Zhang, Yancheng Qu, Peixia Liu, Wenjuan Zhang, E. Tan, Yin Yu, Li Cao",,Agricultural and Forest Meteorology,cf468b9d-634c-4326-84d5-61ca2183a367
be294b87116277648ef989b61b44924d603659ad,Contrasting deep learning audio models for direct respiratory insufficiency detection versus blood oxygen saturation estimation,2026,"Marcelo Matheus Gauy, Natália Hitomi Koza, Ricardo Mikio Morita, Gabriel Rocha Stanzione, Arnaldo Cândido Júnior, Larissa C. Berti, A. S. Levin, Ester Cerdeira Sabino, Flaviane Romani Fernandes Svartman, Marcelo Finger",,Intelligent Medicine,01ca0ac9-f373-4ccb-9526-2b2c053b2497
6730b32660762c9a7f4fa30cc62db8514c6ffb07,SWDAKT: Knowledge tracing using sliding window-based dynamic ability perception,2026,"Jinwei Wang, Jiajie Lu, Zilong Su",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
debc6b4266c26a00091de897bfd9b84e9e57c70c,Leveraging distillation token and weaker teacher model to improve DeiT transfer learning capability,2026,"Christopher Gavra Reswara, Gede Putra Kusuma","Recently, distilling knowledge from convolutional neural networks (CNN) has positively impacted the data-efficient image transformer (DeiT) model. Due to the distillation token, this method is capable of boosting DeiT performance and helping DeiT to learn faster. Unfortunately, a distillation procedure with that token has not yet been implemented in the DeiT for transfer learning to the downstream dataset. This study proposes implementing a distillation procedure based on a distillation token for transfer learning. It boosts DeiT performance on downstream datasets. For example, our proposed method improves the DeiT B 16 model performance by 1.75% on the OxfordIIIT-Pets dataset. Furthermore, we present using a weaker model as a teacher of the DeiT. It could reduce the transfer learning process of the teacher model without reducing the DeiT performance too much. For example, DeiT B 16 model performance decreased by only 0.42% on Oxford 102 Flowers with EfficientNet V2S compared to RegNet Y 16GF. In contrast, in several cases, the DeiT B 16 model performance could improve with a weaker teacher model. For example, DeiT B 16 model performance improved by 1.06% on the OxfordIIIT-Pets dataset with EfficientNet V2S compared to RegNet Y 16GF as a teacher model.",International Journal of Informatics and Communication Technology (IJ-ICT),a08ff97f-fa46-47a0-a1fe-0a36f34e187c
cad89d5f71a88f48a2d1ad7b2a54b6cd5a221108,Dynamic Kolmogorov-Arnold networks for time-varying degradation modeling in solid oxide fuel cells,2026,"Mohamadali Tofigh, Daniel J. Smith, A. Hanifi, C. R. Koch, Mahdi Shahbakhti",,Advanced Engineering Informatics,ec497fa8-833a-4d68-873a-539c20989c22
62fd74fb20486163a7d15c733adb868fa89b7764,Gene-guided multimodal data fusion for cancer patient survival analysis,2026,"Mingjie Xu, Li Cai, Zongbao Yang, Ruxin Wang, Hao Zhang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
e358d97ccd530f35e61e6f565fd5f0f55b2f55af,Overcoming radar sparsity and cross-view misalignment: A sparse-to-sparse fusion paradigm for robust 3D object detection,2026,"Rui Wan, Weigang Meng, Tianyun Zhao, Wei Lu",,Digital Signal Processing,
04ccacdbd1d08ff80bafa142d21efe4b84a9ab09,DefMoN: A reproducible framework for theory-grounded synthetic data generation in affective AI,2026,Ryan SangBaek Kim,,Machine Learning with Applications,24440165-229a-4b91-a09d-2d61ce71d94e
438590a7a08c3e43cfa824790ac1d44be2f85018,Fine-grained space object classification with Convolution-Boosted LSTM using light curves: A new method and a large scale dataset,2026,"Weixiao Li, Yu Zhang, Guo Chen, Jihao Yin",,Acta Astronautica,280fcb4f-798f-40fa-a2c9-9e84b322ed87
1c37b835c94031abeda356cf6caa8a4f7995b173,A lightweight semantic decoding network with group to individual transfer learning for EEG-based visual recognition,2026,"Xiaotian Wang, Doudou Zhang, Qimin Xu, Xinyu Cui, Rongkai Zhang, Yiming Jiang, Fu Li, Yang Li, Guangming Shi",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
24a5c71a5f5c422ce1ac666f4b3f105cc25bd187,Data-augmented predictive deep neural network: Enhancing the extrapolation capabilities of non-intrusive surrogate models,2026,"Shuwen Sun, Lihong Feng, Peter Benner",,Computer Methods in Applied Mechanics and Engineering,3bfaa538-a67d-47d7-bfda-6f82748e9a29
e267538c6113d0f97a809dcc3ef05033a9d62b32,Zero-shot compound fault diagnosis with semantic graph embedding and multi-stage fusion,2026,"Fengli Shen, Xin Lu, Hao Wen, Lei Zhou, Rongjiang Tang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
efdb6a72f31ac6227c6255bd40319499610af0f1,APR-BiCA: LiDAR-based absolute pose regression with bidirectional cross attention and gating unit,2026,"Jianlong Dai, Hui Wang, Yuqian Zhao, Zhihua Liu",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
a900ff49283568128e7c2f7cabcb6d86b00305f7,Deep learning-based high dynamic range 3D measurement: The combination of GAN and U-Net,2026,"Jianhua Wang, Shenhua Zhang, Yanxi Yang",,Sensors and Actuators A: Physical,
61d0769852bc2f850add7637afc4c5ec46cd7771,LayupFormer: A deep generative model for composite laminate layup design,2026,"Wenjie Xie, Hasan Caglar, Kailun Deng, David Ayre, Yifan Zhao",,Composites Science And Technology,14c5a241-b8fa-4a38-a4ef-675eb0fb323a
23a97243eccdc4c6548cbb6823e0dc7aec8044a7,Behaviour recognition of tail and ear biting in pigs using AI-based computer vision,2026,"Qinghua Guo, C. Orsini, P. Langenhuizen, Yue Sun, Shoujun Huo, L. E. van der Zande, I. Reimert, J. Bolhuis, P. Bijma, Peter H. N. de With",,Smart Agricultural Technology,049b17f4-e576-40b2-abcb-cc4f5d74068a
82fab2c6b148cea5880a2a31ae7d9244ac244be3,Transformer autoencoder framework for estimating core temperature of lithium-ion battery from pulse discharge dynamics,2026,"Mustehsan Beg, Keith M. Alcock, Vishnu Sam, Sanjay Rakshit, Sambit Paul, Hongnian Yu, Keng Goh",,Applied Thermal Engineering,25414617-4809-40ff-9ed2-ee2af2296d4e
1319f12a4efb9f9532348460176ba307e44d1d1d,Ret-UNet: Enhancing medical image segmentation with self-retention,2026,"Tianjun Guo, Weixin Zhao, Jian Peng",,Array,a7e4161c-9216-40df-a896-ed51a04aab3a
299a018728e9ff1c942c86b753571a593cb3120f,Contrastive learning with multi-scale temporal modeling for robust music performance analysis,2026,"Sibo Zhang, Yang Liu",,Digital Signal Processing,
82ab468314c7e00c6a8deaad709d258fb37b8770,Physical-level privacy-preserving face recognition via optically encoded single-pixel measurements,2026,"Jia-Shuai Mi, Wen-Biao Xu, Yu-Xiao Wei, Yu-Cheng Wang, Hui-Juan Zhang, Yuanjin Yu",,Optics &amp; Laser Technology,
7b5fb91e16f4d1d3fc894b4efe3a41921a7a3217,Single object tracking based on Spatio-Temporal information,2026,"Lixin Wei, Yun Luo, Rongzhe Zhu, Xin Li",,Signal Processing: Image Communication,
4bf442ab26ff744b513284ab17434aae2eecab6d,Enhanced all-sky Aurora image classification using pyramid vision transformer and contrastive background contextual transforms,2026,"Jian Lian, Weiguang Wang",,Applied Soft Computing,b1994124-f1e8-4f96-a165-b6f19a04fe7e
9ec790ba7c7083a07a3a7c3ef3ec19ef0675f690,Individual parameterized transformer-based deep Q-network for efficient industrial 2d exact guillotine cutting scheduling,2026,"Jie-Ying Su, Yi-Feng Hung, Jia-Lin Kang, Shi-Shang Jang",,Applied Soft Computing,b1994124-f1e8-4f96-a165-b6f19a04fe7e
85fa114b899065c706e3f0925bc4b8f3ae28790a,Efficient dual-branch high-resolution transformer design for crack segmentation,2026,"Yongshang Li, Zihan Ma, Ronggui Ma",,Applied Soft Computing,b1994124-f1e8-4f96-a165-b6f19a04fe7e
b727df13c5cc2ebfa74c58782755970389778b79,WCK-SSAM: Working condition knowledge guided sparse self-attention mechanism for operational status perception of multi-modal fused magnesium furnace,2026,"Hao Yan, Yikang Li, Guangyu Zhu, Fuli Wang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
fcc6d195200e640efa45f0317d703813497ac230,Adapting to the environment: An energy‑efficient dual‑synapse spiking model for mobile micro-learning environmental forecasting,2026,"Delei Zhao, Jian Wang, Guo-sheng Zhao",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
59caf050557eb4a9059bbff3320565ceb09b23d3,MSTVQA: A multi-path dynamic perception method for video quality assessment,2026,"Junwei Qi, Yingzhen Wang, Jingpeng Gao, Yichen Wu, Pujiang Liu",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
29cf1471513437b9c287dbacf268eb76ec96ce04,Optimizing task allocation in Mobile Crowdsensing with multiple opportunistic users and participatory UAVs,2026,"Shi Yu, Bing Shi, Xiao Su, Saisai Li, Shuai Li, Xing Tang",,Ad hoc networks,3597523f-277c-4b10-9ac7-b68f01aef995
896eff7f3263d0a949fe38727eed8c1beb47bedb,A multi-branch learning for vessel fuel consumption prediction using multi-scale feature fusion,2026,"Yi Zuo, Xinyu Li, Licheng Zhao, Zhuo Sun",,Transportation Research Part D: Transport and Environment,0a365e41-001f-4f51-945b-92f184da8d96
35099c830ae6e08ecf5222a89bf5aa1965335c8e,Physics-informed neural networks enhanced by cross-attention-based feature fusion and fuzzy logic for solving nonlinear PDEs,2026,"Xia-Ting Jing, Yu-Long Bai, Bo-Ya Hou, Chun-lin Huang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
80aa38953e7ae83d0398ebc375aa6bbed19b886c,Dual-stage network combining transformer and hybrid convolutions for stereo image super-resolution,2026,"Jintao Zeng, Aiwen Jiang, Feiqiang Liu",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
8c9307da420c3dbc0596a165fdb6b96f178d33c2,Dynamic spectral weighting in CausalSelfAttention: Enhancing transformer performance through frequency-based head modulation,2026,"Zhigao Huang, Musheng Chen, Shiyang Zheng",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
3f0e493ddb7a4ae80c4a14eaeee0133a5a9a1e08,Ship trajectory prediction model based on CovTE-BiLSTM with spatiotemporal feature coupling,2026,"Fangbing Xiao, Tianxiang Xu, Xiufeng Zhang, Weifeng Li",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
3e5c83b7dafde2837f4150088fe6ae4fbbf22848,AI vs. Humans: Comparing road user intention recognition performance,2026,"Koen Vellenga, H. Steinhauer, Göran Falkman, Jonas Andersson, Anders Sjögren",,Transportation Research Part F: Traffic Psychology and Behaviour,1508cae2-28b3-4c10-8fb5-d553cf565c58
61ce0c0dd525325186c5c2e0dbfe339a1ed37231,Artificial intelligence assisted calculation of interfacial heat transfer coefficient in low pressure die castings,2026,"Zhongyao Li, Xuelong Wu, Qinghuai Hou, Xiang Li, Wenbo Wang, H. Qiao, Xiaoying Ma, Shuwei Feng, Shihao Wang, Decai Kong, Yisheng Miao, Ruifeng Dou, Yuling Lang, Junsheng Wang",,International Communications in Heat and Mass Transfer,5df234d6-1f97-43c3-bbad-eb97101fc1de
ef172fb1b645537d914e16392af5c29f6b3c23f6,Adaptive fusion of multi-source information for USV trajectory planning via deep reinforcement learning,2026,"Yunfei Wei, Dongxu Liu, Jiajun Ou, Xiao Guo, Baojin Zheng",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
645717e37e6919dd7460ee4325652600f955f99d,Semi-supervised pointwise VIV detection via few-shot and sequential transfer learning,2026,"Sunho Lee, Sunjoong Kim",,Engineering structures,a957d2b3-bedb-47b7-9074-8cf3277cab30
6e67c0f1157d2968f4e99604e269ea8f6f87035f,"Development, pre-training, and fine-tuning of the CoordConv-Unet model for significant wave height retrieval from Himawari-8 satellite imagery",2026,"Wei Zhang, Kunkun Fan, Zhilin Xu, Yanhai Gan, Zhiyi Gao, Renbo Pang",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
4f8f36700f17975b1d715ef21ba2000088981ed8,HMTE: Memory-transformer representation learning for knowledge hypergraph completion,2026,"Yifan Xue, Ying Sun, Wanqiang Cai, Yingyao Ma, L. Senhadji, Huazhong Shu, Jiasong Wu",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
9b5d93f5c173bd19a650ef03e57952369b421bb7,Driving range estimation for electric buses considering battery degradation levels: A self-attention deep learning approach,2026,"Bohan Zhang, Yongxing Wang, Qinwen Zheng, Xiaoyi Li, Qiuyue Sai",,Measurement,
94ddf2fcd9d335a1fd70f3d200f9af97ff1467a2,An interpretable significant wave height forecasting model using a causal AI framework with error correction,2026,"Mingshen Xie, Wenjin Sun, Ying Han, Changming Dong",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
8f6e5f0f4da00a92e736e516477da4c4c67f33b3,Cross-backbone pixel consistency and layer-wise attention fusion for weakly supervised semantic segmentation,2026,"Mengya Liu, Lei Zhu, Jiahui Cheng",,Digital Signal Processing,
dd78774263427a3038a15b88aa125e1fd1fdf06a,Role of SSL models: Finetuning and feature optimization for dysarthric speech recognition and keyword spotting,2026,"Paban Sapkota, H. Kathania, Subham Kutum",,Computers & electrical engineering,4b85084f-20f0-4592-a3c6-2f38acb5c0b3
bf13b0c79ec851d03a864665ebfea06ffe8f030c,Hierarchical spatio-temporal dynamic dependency learning for multivariate operating status prediction of pump station units,2026,"Panpan Qiu, Jianzhuo Yan, Yongchuan Yu, Hongxia Xu",,Computers &amp; Industrial Engineering,
1543fe8250bcaa5ca31b59321d3e6589cd90422a,Similarity theory-driven dimensionless parameter migration framework: Digital twin approach for cross-condition gas turbine performance monitoring,2026,"Chuming Gao, Zilang Huang, Aiyang Yu, Hong Xiao, Zhishu Zhang",,Applied Thermal Engineering,25414617-4809-40ff-9ed2-ee2af2296d4e
103be5f4da740a79f7e82ba667361112f1a8a58e,A transformer-enhanced real-time detection method for young fruits in complex orchard environments,2026,"Jinyan Li, Xiyang Li, Jiyong Hu, Mingxia Liang, Keyi Jiang, Hongfei Yang, Min Tian",,Smart Agricultural Technology,049b17f4-e576-40b2-abcb-cc4f5d74068a
440130c1cfc8526e8cbc9a8a17bfd7cb7c1f1c08,Accelerating language giants: A survey of optimization strategies for LLM inference on hardware platforms,2026,"Young Chan Kim, Seok Kyu Yoon, Sung Soo Han, Chae Won Park, Jun Oh Park, Jun Ha Ko, Hyun Kim",,Journal of systems architecture,d6974de6-2e30-4295-bdb9-af21f7988e22
d041f839ee3d6734bea6721a0fdb516e17bf6911,Diverse methods of incorporating physics into neural networks: A comprehensive review,2026,"M. Rajabi, M. Fahs, Maryam Mansouri Bajgiran, Anis Younes, François Lehmann",,Journal of Hydrology,e857b6f5-698d-4789-aaff-08e5186da0b9
52f98bd231f5c88919b11c8940c1365eb3e514f2,An efficient privacy-preserving transformer inference scheme for cloud-based intelligent decision-making in AIoT,2026,"Mingshun Luo, Haolei He, Wenti Yang, Shuai Yuan, Zhitao Guan",,Journal of systems architecture,d6974de6-2e30-4295-bdb9-af21f7988e22
b0d8aa5bd40cbd47afb4486e9d5b7c149119df68,A knowledge-driven approach for automated fire safety compliance checking in operational buildings,2026,"Dayou Chen, Long Chen, Yi Yang, Qiuchen Lu, Craig Hancock, Russell Lock, Simon Sølvsten",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
441043c77e2b8193e02245021bd4db4f0407289a,HSPF-Net: Hybrid CNN-transformer with serial-parallel fusion for skin lesion segmentation,2026,"Hao Fang, Yu Sun, Shuai Zhang, Xuyang Teng, Xiaohui Li, Xiaodong Yu",,Digital Signal Processing,
ab05854f3cbcd76f109652419677a892b06979ab,Pack Transformer: Transformer-based hybrid model for lithium-ion battery packs,2026,"Habtamu Hailemichael, B. Ayalew, Miriam Figueroa-Santos, Morgan Barron",,Journal of Power Sources,48372f1f-53da-420c-96a1-47cb3f05f92c
d57583bfafc1d7e9ef45f15945644d21bb438d43,Enhanced ECG arrhythmia detection with deep learning and multi-head attention mechanism,2026,"Saoueb Kerdoudi, Larbi Guezouli, Tahar Dilekh",,Computers & electrical engineering,4b85084f-20f0-4592-a3c6-2f38acb5c0b3
8b447bcafe2e9f2b54bf8e0e61daad41bb035bce,Oncology data extraction with large language models from real-world breast cancer electronic health records in Spanish,2026,"Julio Montes-Torres, Francisco J. Moreno-Barea, Leonardo Franco, N. Ribelles, Emilio Alba, J. Jerez",,Machine Learning with Applications,24440165-229a-4b91-a09d-2d61ce71d94e
ae020375b9fbc637fc64e595f7cfd32f1ff87d1a,Underwater visual tracking with a large scale dataset and image enhancement,2026,"B. Alawode, Sajid Javed, F. Dharejo, Mehnaz Ummar, Arif Mahmoud, Fahad Shahbaz Khan, Jiri Matas",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
d74d00d00fcb68d74009ec217b95abada2b90430,Estimating structural displacement for offshore engineering structures using continuous normalizing flow model,2026,"Haixu Zhao, Mingqiang Xu, Shuqing Wang, Haojie Ren",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
2001f5c875dd2d5e134a31c81cfca38ab8d2180f,Dynamic graph-enhanced multi-scale attention network for FW-UAV fault diagnosis under small sample,2026,"Rong Yi, Jianguang Lu, Xianghong Tang",,Applied Soft Computing,b1994124-f1e8-4f96-a165-b6f19a04fe7e
759d2df52d36624c2b3ccec501b44983d4566ef9,Towards explainable visual question answering via cross-modal causal reasoning,2026,"Wei Li, Fuyun Deng, Zhixin Li",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
36711fe396a0609a6659b3c2af2106f8ceac086d,In-context learning enhanced large language model for robust distribution system state estimation,2026,"Yue Li, Gang Cheng, Junbo Zhao, Yitong Liu",,Applied Energy,00a6fb8b-7f32-4ae0-a417-8033f5f369f9
1cf661142fa3b9903c80df654f8b9eaf080705f1,Contrastive coarse-to-fine medical segmentation with prototype guidance and dual-granularity fusion,2026,"Zekai Liu, Muxi Li, Fei Yang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
831bc4ec6c1dc04fd2b8f6c2fcec0849138c22fa,Long-FAS: Cross-domain face anti-spoofing with long text guidance,2026,"Jianwen Zhang, Jianfeng Zhang, Dedong Yang, Rongtao Li, Ziyang Li",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
e56d451984a0445f70a82e796cdbf4a84386747e,A single-to-multiple framework for structural dynamic prediction via graph data representation and mixed strategy deep learning,2026,"Truong-Thang Nguyen, V. Dang, Ngoc-Son Vu, Manh-Hung Ha, Xuan-Dat Pham",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
14c3ea79d6bb22b23a7b749f0f6a3bd74402dca7,One-shot prediction of cross-chemistry Li-ion battery aging trajectories via computational data augmentation and knowledge transfer,2026,"Jie Huang, Yi Yu, Heng Li, Liqun Chen, Peng Wei, Yu Zhou",,Journal of Energy Storage,9c4d2d6f-3684-4677-b2ec-700ebb73e760
c0693bc917f72c0f7917e74ccad5dbd0a6bc5861,Iterative deep learning for cetacean whistle detection in the Strait of Gibraltar,2026,"Alba Márquez-Rodríguez, Neus Pérez-Gimeno, Daniel Benítez-Aragón, Gonzalo M. Arroyo, Andrés De la Cruz",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
19e817c8ae969a310f3aac3d7399b6383654cff6,"Large language models in digital forensics: capabilities, challenges and future directions",2026,"M. Chernyshev, Zubair A. Baig, N. Syed, Robin Doss, Malcolm Shore",,Forensic Science International: Digital Investigation,
c656f4456f1d05ce1d0e4e4d0cad283a0df4bc76,"A comprehensive review of deep learning for solar nowcasting: Enhancing accuracy, reliability, and interpretability",2026,"Zhijin Wang, Senzhen Wu, Yaohui Huang, Ruyu Liu, X. Liu",,Applied Energy,00a6fb8b-7f32-4ae0-a417-8033f5f369f9
211973eab4dbeae8327660e2538a54f370ad2daf,Long-term traffic prediction with inner-outer contrastive representation learning,2026,"Xingli Jia, Yuanhai Qu, Wencong Xu, Zihao Zhao, Haoran Zhu",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
b05637140d729dc4400bb6c59a7b60d3a5088426,A transformer based multi-task deep learning model for urban livability evaluation by fusing remote sensing and textual geospatial data,2026,"Wen Zhou, C. Persello, Dongping Ming, Shaowen Wang, Alfred Stein",,Remote Sensing of Environment,2544009c-f3cc-45f2-b79b-aff3d09cfc34
d50e6190b1ebe5441c291325c1496c18d334ac7e,Enhancing cloud detection across multiple satellite sensors using a combined Swin Transformer and UPerNet deep learning model,2026,"Shulin Pang, Zhanqing Li, Lin Sun, Biao Cao, Zhihui Wang, Xinyuan Xi, Xiaohang Shi, Jing Xu, Jing Wei",,Remote Sensing of Environment,2544009c-f3cc-45f2-b79b-aff3d09cfc34
f38de89d81a8f8ef0a84800ca3b510a742123023,Improved flash drought forecasting and attribution: A spatial-temporal causality-aware deep learning approach,2026,"Sijie Tang, Shuo Wang, Jiping Jiang, Yi Zheng",,Journal of Hydrology,e857b6f5-698d-4789-aaff-08e5186da0b9
999cbd71370545aa9a4d64f98b168cfa181cfbc6,Monocular vision-based 3D ship detection: From image pixels to spatial perception,2026,"Zhe Kang, Feng Ma, He Li, Chen Chen, Zaili Yang, Jin Wang",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
3131ba3a6541a8bc0a753324fa2480f788a893f0,Adaptive group-weighted convolutions: Enhancing equivariant networks via learnable symmetry importance maps,2026,"Pradeep Singh, Kanishk Sharma, Balasubramanian Raman",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
227bb86a64af84dab1bac9ade08aedba75d6d50e,A multi-defect detection framework for sweet potato based on feature fusion and adaptive attention under complex postharvest conditions,2026,"Xinyu Guo, Jian Zhang, Haitao Xiong, Tao Zhang, Ranbing Yang, Xufeng Wang",,Journal of Stored Products Research,7420110e-cb1a-4a64-9aee-a73e7a9e70a1
a942e61cfc827a335dd3ec4ee9ed9400cbf07d83,LiteFormer: A lightweight encoder-only Transformer for efficient financial time series forecasting across global stock indices,2026,"Nguyen Quoc Anh, Tran Truong Tuan Phat, H. X. Son, Thai Thi Thanh Nhan, Nguyen Ngoc Phien, Trung Phan Hoang Tuan, Ngan Nguyen Thi Kim",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
9b3e8ff5c315bb58d260616a0b71d7d757c851b3,Grouped query attention supported with graph-based query clustering,2026,"Ling Zheng, Yujia Zhang, Liang Shen, Chong Miao, Qiang Shen",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
9d4c0bf7e037a128744255c35e3dc6830e544513,Progressive deep feature learning network based on fault-aware deformable convolution and its application in railway defect visual inspection,2026,"Jiahao Shen, Sheng Xiang, Penghua Li, Jie Hou, Yongfu Li, Yi Qin",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
5340b7b62da766548e8f24299f63b4fa4965861c,WeTRaC: Scalable EV charging demand forecasting for heavy-duty fleets,2026,"Alexander Aushev, J. Anttila, Yancho Todorov, Ari Hentunen, Mikko Pihlatie",,Applied Energy,00a6fb8b-7f32-4ae0-a417-8033f5f369f9
c8954bc01b3b1e223c1220b083afb9f262753447,A dual-branch fusion network based on Convolutional Neural Network and Mamba for outdoor fire detection,2026,"Jin He, Wei Wang, Zhijian Gou, Xiaozhao Jin, Gexiang Zhang, Mianxiong Dong",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
887bcbfd22b66d43d781ce840881f449e693fd12,Class debiased teacher for source-free object detection,2026,"You Ma, Shihan Mao, Lin Chai, Hongwei Tong",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
4461ffa0be643d73005cec4011e1fde03e87863c,Neural style transfer architectures for improving generalization in low-resource spoken language identification,2026,"Spandan Dey, Goutam Saha",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
c010538eb5449b185a77fa88303e1a9d993726ff,Can the tone of central bankers’ speeches help shape inflation expectations?: Evidence from Japan,2026,"Dooyeon Cho, Seunghwa Rho",,"Journal of international financial markets, institutions, and money",b636f98c-be89-43a6-bd85-4f5ebfa9ddc9
7133c643c9b189323fa3044e1c63dde4969f4b3c,A multi-faceted analysis of the influence of state energy policies on spatial clustering of wind and solar farms in the U.S.,2026,"Anton Rozhkov, Koyel Das",,Energy Policy,620ad9e7-3a61-4897-8ebe-b93ac9719c99
3b986c712094a4d717f3fd80865e201441b07cbf,A robust and interpretable framework for sports activity recognition based on wearable sensor signals and image representations,2026,"Jian Li, Yibo Fan, Junhui Gong, Junyi Chen, Ruoyu Chen, Wenyan Zhang, Yuliang Zhao",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
ad0e075e391856d5c23369c891f470ec7bf76a2b,Accelerating LLM Inference via Low-Bit Fine-Grained Quantization Algorithm and Bit-Level Accelerator Co-Design,2026,"Xilong Xie, Liang Wang, Limin Xiao, Li Ruan, Tairan Zhang, Jinquan Wang, Yongyue Wang, Xiaojian Liao","Large language models (LLMs) have emerged as one of the most impactful and transformative paradigms in natural language processing. Despite their remarkable success, the intensive computational demands and substantial memory footprint impose a significant barrier to efficient LLM inference. In this paper, we present a comprehensive solution to improve LLM inference performance under ultra-low weight precision, meticulously optimized through algorithm and architecture co-design. To achieve this, we first propose a fine-grained intra-cluster bit allocation method that partitions the weights into small clusters and explicitly considers the distribution of outliers and salient points within each cluster. Then, an intra-cluster protection mechanism is proposed to selectively preserve important weights during quantization, where an extended integer format and group-wise scale factor search are further introduced to mitigate accuracy degradation caused by aggressive bit-width reduction. Furthermore, we develop a memory-aligned encoding scheme to facilitate efficient memory access while enabling flexible identification of mixed-precision representations. Finally, we design a lightweight bit-level accelerator for low-bit LLM inference, offering simplified hardware design and enhanced adaptability through parallel bit-level computation. Compared to existing state-of-the-art quantization algorithms, our algorithm achieves higher model accuracy under ultra-low weight precision. Meanwhile, the proposed bit-level accelerator delivers speedups of 1.59<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\times}$</tex-math><alternatives><mml:math><mml:mo mathvariant=""bold"">×</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq1-3628629.gif""/></alternatives></inline-formula>, 1.38<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\times}$</tex-math><alternatives><mml:math><mml:mo mathvariant=""bold"">×</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq2-3628629.gif""/></alternatives></inline-formula>, and 1.61<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\times}$</tex-math><alternatives><mml:math><mml:mo mathvariant=""bold"">×</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq3-3628629.gif""/></alternatives></inline-formula>, along with energy efficiency improvements of 1.52<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\times}$</tex-math><alternatives><mml:math><mml:mo mathvariant=""bold"">×</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq4-3628629.gif""/></alternatives></inline-formula>, 1.42<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\times}$</tex-math><alternatives><mml:math><mml:mo mathvariant=""bold"">×</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq5-3628629.gif""/></alternatives></inline-formula>, and 1.22<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\times}$</tex-math><alternatives><mml:math><mml:mo mathvariant=""bold"">×</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq6-3628629.gif""/></alternatives></inline-formula> over ANT, OliVe, and FineQ, respectively.",IEEE transactions on computers,42cd70f7-45f1-4f5a-9723-42d222d6c56e
fb912454fceb2d8f1b2861a2689ff1ec82fcb016,A multi-scale representation and multi-level decision learning network for multimodal sentiment analysis,2026,"Xiang Li, Zhiqiang Dong, Xianfu Cheng, Dezhuang Miao, Haijun Zhang, Tianbo Wang, Xiaoming Zhang, Zhoujun Li",,Expert systems with applications,987139ae-a65d-49bb-aaf6-fb764dc40b19
8250db2de5eba71c04807fb5d4861b46faa02952,Enhancing Segment Anything Model with spatial context and textural detail for cardiac MRI segmentation,2026,"Guowei Zheng, Pengbo Bo, Song-Yuan Xu, Linqin Wang, Zhaoyang Cong, Liangliang Liu, Ziyang Zhao, Caiming Zhang",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
5316f3b248bed9ed4ec67917f4d78b7d3fa37dfc,Graph-guided topic modeling and multi-level context-aware framework for automated radiology report generation,2026,"Jinping Liu, Youkun Deng, Jiayu Liu, Dianyi Song, Xiaohui Duan, Jinxiu Li",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
9f79a039bd9cb056b55deb52ea458861772d2d56,A multi-scale transformer framework with consistency and dual-view for depression detection,2026,"Dongfang Han, Guo-Xing Xiang, Jingyun Zhu, Yuanyuan Liao, Jihong Zhu, Askar Hamdulla, Turdi Tohti",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
8238f68629a0dc95692f663cf6824bf87d342dde,AI-assisted voice enabled computing framework for hydrological analysis,2026,"Carlos Erazo Ramirez, Ibrahim Demir",,Environmental Modelling &amp; Software,
1377e04ab461da5cfc2a406fa375341407da57ed,Fusion of deep feature and apparent feature for flotation grade prediction based on apparent information guidance encoder-decoder network,2026,"Yuming Wu, Yongfang Xie, Shiwen Xie, Zongze Wu, Zhaohui Tang",,Information Fusion,06afdd0b-0d85-413f-af8a-c3045c12c561
e6717ddb770a0842eb7c4c6485cf4f85afd67f87,GCIFormer: Global Context Interaction Transformer for volumetric medical image segmentation,2026,"Jiaxu Jiang, Heng-Chao Li, Sen Lei, Nanqing Liu, Kezhou Li, Yongjian Sun",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
f0bcdb7c54344fb93b5e5eaf6ba6b45a7407334f,AlzFusionFormer: Integrating multiple transformers for early Alzheimer 's disease detection from multi-modal data,2026,"Shijin Knox G.U., Anurenjan Purushothaman, P. R, Sreeni K.G.",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
b75145e84bd9202d2fa5d3b46badb5ee72edb6ae,Conditional Generative Adversarial Network-based framework for multi-feature uncertainty modeling in energy systems,2026,Mojtaba Moradi‐Sepahvand,,Electric power systems research,0e93ab3a-8aef-458b-a68d-ab942f5a3306
79de21c9fbf5a4a9d8c3e00783a73afb9d4543c2,Self-screening of noisy labels and hard sample color normalization in histopathology image classification with spatial-texture attention networks,2026,"Hongbo Zhao, Miao Zhang, Ping Jiang, Yi Shen",,Biomedical Signal Processing and Control,1bac31b4-014a-4981-ae41-af2a40acc162
6d055d0a4b996892b159c9bab47fa6c48f0f18c3,RELTO: A reliability-oriented DRL approach with context-aware adaptive reward weighting for multi-objective task offloading in MEC,2026,"Anam Nasir, Xiang He, Teng Wang, Haomai Shi, Zhongjie Wang",,Ad hoc networks,3597523f-277c-4b10-9ac7-b68f01aef995
65cb723cc6ca14d7e5f4fcd83caa57a9bce40af6,"The impact of large language models on medical research and patient care: A systematic review of current trends, challenges, and future innovations",2026,"Sohaib Asif, Fazal Hadi, Qurrat-ul-ain, Yuqi Yan, V. Y. Wang, Dong Xu",,Computer Science Review,3aa92b7f-af7a-4ebd-8925-1152710bfbc7
4ec83e475b83f2cdbcc4883e43681ed161e8e2e8,Boosting unit test generation via structure-aware fine-tuning of pre-trained model,2026,"Shaojian Qiu, Weibiao Chen, Haiyang Liu, Shaosheng Wang, Han Huang",,Information and Software Technology,64ae7767-9467-4e70-9e31-e4185f9f16ff
16f08fa7d07d56fe4cc0562f52caafb0df6ec547,BS-Mamba: A battery-specific Mamba network for robust battery electrode CT image segmentation,2026,"Jianlong Yu, Xuanheng Li, Zijun Zhang, Jingmin Lian, Yi Sun",,Measurement,
65fa44bb36c24a9866b6b4f77834cb45ac611b3c,Can large language models automate the HAZOP process without human intervention?,2026,"Junseo Lee, Sunhwa Park, Sehyeon Oh, Byungchol Ma",,Safety Science,276bfe78-bf8e-4e9c-98a5-371fe883acbb
7a1b77f083b59ad98c046325eb4a4e65b7d48a06,A comprehensive review of recommender systems: Transitioning from theory to practice,2026,"Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, F. Navah, Amirmohammad Kazemeini",,Computer Science Review,3aa92b7f-af7a-4ebd-8925-1152710bfbc7
7bae8611f0909fcdb49f0dcf6845c9a1a33920e5,Using LLMs to enhance code quality: A systematic literature review,2026,"Nawaf Alomari, Moussa Redah, Ahmad Ashraf, Mohammad R. Alshayeb",,Information and Software Technology,64ae7767-9467-4e70-9e31-e4185f9f16ff
399c2232a793f42ae9b5fdd489c160cb0b0354cb,SeismoDual: A dual-domain deep learning framework for robust seismic phase picking,2026,"Kuan-Wei Tang, Kuan-Yu Chen",,Computational Geosciences,b13c1aa5-5899-45c8-9d9b-e8c213fdc308
25004f9da65ddca931fed0de05428863322cb7e4,Dynamic graph transformer for pedestrian potential trajectory prediction under the world perspective,2026,"Yanjie Wen, Zhihong Li, Ping Xu",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
caf5742eeba60e22c938971d506353c9b2c6e48f,Multimodal pain assessment with transformers,2026,"Manuel Benavent-Lledo, Maria Dolores Lopez-Valle, David Ortiz-Pérez, David Mulero-Pérez, José García-Rodríguez, A. Psarrou",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
e7fa57e8087d90f0d64e38f7473b211d273bcb35,Deep multi-view clustering based on sample-level adaptive fusion and clustering structure aggregation,2026,"Zengbiao Yang, Yihua Tan",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
0fd75acd7542fc098c118151ce8b2290a173fed3,"A review of knowledge graph construction using large language models in transportation: Problems, methods, and challenges",2026,"Yancheng Ling, Zhenlin Qin, Zhengliang Ma",,Transportation Research Part C: Emerging Technologies,a8fbd64a-2df2-4275-b527-7935d0142fff
646367c5fcf902e1f3c566ea120f215519ceefd8,Robust measurement of surface tension and contact angle using physics-informed autoencoders,2026,"Cunliang Pan, Chengxin Qiao, Shi Feng, Yuanjia Li, Yonggang Zheng, H. Ye",,Colloids and Surfaces A: Physicochemical and Engineering Aspects,
4469a7f86437e9c7ab6af442ad55445248885a5a,Oil production forecasting using temporal Kolmogorov-Arnold networks,2026,"Mengze Zheng, Tao Zhang, Jing Cao, Zhong Chen, Jian Zou",,Computers and Chemical Engineering,e4866174-bf20-4fac-a8e3-2f02202f3cd8
7887fabb9d71a9ccad290b3fa91e49b368c9cb94,Research on wheel-rail impact monitoring and identifying method based on combined Variational Modal Decomposition-Envelop Spectrum (VMD-ES) and Transformers framework,2026,"Caihao Lan, Rongshan Yang, Wenjin Zhu, Tingying Tang, Xinyu Niu, Zhuohang Li, Peigang Li",,Engineering structures,a957d2b3-bedb-47b7-9074-8cf3277cab30
0ffc65eb11ca6d2d9b1485275b0734cc9005b5a0,A singular learning theory for unified large language model pruning,2026,"Xinyu Wang, Zhaoxin Fan, Faguo Wu, Hongwei Zheng, Yuanze Hu, Gen Li, Zhichao Yang, Qiu Ye, Yifan Sun, Wenjun Wu",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
4ac8959819fce8da3e158df0f03f44c2a7db5dc7,FlowGS: End-to-end correspondence-guided 3D Gaussian Splatting from sparse unposed images,2026,"Kai Hong, Yue Ming, Chuanchen Luo, Jiang Zhou, Haotian Guo",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
f936b731a79626a21a0a0fd24bebc6a69d6710ad,Investigating the effect of attention mechanisms on deep neural network engineering performance and brain-like properties,2026,"Denghui Bai, Tingting Zhang, Qiuzhu Zhang, Junjun Zhang, Zhenlan Jin, Ling Li",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
d304d5aa4734d1f3a3a1f1da92697f66f8edc62a,Multimodal large model driven pseudo labeling for unbiased scene graph generation,2026,"Songju Li, Bin Sun, Shutao Li, Bin Yang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
d7e50ca3957d100456dfd32e4815035b66003f77,A novel approach for software vulnerability detection based on ensemble learning model,2026,"Cho Do Xuan, Dat Bui Quang, Vinh Dang Quang",,Computers & electrical engineering,4b85084f-20f0-4592-a3c6-2f38acb5c0b3
f199c98e167ef27bc6874ee4bad06de20c55b710,"Dynamic, high-resolution poverty measurement in data-scarce environments",2026,"Zhuo Zheng, Timothy Wu, Richard Lee, David Newhouse, Talip Kilic, Marshall Burke, Stefano Ermon, David B. Lobell",,Journal of Development Economics,955388ff-41f5-47d4-84e5-495867b7db1f
5d75d7a849e4d9bebd67d2fd55c56916bb1f34e7,"COVEE: A dataset for cognitive modeling with video, electroencephalography, and eye tracker",2026,"Chuhao Deng, Jayanth Shreekumar, Worawis Sribunma, Maheed H. Ahmed, Nevena Ivanovic, Sabine Brunswicker, James M. Goppert, Mahsa Ghasemi, Inseok Hwang",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
8ad54100e6d1e3ff9e4df3a997f17c849ee48447,Bridging cross-modal sparsity via adaptive pillar propagation and hierarchical multi-granularity feature distillation for 3D object detection,2026,"Rui Wan, Weigang Meng, Tianyun Zhao, Wei Lu",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
76c4ecadfecf8134bf15cb761ed2a9083b7950eb,Mixture of emotions: Global-to-local emotion representation extraction for emotion recognition in conversation,2026,"Wei Zhang, Lifang Wang, Mengting Yang, Ronghan Li",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
b632cdec245c709b9711cf733137f299050e8f10,BAP-PR: Boundary-aware prompting progressive reasoning for few-shot named entity recognition,2026,"Jianchao Yao, Bin Song",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
fddbb853f29894e49b4c833057af93185af1ef1e,A survey of machine learning-driven task scheduling approaches for multiprocessor systems,2026,"Shaima Rahim, Piyoosh Purushothaman Nair, Rajesh Devaraj",,Journal of systems architecture,d6974de6-2e30-4295-bdb9-af21f7988e22
7aa14607880a2314d494065f710517706392737f,Keypoint detection in Tai Chi Chuan Essence via Waist and Limbs Feature Separation,2026,"Yi Yang, Hao Fu, Yunlong Lv, Wei Qian, Tian Wang",,Signal processing. Image communication,5e329303-0790-4991-832f-dd8a737ec6fb
0667675e4f064e66c67f459c86caedea8e65813a,Evaluating Semantic Segmentation–Based Scene Descriptions for Multilane Rural Highway Point Clouds against Ground Truth,2026,"H. Elmasry, Honglin Jiang, Amr M. Sakr, Karim El-Basyouny",,"Journal of Transportation Engineering, Part A: Systems",
ba164797f91e989e2e5617c8b59a19423792adec,Photovoltaic power prediction via APSformer combined with secondary decomposition and optimization algorithms,2026,"Lingfeng Li, H. Liu, Xiongfeng Zhao, Weihao Ren, Yuanyuan Guo, Jiwen Yu",,Computers & electrical engineering,4b85084f-20f0-4592-a3c6-2f38acb5c0b3
b1c8e07b6f110962485730b77e8d596c8dc5d0c8,"Utilizing large-scale foundation models for prognostics and health management in wind turbines: Techniques, challenges, and future directions",2026,"Jiachi Yao, Te Han",,Renewable & Sustainable Energy Reviews,f9c45c04-838e-413a-9737-fc5d1344c88c
bf2ad1d18ec728101136fa4689b20e85c8610a61,Aircraft assembly process planning based on knowledge graph constructed by integrating LLMs and SLMs,2026,"Yunfei Ma, Shuai Zheng, Zheng Yang, Pai Zheng, Jiewu Leng, Jun Hong",,Journal of manufacturing systems,dc56537e-0e06-44dd-8fe9-50131609f0c2
f033a79a1e06f5e55dd62ad303c5a762d6280b84,Accelerating CFD-driven training of transition and turbulence models for turbine flows by one-shot and real-time transformer integration,2026,"Yuan Fang, M. Reissmann, R. Pacciani, Yaomin Zhao, A. Ooi, M. Marconcini, Harshal D. Akolekar, R. Sandberg",,Computers &amp; Fluids,
cd7415e4cefd9e6c3865bd2e25bdaf621378bb57,DeepKGI: Cross-layer graph fusion and interpretable key gene identification for cancer drug response prediction,2026,"Wei Wang, Yuchen Zhu, Ziyuan Wang",,Applied Soft Computing,b1994124-f1e8-4f96-a165-b6f19a04fe7e
31fb2c8e531c87ddc3c88e5480d8431381445ec0,3D pore cloud-informed deep learning framework for multiscale structure–energy absorption correlation in porous sandwich materials,2026,"Jiaxiu Zhang, Wei Zhao, Ran Chen, Liangliang Nan, Wenhao Chen, Mingqiang Wei",,Composite structures,e1226c34-dfef-45ef-86db-48a08dcc800d
61fce02593a89d8b389ee1b077a9694a15e87612,Dynamic response reconstruction of high-rise buildings under hazards using a Pyramid-LSTM-Attention network,2026,"Haoran Pan, Fan Jiang, Jia Guo, Ting Deng, Jiyang Fu",,Engineering structures,a957d2b3-bedb-47b7-9074-8cf3277cab30
81159fffe075deb362344866fe9225b98c57bfc7,Retrieving atmospheric water vapor profiles over Europe combining NOAA-20/CrIS and ground-based GNSS-PWV data,2026,"Jingyuan Zhang, Qinglan Zhang, Shirong Ye, Hong Hu, Yanlan Wu, Peng Jiang",,Atmospheric research,44f17509-3404-4acb-834b-5b5c4d9cf22f
9070c8b8c17b4c4c0338eb0d7ebaabaab7146120,A novel Brownian bridge diffusion-based generative inpainting algorithm for ancient murals,2026,"Yong Chen, Zhixin Fan, Shilong Zhang",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
cf937ca0846062f9fe2f13cee0dec4ed6a967ef6,Analysis of image aesthetics assessment as a positive-unlabelled problem,2026,"Luis Gonzalez-Naharro, M. J. Flores, Jesus Martínez-Gómez, J. M. Puerta",,Signal processing. Image communication,5e329303-0790-4991-832f-dd8a737ec6fb
1858c750d15439086d76695343245b78677006e6,Disentangling response sequences with causal invariance for knowledge tracing,2026,"Shengze Hu, Junjie Hu, Huali Yang, Jing Geng, Xinjia Ou, Zhuoran Xu, Han Wang, Tao Huang",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
220383a5b916b8a7155ee38a1676545c0cf14254,Evaluation of a single interface trap position on the low-frequency noise of junctionless nanowire transistors,2026,"Everton M. Silva, Renan Trevisoli, Rodrigo T. Doria",,Solid-State Electronics,694987df-6149-476e-8eb3-dae91133a523
d41b3995833755ad58ec14f1d0b3373ad91e821f,Recursive mutual-supervised multi-modal mask-variational fusion with domain-anchor alignment for domain-specific Multimodal Neural Machine Translation,2026,"Qingyuan Zhao, Junjun Guo, Zhengtao Yu",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
9cac8359af264c4fff3886046c9c4e722bfdfe87,Intelligent surface roughness detection for deep-hole drilling based on pyramid adaptive transformer with MSST data fusion,2026,"Yue Si, Wang Xu, Zhenchao Yang, Yanghe Liu, Lingfei Kong, Yanfei Zhang",,Measurement,
dbc7bfeb63b5707aba66f1811a6d415b216369d8,Intelligent system modeling using GenAI: A methodology for automated simulation model generation,2026,"Lin Zhang, Yuteng Zhang, Dusit Niyato, Lei Ren, Pengfei Gu, Zhen Chen, Y. Laili, Wentong Cai, A. Bruzzone",,Simulation modelling practice and theory,36cd9dce-74c9-49f2-a42b-b51fc30bcb46
915d7dbb6f0a3ff6d8de3825a6ce94e53cf3adc1,EmoVisioNet: A hybrid network unifying lightweight CNN and attention-based vision model for facial emotion detection,2026,"Gargi Mishra, Supriya Bajpai, D. Saini, Rachna Jain, D. K. Jain, Vitomir Štruc",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
8f5f3aa101cc5d3b3d25226ccaac2726dc660eb2,Transformer-based model predictive control with anode potential awareness for online fast charging optimization of lithium-ion batteries,2026,"Zixian Zhuang, Hongxu Chen, Ying Chen, Weiling Luan, Haofeng Chen, Xiaoyan Ji",,Applied Energy,00a6fb8b-7f32-4ae0-a417-8033f5f369f9
d6116ef0e4b9ec13756ab900b1928d5c0ae0b50f,CitrusNet: A vision transformer-CNN approach for citrus detection from multi-source imagery with multi-scale feature integration,2026,"Haochen Wang, Juan Shi, Hamed Karimian, Fei Wang, Faizan Javed, Bo Liu, Shengnan Shi, Ziwei Li, Yang Tao",,Computers and Electronics in Agriculture,80fdf70e-8520-4bb7-b387-3abebc9970b7
afca1b315de456b27f9aa75a5ea9fe9a52ecd7a5,Explainable spatiotemporal deep learning for subseasonal super-resolution forecasting of Arctic sea ice concentration during the melting season,2026,"Jianxin He, Yuxin Zhao, Shuo Yang, Woping Wu, Jian Wang, Xiong Deng",,Isprs Journal of Photogrammetry and Remote Sensing,227fb221-5e57-477c-b756-e39dd8ffd538
dcfcf6a3bf5d0cfcd629dc30589d6e91a9b7db6e,DAGCCF: Dynamic alternating graphs based cross-modal complementary fusion for conversational emotion recognition,2026,"Hongkun Zhao, Yang Chen, Siyuan Liu, F. Kong, Kang Li",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
8e66772959ee59a43275e680bae6980770d447a7,A semantic framework for drug-target affinity prediction using Mamba and graph convolutional networks for multimodal feature fusion,2026,"Maoyuan Zhou, Jingjie He, Xingyu Liu, Junmin Huang, Jirui Zhang, Jiaxing Li, Xiaorui Huang, Qianjin Guo",,Chemometrics and Intelligent Laboratory Systems,33667b33-8a83-49dd-97b5-88bd7a4513b9
be2d8ab6d9d18669e7ded4d26d727dba281d27b3,Multi-Agent Consultation and Uncertainty-guided Voting for text-to-image person retrieval,2026,"Mingcheng Ni, Zijie Wang, Aichun Zhu, Jingyi Xue, Guannan Dong, Yong Cheng",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
4b62aa25c8b799f475f1ecbaf508492614fc3a2d,CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model,2026,"Junjue Wang, Weihao Xuan, Heli Qi, Zihang Chen, Hongruixuan Chen, Zhuo Zheng, Junshi Xia, Yanfei Zhong, Naoto Yokoya",,Isprs Journal of Photogrammetry and Remote Sensing,227fb221-5e57-477c-b756-e39dd8ffd538
aead706efd000c29cf4369657034070fe9efd800,More realistic and accurate precipitation nowcasting with Conditional Rectified Flow Transformers,2026,"Yunlong Zhou, Chen Zhao, Fanfan Ji, Renlong Hang, Qingshan Liu, Xiaotong Yuan",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
95f001e4c8028d55daac031e3e5546070e95c32f,Novel hybrid machine learning framework for high-fidelity prediction of fly ash-based geopolymer concrete strength,2026,"Amer Hassan, Radhwan A. A. Saleh, H. Al-Sameai, J. de Moura, T. Alomayri, Chunwei Zhang",,Composite structures,e1226c34-dfef-45ef-86db-48a08dcc800d
66f849ff8c1dcd96709e7d7ff4bb1f5c3700096a,Structural deformation monitoring data augmentation based on modified Transformer,2026,"Simin Liu, Weiping Jiang, Ruiqi Liu, Jian Wang, Zhao Li, Zhongtao Ye",,Engineering structures,a957d2b3-bedb-47b7-9074-8cf3277cab30
df6867fb05c8c9516ab2aa1a9b4a28530faaeb53,DeepSeek-R1-assisted design and maintenance of concrete-filled steel tubular structures through automated modeling code generation,2026,"Xiao-Guang Zhou, Chao Hou",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
f925adf9cbd26571be85f8b777c7b70d47045d80,"InSAR and GAT-LSTM integration for dam displacement prediction: Lessons from the Oldman River Dam, Canada",2026,"R. Farhadiani, S. M. Mirzadeh, Ehsan Roshani, Daniel Cusson, Saeid Homayouni",,International Journal of Applied Earth Observation and Geoinformation,4502a19c-ac5e-45a7-9302-2f7b3bcdc0b7
7a791b8c64db7ee1a9e1219d85876d593686ffc2,UrbanMMCL: Urban region representations via multi-modal and multi-graph self-supervised contrastive learning,2026,"Jinzhou Cao, Jiashi Chen, Xiangxu Wang, Weiming Huang, Dongsheng Chen, Tianhong Zhao, Wei Tu, Qingquan Li",,Isprs Journal of Photogrammetry and Remote Sensing,227fb221-5e57-477c-b756-e39dd8ffd538
1a1060d89916a41c1b5736e95dd075ce1756d16a,ME-YOLO: A novel real-time detection network for pavement interlayer distress using ground-penetrating radar,2026,"Senguo Cao, Congde Lu, Xiao Wang, Peng Zhang, Guanglai Jin, Wenlong Cai",,Journal of Applied Geophysics,465ca091-5d6f-46b3-9000-be596a811762
485d2083e338458c4c26e0431055bd1bf4224e3c,A cascaded border-aware network for visual tracking,2026,"Qun Li, Haijun Zhang, Kai Yang, Zhili Zhou",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
46a8190078be0425bd224708e74fc19a8f0e5c84,Revolutionizing breast cancer diagnosis: A computer-aided diagnosis framework with Vision Transformers for multistage histopathology-based classification,2026,"H. M. Balaha, Yousry M Abdulazeem, Mansourah Aljohani, Mohammed El-Abd, Mahmoud Badawy, Mostafa A. Elhosseini",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
377362ddbf77dbf6d8e3061a269fd52d2793fef2,Dual-modal deep learning framework for online penetration prediction in CMT groove welding,2026,"Qi Jiang, Yiming Wang, Yan Kong, Yu Liu, Yifan Liu, Tianhao Wang",,Measurement,
d38fa5e6b85ec170656f3f91f89513a6e0fa488f,Deep multimodal fusion of spectral and visual data for laser welding defect classification,2026,"Qin Zhang, Zhongyou Zhao, Zhenming Wang, Zixuan Wan, Hui-ping Wang, Guangze Li",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
d302362dc7242c724a7a3c05d74697ebec4ab0b4,Attention-assisted neural ordinary differential equation model for the state of health estimation of lithium-ion batteries with high accuracy,2026,"Yiming Li, Man He, Jiapeng Liu",,Journal of Power Sources,48372f1f-53da-420c-96a1-47cb3f05f92c
302a8440899f8180df7719309716f4c03277988d,LSTM-Transformer with decomposition–reconstruction for enhanced solar irradiance forecasting incorporating meteorological variables,2026,"Zizhen Liu, Xuejing Zhao",,Renewable Energy,26190f1b-cdff-4280-96ba-ba2c4dae2a27
4d9fc990085a0b87fe63cd50d5d63259fa219b3a,Multitask learning via task embeddings for glass property prediction with improved sample efficiency,2026,"Gregor Maier, Jan Hamaekers, Benedikt Ziebarth",,Computational materials science,3ec21075-eb50-4e2c-bdcb-ac69565537af
40166582f642921475e908f04edcb3dae3b19756,Generalizable deep sequence models for 4D trajectory prediction of tower crane loads,2026,"M. H. Kazemi, Yuqing Hu, Yi Wu, John I. Messner, Scarlett R. Miller",,Automation in Construction,cbe2e2e0-f4d3-4923-8b48-a02259e5f89c
b983bfee8b8bd83a7de493a0997e516fdd0c4917,A hybrid deep learning model integrating interpretability and cloud model for dam deformation and dynamic risk early warning,2026,"Wencheng Wang, Xiuwen Li, Hao Wu, Qiang Yue",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
1c54cb7c86f42be16cc8f907ff0a82f5ed9d9860,Frequency-aware retinex-wavelet decomposition hybrid network and luminance-guided transformers for low-light image enhancement,2026,"Yi Lu, Jun Huang, Yong Ma, Fan Fan, Kangle Wu, Ge Wang",,Optics &amp; Laser Technology,
7ad2d9049c2c4e8fc822a9abd4ea7c586fd459fa,SoluBat: A bidirectional mamba framework for high-throughput protein solubility prediction in bioprocess optimization,2026,"Linjie Wu, Chen Zhang, Zijian Hu, Xurong Wang, Hongrong Liu, Bin Zhu",,Process Biochemistry,4cac86a9-86aa-4c2c-867b-e54ab8fc51a6
d6ac28cd1e6d2b4751b3f464a0dea6f931d28820,Similarity-guided interaction and mismatched feature emphasis network for text-to-image person re-identification,2026,"Qiyue Ran, Shidu Dong, Kai Yuan, Ting Wen",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
f43590398653a8e22059241edf65d7a1093d7f21,Mitigation of the time-lag issue in LSTM-based significant wave height prediction using long-term (1984–2023) ERA5 dataset,2026,"Brenner D'Costa, Sourav Mandal",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
1ae544225285b30653f14c11e1d05c8cde8b3e03,Multimodal hateful meme detection using knowledge distillation and hierarchical vision transformer framework,2026,"Sajal Aggarwal, Anusha Chhabra, D. Vishwakarma",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
1ac66ad7c9daec98fe912bfd491dc5ee4cfe876e,Clip-based road-marking detection with LLM-guided driving prompts,2026,"Shaofan Sheng, Nicolette Formosa, Yuxiang Feng, Mohammed Quddus",,International Journal of Applied Earth Observation and Geoinformation,4502a19c-ac5e-45a7-9302-2f7b3bcdc0b7
372c313b8c848761cb48e2c43ce4aa0bd0ac8102,"Multistep-ahead prediction of daily water temperature for Poyang Lake, China, using monthly monitoring data",2026,"Gang Li, Xiting Li, Qixin Lin, Zhangjun Liu, Yuqin Gao, Zhen Cui",,Journal of Hydrology: Regional Studies,
e9ee22da29581738b205cec1bc44a4fa68b6eeb9,Efficient transformer-based semantic segmentation of colonic polyps using SegFormer,2026,"Gul E. Arzu, Muhammad Fayaz, Usman Ali, L. Dang, Hyeonjoon Moon",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
11f7e6344eb4dbb07be4b2faa7487e74d910eb6b,A context and preference-aware neural network for auction design,2026,"Yuanyuan Zhang, Jun Zhu, Yonglong Zhang, Mingxuan Liang, Xueqing Li, Xinpeng Lu",,Neurocomputing,df12d289-f447-47d3-8846-75e39de3ab57
6b5987384c54f97a0689bd4b43ad7ff3fbef291e,Multimodal synthetic images generation and aggregation framework for low-cost and high-accuracy nasopharyngeal carcinoma tumor segmentation,2026,"Yongbao Li, Xuanru Zhou, Huali Li, Yinda Du, Ruofei Li, Linghong Zhou, Ting Song",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
804352173bc03ff7c58393c3c6a2a2e173540f70,Joint estimation of the state-of-charge and state-of-energy of lithium-ion batteries under diverse temperatures and discharging conditions using a data-driven model,2026,"Baoliang Chen, Jujin Pan, Yonggui Liu",,Computers & electrical engineering,4b85084f-20f0-4592-a3c6-2f38acb5c0b3
7d627667372874a5d458fea5a45e453df6ba77c9,LE-DLCM: Decoupled learner and course modeling with large language models for enhanced course recommendation,2026,"Jinjin Ma, Zhuo Zhao, Zhiwen Xie, Yi Zhang, Guangyou Zhou",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
6e9f83a5fbb999b411ab9f4d62aa641392309b84,ShapeAfford: Reconstructing 3D Shape With Manipulation Affordance via Geometry-Affordance Synergy,2026,"Gaolin Zhao, Shuo Yang, Jinqiu Fan, Ran Song, Qi Jiang, Lei Han, Wei Zhang","To facilitate robot manipulation tasks, we propose ShapeAfford that reconstructs 3D object models with per-point affordance annotations from multi-view images and textual instructions. By integrating geometric modeling with affordance reasoning into an end-to-end framework, ShapeAfford leverages the mutual constraints and the interplay between object geometry and its affordance to overcome inherent limitations in affordance prediction such as point cloud sparsity and viewpoint sensitivity. We also construct a novel dataset containing 3D models represented as point clouds and meshes, affordance annotations, and natural language instructions, enabling the model to reconstruct 3D objects along with corresponding affordances. Experimental results demonstrate that ShapeAfford outperforms existing approaches on both 3D reconstruction and affordance prediction tasks. Furthermore, we show that our method can be directly used in downstream robot manipulation tasks in real-world environments.",IEEE Robotics and Automation Letters,93c335b7-edf4-45f5-8ddc-7c5835154945
ee1ad4deb3a1b0c1100ec36c639d38f59f130df4,Based on Tensor Core Sparse Kernels Accelerating Deep Neural Networks,2026,"Shijie Lv, Debin Liu, Laurence T. Yang, Xiao-bin Peng, Ruonan Zhao, Zecan Yang, Jun Feng","Large language models in deep learning have numerous parameters, requiring significant storage space and computational resources. Compression techniques are highly effective in addressing these challenges. With the development of hardware like Graphics Processing Unit (GPU), Tensor Core can accelerate low-precision matrix multiplication but achieve acceleration for sparse matrices is challenging. Due to its sparsity, the utilization of Tensor Cores is relatively low. To address this, we propose the based on <bold>T</bold>ensor <bold>C</bold>ore <bold>C</bold>ompressed <bold>S</bold>parse <bold>R</bold>ow format (TC-CSR), which facilitates data loading on GPUs and matrix operations on Tensor Cores. Based on this format, we designed block Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM) kernels, which are common operations in deep learning. Utilizing these designs, we achieved a <inline-formula><tex-math notation=""LaTeX"">$\mathbf {1.41\times }$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=""bold"">1</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=""bold"">41</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""liu-ieq1-3637268.gif""/></alternatives></inline-formula> speedup on Sputnik in scenarios of moderate sparsity and a <inline-formula><tex-math notation=""LaTeX"">$\mathbf {1.38\times }$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=""bold"">1</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=""bold"">38</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""liu-ieq2-3637268.gif""/></alternatives></inline-formula> speedup with large-scale highly sparse matrices. Benefit from our design, we achieved a <inline-formula><tex-math notation=""LaTeX"">$\mathbf {1.75\times }$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=""bold"">1</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=""bold"">75</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""liu-ieq3-3637268.gif""/></alternatives></inline-formula> speedup in end-to-end inference with sparse Transformers and save memory.",IEEE Transactions on Parallel and Distributed Systems,7c9d091e-015e-4e5d-a11f-9bc369fcf414
d94fd96feff501c65cb54b249a04ae689ed1da76,End-to-End Diffusion-Based 3D Object Reconstruction From Robotic Tactile Sensing,2026,"Han Zhang, Xiaohui Zhang, Jun Huang, Zhao Feng, Xiao Xiao","Tactile sensing is essential for robotic perception in scenarios where visual input is limited or unavailable. In this work, we propose a fully tactile-based 3D object reconstruction framework that recovers object shapes exclusively from contact observations. A robotic system comprising a robotic arm, a robotic hand, and tactile sensors captures high-resolution tactile images during multi-contact grasps. These images are processed using TouchVIT to extract depth features, which are mapped to local point clouds via a CNN-based encoder and registered in 3D space using forward kinematics. To reconstruct the global object shape, we introduce TouchDiffusion, a conditional diffusion model that iteratively denoises Gaussian noise guided by the observed tactile geometry. We further develop a simulation environment built on TACTO and TactileGym to support data generation and enable sim-to-real transfer. Our framework enables high-fidelity, vision-free reconstruction of complex 3D objects from sparse tactile inputs. Experiments in both simulated and real-world settings validate the effectiveness and generalizability of our method, outperforming existing tactile-only approaches.",IEEE Robotics and Automation Letters,93c335b7-edf4-45f5-8ddc-7c5835154945
c63250f93cd1ff1c91d8503f3ab20dc9f019a324,SEAGNet: Spatial–Epipolar–Angular–Global feature learning for light field super-resolution,2026,"Xingzheng Wang, Haotian Zhang, Yuhang Lin, Yuanbo Huang, Jiahao Lin",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
8d17b9450c130150f85857256200bf022a096bff,Classification of EEG-fNIRS bimodal brain signals for motor imagery tasks based on wavelet transform and spatio-temporal domain processing.,2025,"Lingyue Zhang, Baojiang Li, Manliang Cao, Cheng Peng, Haiyan Wang",,Neuroscience,949aebe2-ede9-4337-9595-34ede95956cc
2c007e32cc2820f5bf87cf392ae537fb7a316038,Automating concrete production control with computer vision-based aggregate characterisation,2026,"Max Coenen, D. Beyer, Sahar Mohammadi, Max Meyer, Christian Heipke, M. Haist",,Automation in Construction,cbe2e2e0-f4d3-4923-8b48-a02259e5f89c
7b9b2dbbfd3aaac6316e9c790ca0aba169a83049,When Mamba meets CNN: A hybrid architecture for skin lesion segmentation,2026,"Yun Xiao, Caijuan Shi, Jinghao Jia, Ao Cai, Yinan Zhang, Meiwen Zhang",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
58a197a499f7851281393062e4b1a41852f18eac,Short-term photovoltaic forecasting: A parallel TimesNet and AT-Informer-AT method,2026,"Weijie Yu, Yeming Dai, Wenjie Wang, Tao Ren, Mingming Leng",,Renewable Energy,26190f1b-cdff-4280-96ba-ba2c4dae2a27
44c43807c4c84a412b857f2581a33da2fc236a8a,DBSSFNet: Dual branch spatial-spectral-fusion-net for hyperspectral image classification,2026,"Shufang Xu, Ruizhe Liu, Bo Jia, Zhujie Xie, Hongmin Gao",,Optics &amp; Laser Technology,
1c54ee308f5ab1acb278c88bff6359d6e9d54300,MELON: Hierarchical multi-agent trajectory prediction with spatio-temporal uncertainty adaptation,2026,"Yang Cui, Dong Guo, Yi Han",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
cdb39e5381df686a30fa5de2ede676c10f16e877,Improving cell localization with attention-guided diffusion models,2026,"Wei Liu, Chao Xu, Ying Yuan, Wenqi Ye, Huan Xiong, Wenqiao Qiu, Lili Guo, Xinda Li, Xulong Li, Ruxiang Xu",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
931a1e1807c052d14ef4484b4329637f1ccff1d3,PIDiff: Integrating a High-Performance Transformer Into Diffusion Models for Robust and Efficient Imitation Learning,2026,"Yunzhi Huang, Peng Jin, Liang Han, Yingzhao Li, Xudong Hou","Imitation learning is a critical approach for robots to acquire skills by mimicking human behavior. However, traditional imitation learning frameworks often exhibit poor action prediction accuracy and low robustnesswhen handling complex tasks. To tackle these limitations, we propose the PIDiff policy that integrates an enhanced linear-complexity Transformer with a diffusion model and introduces a PointNet-based encoder to efficiently extract visual features, thereby enhancing the policy’s understanding of the environment and improving learning efficiency and robustness. Specifically, we introduce the designed SubLink Block and CrossLink Block, enabling the improved Transformer to serve as the noise predictor in the diffusion strategy while retaining the diffusion model’s ability to capture multi-modal distributions. To comprehensively evaluate the effectiveness of our method, we conducted systematic experiments across multiple tasks on three robotic manipulation benchmark platforms in simulation environments and performed evaluations in various real-world scenarios. The experimental results demonstrate that our approach significantly outperforms baseline methods, achieving higher success rates, faster inference speeds, reduced GPU memory usage, and enhanced robustness. Code and data: https://github.com/jinpeng1018/PIDiff",IEEE Robotics and Automation Letters,93c335b7-edf4-45f5-8ddc-7c5835154945
1a7a3c0f395ad996fc1ecf3860bb0868aab6df4c,SGE-GLoc: Semantic Gaussian Ellipsoid Scene Graphs for Efficient LiDAR Global Localization,2026,"Xi-Meng Cai, Ji Wu, Shu Han, Li Yu, Wen Yang, Huai Yu, Gui-Song Xia","Global localization, encompassing robust place recognition and precise transformation estimation, is crucial for mobile robot navigation when the global navigation satellite system (GNSS) is unavailable. While LiDAR-based approaches are favored for their accuracy in 3D perception and resilience to illumination changes, existing methods often suffer from complex scene representations or computationally intensive processes, hindering scalability in large-scale applications. This letter introduces SGE-GLoc, a novel LiDAR-based global localization framework, which utilizes a compact scene graph representation where individual objects are modeled as Semantic Gaussian Ellipsoids (SGEs), encapsulating both geometric and semantic attributes. For efficient place recognition, we encode SGE features into histograms and employ a Graph Neural Network (GNN) to learn local and global topological information, generating a discriminative two-stage descriptor for scene similarity scoring. For accurate 6-DoF transformation estimation, we first establish initial correspondences based on SGE matching and subsequently refine the transformation by exploiting geometric constraints between SGEs. Extensive evaluations on four public datasets, including KITTI, KITTI-360, Apollo, and HeLiPR, demonstrate that SGE-GLoc achieves high place recognition accuracy and low localization error, while maintaining a highly compact scene graph representation with an average storage of only 1.28 KB per scene. Furthermore, the integration of SGE-GLoc into a SLAM system showcases its practical efficacy in real-world scenarios.",IEEE Robotics and Automation Letters,93c335b7-edf4-45f5-8ddc-7c5835154945
af511b82252ec4e8ce2c57bc8a6b260bd760d369,Deep learning-driven hyperspectral imaging for drought stress detection in dragoon lettuce for space production,2026,"Ha Won Kim, Eun-Sung Park, Moon S. Kim, I. Baek, Blake Costine, LaShelle E. Spencer, Aubrie E. O'Rourke, Hoonsoo Lee, G. Kim, Changyeun Mo, Byoung-Kwan Cho",,Computers and Electronics in Agriculture,80fdf70e-8520-4bb7-b387-3abebc9970b7
2cc665e29d335cd20e3a1a61e22ccd6362f092c1,A hybrid prediction model for deep-water semi-submersible platforms motion based on Transformer,2026,"Ying Li, Qiyuan Zhong, Xuanqi Chen",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
2b77fd1b08477a73e5e95fc7eb928303c3ddcb3c,An improved dynamic attention mechanism-based transformers approach for motor imagery electroencephalogram signal classification,2026,"Uzma Nawaz, Mufti Anees-ur-Rahaman, Zubair Saeed",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
3f964ead988ed12d070ff58bdb10d900a2343879,Quadruplex-depth based multi-view stereo network with wave-shaped depth cells and Epipolar Transformer,2026,"Boyang Song, Jin Xiao, Xiaoguang Hu, Baochang Zhang",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
b5869274b27515069e163181c71e9a7814f96c29,MSBC-Segformer: An automatic segmentation model of clinical target volume and organs at risk in CT images for radiotherapy after breast-conserving surgery,2026,"Yadi Gao, Qian Sun, Lan Ye, Chengliang Li, Peipei Dang, Min Han",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
16ede918e001af4c6a4f6eaef401dd898f4a8243,"Air quality index prediction based on spatio-temporal graph neural networks: An empirical study of Xi’an, China",2026,"Shiyuan Cui, Yifan Yang, Guisheng Liu, Lin Shen",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
ca27beb8e515448d43e1cb1e973c52a990c96d99,CMALDD-PTAF: Cross-modal adversarial learning for deepfake detection by leveraging pre-trained models and cross-attention fusion,2026,"Yuanfan Jin, Yongfang Wang",,Image and Vision Computing,6cc36eeb-d056-42c4-a306-7bcb239cc442
6c736c0a87fbe4851f8790876f36e76b7701e851,Tacit algorithmic collusion in deep reinforcement learning guided price competition among a set of fast-charging electric vehicle hubs,2026,"Diwas Paudel, Tapas K. Das",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
51d10cf860a94c83813171283c124fde0acd407a,Machine learning for resilience analysis: a review of systems under cyberattacks,2026,"Ruoqing Yin, Liz Varga",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
6f982c44aad470038f3ddf86c4679307c26261f8,Real-time monitoring of public affective risk perception during extreme rainstorm events: A deep learning framework with disaster knowledge-enhanced attention mechanism,2026,"Zunxiang Qiu, Xinchun Li, Quanlong Liu, Shuya Yang",,Urban Climate,10ed57b9-b21e-45c0-a05f-5cf34a9974e7
04aa307867ade5a66c1e6bcaf57d66291a9b2df7,SeaTraNet: A local-global feature fusion network for abnormal behavior recognition of single trawler,2026,"Lingkai Kong, Zhuhua Hu, Yaochi Zhao, Wei Wu, Yanming Gu",,Ocean Engineering,771dfc3e-58bf-4d14-9583-372ad5db9c88
fba9bac387faa98c07e31dc97ac77bb90388f027,Spectrum-based anomaly detection using channel state information and attention mechanisms for elderly health monitoring,2026,"Abid Hussain, Xiaoqiang Zhu, Zhang Sihai, Fujiang Lin",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
d5bd5efc135c1e448aa9d5505199a96be488b497,Learning to retrieve containers: A scale-diverse deep reinforcement learning approach for the container retrieval problem,2026,"Woo-Jin Shin, Inguk Choi, Sang-Hyun Cho, Hyun-Jung Kim",,Transportation Research Part C: Emerging Technologies,a8fbd64a-2df2-4275-b527-7935d0142fff
ec81bcfa425ec2630802c38437194bf4936e9ee6,Collection-driven and resolution-aware prompt learning for few-shot remote sensing scene classification,2026,"Yufei Zheng, Shengsheng Wang, Yansheng Gao",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
6107c3146952c0f863df8dd1b8d942440e3504a6,Efficient transformer tracking with multi-axis mixed attention,2026,"Shaofeng Liang, Rui Li, Yingjing Shi, Zhi Deng",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
df9c8b9a30dd7141ae283d13409ba62ac9db3366,A novel time series classification model with Transformer for seamless surrounding rock perception in tunnel boring machine tunneling,2026,"Ke Lei, Zhenliang Zhou, Haokai Li, Zhongsheng Tan, Li Gong",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
65070d7ef57d34fd57a4a0e172fccbcb7c637823,Semantic Text Kernels: A hybrid interpretable framework for deep semantic analysis in textual data,2026,"Nikhil V. Chandran, Anoop V.S., A. S.",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
f6abd9f35ede987a0d3b0982632a3846ca58418c,SSFormer: Self-supervised transformer model for predicting the properties of crystalline materials,2026,"Jie Feng, Tao Sun, Jianmei Yuan",,Computational materials science,3ec21075-eb50-4e2c-bdcb-ac69565537af
c2becff7679c5523407156e532d8620e1d08da59,Human-Like Autonomous Driving Car-Following Behavior Learning Based on Adversarial Training and Uncertainty-Aware DDPG,2026,"Xiaobo Chen, Jinpeng Zang, Fengyin Zhao","Designing effective car-following (CF) models is essential for the development of safe and efficient autonomous driving systems, enabling ego vehicles to adjust their speed based on leading traffic. However, current data-driven CF models, particularly those based on reinforcement learning (RL), often face challenges in engineering reward functions and adequately addressing uncertainties in vehicle motion predictions. In response, this letter introduces a novel human-like CF policy learning approach that synergizes the Generative Adversarial Imitation Learning (GAIL) framework with an enhanced Deep Deterministic Policy Gradient (DDPG) algorithm, utilizing both human driving data and environmental interactions. We first develop a deep ensemble learning model for robust leading vehicle motion prediction with uncertainty quantification. Then, we employ GAIL to learn an implicit reward function, thereby avoiding the manual design of the reward function in DDPG. Moreover, we augment the state with predictive insights for proactive decision-making, and incorporate a dual critic strategy and behavior cloning loss to improve learning stability and mimic human-like driving. Extensive evaluations on multiple naturalistic driving datasets, complemented by thorough ablation studies, demonstrate the effectiveness of our proposed approach in achieving superior accuracy, safety, and comfort in CF tasks. The code of our method will be publicly released.",IEEE Robotics and Automation Letters,93c335b7-edf4-45f5-8ddc-7c5835154945
74c9fe832bb3ac6402168dd8c6df4e8214ca2568,A vision transformer for image-based quantification of tetracycline in water,2026,"Barbaros Durmus, Neslihan Durmus, M. Yegin, Ozge Hanay, M. Kutlu Sengul, Turker Tuncer",,Journal of Molecular Liquids,4c250fd4-0f57-4872-a68c-8ef5988cccfb
fc18ff00bc31480d5a911553a5c3ffc4f9ced689,Adaptive progressive learning for minimizing false alarms in fire detection with limited Re-training data,2026,"Yusun Ahn, Soocheol Kim, Kyuwon Han, K. Lee, HoeSung Yang, Kwangsoo Cho, Jin Hwa Ryu",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
1a0e574bc1bc83c7711810593a7ff75cdc740c13,Ensemble modelling based on transfer learning for enhancing crop mapping through synergistic integration of InSAR coherence and multispectral satellite data,2026,"Niantang Liu, Qunshan Zhao, Richard Williams, Si-Bo Duan, Yingwei Sun, Brian Barrett",,Computers and Electronics in Agriculture,80fdf70e-8520-4bb7-b387-3abebc9970b7
b7e29949c48820a5ecd223e6f4ef632e82765ffa,Detection of selected gear tooth defects using deep neural networks,2026,"Piotr Bojarczak, Edyta Osuch–Słomka, Sebastian Stanisławek, Bartosz Lucedarski","Gear transmissions, due to demanding operating conditions, are particularly susceptible to wear. Surface fatigue wear (pitting) is among the most difficult to predict and, at the same time, the most critical forms of damage to the working surfaces of cylindrical gears, alongside scuffing. Pitting is classified as an unacceptable tribological degradation process leading to catastrophic wear, the propagation of which ultimately results in tooth fracture and consequently the failure of the entire gear system. For this reason, detecting them is an important task. This paper presented the algorithms for classifying pitting defects using deep learning networks. Classification was based on recorded images of defects. Three types of feature extractors (convolutional network, vision transformer, and hybrid model) used in the classifier were tested. A neural network and a vision language model were used as the classifier. The best accuracy of 86% was achieved for the vision transformer together with the neural network. The novel elements are a comparison of three types of feature extractors with respect to their application to the detection (classification) of gear tooth pitting damage and the use of a multimodal language model for the detection of gear tooth pitting damage.",Advances in Science and Technology Research Journal,29551c2b-d7a4-4b8f-88f3-ae2e1e0a768d
71d59278c1944d31a7f8c7a671477d5e67e5e73e,Global-to-Local Deep Interaction and Boundary-Aware Transformer for accurate polyp segmentation,2026,"Xiaojuan Liu, Xuan Li, Zhi Liu, Ke Peng, Shanxiong Chen, Yijue Zhang, Bo Hou",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
ed5e14f439cb82156ac0544522086dd02da5c7e3,A dual-branch multi-scale encoding and fusion model for multivariate time series forecasting,2026,"Jiachao Li, Mengxiao Yin, Junyuan Huang, Tao Luo",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
9e6549eaa4ed956191e89bd7d467110c7aa9bda7,A comprehensive review on generative artificial intelligence models for fault diagnostics and prognostics of rotating machineries,2026,"Tauheed Mian, Pradeep Kundu",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
1563b5a5c5010dad129ab98ae078370b72bb86c7,Multi-task time series forecasting with adaptive graph neural networks based on feature uncertainty,2026,"Xiao Han, Zhisong Pan, Yongjie Huang",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
38fd034101e367dee51b840056b8eb2198287c54,Synthetic learning for primitive-based building model reconstruction from point clouds,2026,"Zhixin Li, Jie Shan",,Isprs Journal of Photogrammetry and Remote Sensing,227fb221-5e57-477c-b756-e39dd8ffd538
75eaa7b868a566c7922525ee14029ee64758fbc0,A novel framework for wave forecasting based on deep learning: A case study in the Gulf of Aden,2026,"Feng Luo, Yifan Qin, Jian Shi, Zhipeng Chen, Yongzhi Wang, Aifeng Tao, Jinhai Zheng, Lin Lv",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
922d3419dae43b953de14839095439d831b8a2eb,Safe reinforcement learning with online filtering for fatigue-predictive human–robot task planning and allocation in production,2026,"Jintao Xue, Xiao Li, Nianmin Zhang",,Journal of manufacturing systems,dc56537e-0e06-44dd-8fe9-50131609f0c2
d5fe26acf240ce0af43b0429824db7e234bc8228,Physics-aware adaptive model predictive control guided by reinforcement learning for enhanced cyber-resilience of building energy systems,2026,"Jiejie Liu, Binghui Wu, Xianyang Meng, Jiangtao Wu, Zhenjun Ma",,Energy Conversion and Management,58906804-ea03-47ac-8a80-03da481b4c85
2ee98bac48ce8d064d29a4d1157bad54a04e44fd,Electoral systems and geographically targeted oversight: Evidence from the Taiwan Legislative Yuan,2026,"Yen-Chieh Liao, Li Tang",,Electoral Studies,f3cebae2-0a3c-487c-8861-036b189f3f18
a1979a8b6fb243af25c9e2ce6be38eab10de5571,A novel diffusion model-based deep learning approach for anomaly detection in computer connector image analysis,2026,"Shu-Kai S. Fan, Li-Chin Lai",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
3fe43588ef2dec8abcfdd4fe46d47c1ed02f6853,Welding heat source parameter optimization using a dynamic hybrid surrogate model,2026,"Xiaobin Li, Wenming Huang, Peifan Jiang, Bahmaninezhad Fatemeh, Xi Vincent Wang, Huajun Cao",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
c9110ef3f642f8e579df5218e8536496b3907392,CAN-BiGRUBERT: Unveiling automotive vehicle intruders by profiling and characterizing anomalies in controller area network,2026,"Shaila Sharmin, Arash Habibi Lashkari, Hafizah Mansor, Andi Fitriah Abdul Kadir",,Computer Networks,f8b7e518-b406-437e-84da-b0948622c1c9
e9278aade21652f55dad43ab0b11ac87bbfb4d34,SpinVision: An end-to-end volleyball spin estimation with Siamese-based deep classification,2026,"Shreya Bansal, Anterpreet Kaur Bedi, Pratibha Kumari, Rishi Soni, N. C. Krishnan, Mukesh Saini",,Computer Vision and Image Understanding,5fbb417b-d7a5-44e6-856d-993f0624ed9c
46042313bc69aa8b1d277ec5bc6e5ad5b65c3736,Adaptive weighting-guided dual-level contrastive learning for multi-view clustering,2026,"Shuangyang Wang, Lihua Zhou, B. Kong",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
b5ba39ebeaa893f8186eeb6aab69d270d4c1cdd7,Estimating regolith thickness in the complex lateritic landscape of the southern zone of rejuvenated drainage in Southwest Western Australia,2026,"Jessie Weller, Sally Thompson, Matthias Leopold, Stanley Mastrantonis",,CATENA,ee632f99-01ca-4f8c-bc76-9361d3d010d0
527ed41bc1f0e7fd7f1698c8f3c52b1b1c630ce5,Weakly-supervised entity matching via LLM-guided data augmentation and knowledge transfer,2026,"Wenzhou Dou, Derong Shen, Xiangmin Zhou, Yue Kou, Tiezheng Nie, Hang Cui, Ge Yu",,Knowledge-Based Systems,12fff95b-d469-49a0-84a5-4fd4696c3f28
fd80cef437fd4522d3cbccf17b2034f5c313308e,Enhanced financial market forecasting using a hybrid deep learning prediction model with encoder-decoder architecture,2026,"Muhammad Zubair, Zhensheng Huang",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
a09dd4190327ceca22963cf3ed873e181c11d42a,Intelligent fault detection in seismic data using U-shaped residual network-temporal-spatial attention mechanism with fourier forward-inverse transform constraints,2026,"Jing Yang, Renqi Lu, Stefan Buske, Kang Wang, Shuo Zhao, Wei Tao, Minghao Cai, Guansheng Liu",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
d55d83153d5695f288a36ae9871fc1628815d655,Efficient prediction of phase-field crystal dynamics via β-variational autoencoders and time-series transformers on coupled physical fields,2026,"Zhixian Lv, Jiahao Huang, Chengyang Yue, Junseok Kim, Yibao Li",,Computers &amp; Mathematics with Applications,
0b5d68d1030dc6e11f0533cfb4de7b4e37819714,Sample-efficient charging optimization for lithium-ion batteries based on dynamic data-driven model with short-branch rollout,2026,"Quanxue Guan, Bohao Zhang, Yuqian Fan, Q. Guan, Ziyu Zhao, Xiaojun Tan",,Journal of Energy Storage,9c4d2d6f-3684-4677-b2ec-700ebb73e760
6c9082413c8da45253d3ac232f48406a2623c064,A topology-aware segment anything model for domain-invariant crack segmentation,2026,"Xue Li, Siyi Yu, Shiyun Xiao, Xiangbo Lin, Zihan Zhao",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
8568e99530ccde66cbba382e5bf6fad1b40a2aa9,CNC-VLM: An RLHF-optimized industrial large vision-language model with multimodal learning for imbalanced CNC fault detection,2026,"Zisheng Wang, Junjie Chen, Chisen Wang, Cong Peng, Jianping Xuan, Tielin Shi, Ming J. Zuo",,Mechanical systems and signal processing,dc4b3846-1e31-4c19-a196-e8b1d091037f
c931245749078fb5ea7ad5eccd37f3bebde8d31f,Early classification of industrial alarm floods using a hybrid neural network and optimal time-encoded histograms,2026,"Amirhossein Najafi, Tongwen Chen",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
1362053f8bedbf1805afe5645360c32a7dc0f84a,Development of a data-driven user-defined hardening model for cyclic behavior via transformer,2026,"Jonghwan Lee, Burcu Tasdemir, Michael Martin, David M. Knowles, M. Mostafavi",,Materials &amp; Design,
a61c4fce4453deae287f940068de34efdf5a9fa2,Efficient Temporal Adjacent Transformer for remaining useful life prognostics,2026,"Jiang Zhu, Jun Ma, Jiande Wu, Lekang Fan",,Engineering applications of artificial intelligence,1a24ea21-4c37-41d8-9e76-ab802d4afb3e
26d543584d3efd97d03392abd0cb7267234175eb,DA-MiTUNet: A Mix Transformer with dual attention embedding in unet for Land-Sea segmentation of remote sensing images,2026,"Jiawei Wu, Zijian Liu, Qixiang Tong, Zhipeng Zhu, Hui He, Xinghui Wu, Haihua Xing",,International Journal of Applied Earth Observation and Geoinformation,4502a19c-ac5e-45a7-9302-2f7b3bcdc0b7
991df496b1084e9afd373275832b75ca4399323c,A3BRec: A Novel Association-Integration Sequential Basket Recommendation,2026,"Yi-Cheng Chen, Yen-Liang Chen","Owing to their widespread practicability, recommendation systems play an important role in our daily life. Recently, several studies have examined recommendations based on learning from user purchase history using association rules to extract the complementary and substitution relationships between items. However, the integration of generalized rules into a customized recommendation model is a challenging task. This study proposes a novel recommendation model, A3BRec, which incorporates association rules with a transformer network to predict potentially interesting items for the next basket. We introduce a ternary-stage framework to integrate basket associations into sequential next-basket recommendations. Furthermore, extensive experiments were conducted on real-world datasets to demonstrate the performance and superiority of the proposed model over the state-of-the-art methods for various evaluation metrics. We also use a case study to show the improvement and influence of the proposed ternary integration in A3BRec on recommendation quality.",IEEE Transactions on Knowledge and Data Engineering,c6840156-ee10-4d78-8832-7f8909811576
c6f8ca3e5c5a2c6997311fe61b43d54da8d4614c,A Dual-Layer End-to-End Cost Estimation Model for LSM-Tree-Based Database Systems,2026,"Yinan Jing, Songli Wang, Zhenying He, Kai Zhang, X. S. Wang, Jinbao Li","In database systems, cost estimation for query plans has a variety of uses, including query optimization, resource management, load balancing, query scheduling, performance monitoring, and automated maintenance. Existing methods mainly targeted B-tree-based systems, where costs and cardinality are highly correlated. However, LSM-Trees, due to their unique storage structure, violate the assumptions of existing learning methods, causing cardinality to be irrelevant to cost estimation. In addition, the constantly changing data layout leads to severe data drifts when updating data, preventing current learning-based models from accurately estimating costs in an agile way. To address these challenges, we propose a dual-layer end-to-end cost estimation model for LSM-Tree-based database systems. This model treats cost estimation as a regression problem and comprises two layers: the storage layer and the query plan layer. The storage layer employs lightweight neural networks to leverage data distribution, provide information to the query plan layer, and address the challenges posed by data drift in LSM-Trees. The query plan layer uses the Transformer framework and incorporates structural information to learn the representation of plans. This dual-layer architecture allows our model to effectively embed storage information and query plan tree details. The results show that our proposed model achieves state-of-the-art cost estimation accuracy for database systems based on LSM-Trees. Additionally, our architecture significantly reduces the model’s updating costs, ensuring robust performance amid frequent data drifts.",IEEE Transactions on Knowledge and Data Engineering,c6840156-ee10-4d78-8832-7f8909811576
8dbb3d814ef9abb182eeb5301c5d22169ab3e803,MTNet: A Multi-Task Learning Framework That Integrates Intra-Task and Task-Specific Dependencies for Traffic Forecasting,2026,"Shaokun Zhang, Rui Wang, Hongjun Tang, Kaizhong Zuo, Peng Jiang, Peng Hu, Wenjie Li, Biao Jie, Peize Zhao","Traffic prediction is essential for modern transportation systems, enhancing traffic management and urban planning. Accurate predictions of traffic flow and speed are crucial for understanding road usage, mitigating congestion, and providing real-time traffic monitoring and dynamic route guidance, thus improving road safety and infrastructure efficiency. Traditional research has often focused on predicting traffic flow or speed independently, leading to higher resource consumption due to the need for separate models. Few studies have explored the simultaneous prediction of both metrics, with recent attempts failing to account for spatial correlations, resulting in suboptimal performance. To address these challenges, we propose MTNet, a multi-task learning framework for joint traffic flow and speed prediction. MTNet employs a Transformer-like Encoder-Decoder architecture to process and enhance feature representations, capturing complex spatio-temporal correlations. Specifically, MTNet extracts intra-task dependencies using a cross-task interaction module and models task-specific spatiotemporal dependencies using spatial and temporal-aware modules with cascaded residual structures. Additionally, spatio-temporal positional encoding is integrated to increase awareness of long-term and long-distance dependencies. Extensive experiments on three diverse traffic datasets—Manchester, PeMSD4, and PeMSD8—demonstrate that MTNet significantly outperforms state-of-the-art methods in both traffic flow and speed prediction. MTNet achieves substantial improvements in prediction accuracy and efficiency, striking an optimal balance between performance and computational resource usage.",IEEE Transactions on Knowledge and Data Engineering,c6840156-ee10-4d78-8832-7f8909811576
3ddece2971251ff02393d0325d54c6dbd2b2f416,A Dynamic-Causal Lens of Stock Interactions: Graph Modeling From Symbolic Movement Patterns,2026,"Haotian Liu, Bowen Hu, Yadong Zhou, Yuxun Zhou","In modern quantitative finance and portfolio-based investment, modeling latent interactions among stocks is paramount for prediction and decision-making on profit, risk management, hedging, etc. While previous studies have constructed complex stock graphs for applying sophisticated variants of graph neural networks (GNNs), existing graph modeling approaches still face two limitations: 1) Correlation-based statistical relationships fail to unveil nuanced stock interactions effectively and determine directional influence. 2) Rigid and static graphs overlook the evolving graph structure of stocks in volatile financial systems. In this paper, we propose a dynamic-causal graph neural network (DC-GNN) to discover causal interactions from the non-stationary price time series and dynamically model graph structures for stock movement prediction. More specifically, we identify the pattern prototypes of all directed stock pairs from long-term price movement knowledge to quantify their causal interactions. These prototypes capture the pattern-to-pattern correspondence across time series based on symbolic dynamics. By inferring real-time stock networks from the prototypes, we encapsulate neighbor-induced causal impacts within heterogeneous edges to model bullish, bearish, and neutral effects among stocks. Extensive experiments conducted on real-world trading data demonstrate the superiority of the proposed framework over various state-of-the-art baselines and its effectiveness, robustness, and interpretability in Fintech.",IEEE Transactions on Knowledge and Data Engineering,c6840156-ee10-4d78-8832-7f8909811576
05de4af1cae84188f9d0a1193d7d52ed8f2556f0,Defending Attacks on Anti-Fraud Model With Generative Graph Representations,2026,"Jiasheng Wu, Xincheng Wang, Jie Yang, Dawei Cheng, Guang Yang, Bo Wang","Fraud detection is a typical data mining mission in the field of finance. In recent years, due to their capability of mining hidden associations between entities, graph neural networks (GNNs) have been widely applied to detect financial fraudsters. However, GNNs are fragile in their data aggregation process and will be attacked on purpose by fraudsters, therefore, some other pioneers have explored methods to enhance the robustness of GNN-based fraud detection models. But most existing models based on ideal settings, as real-life criminals tend to attack as far as they can reach, struggle to establish a unified effective approach for attacked and unattacked data of different scales in realistic scenarios. Furthermore, mainstream robust defense models indiscriminately modifying and truncating data will lose important information of the major unattacked parts in the original graph, which lowers their overall fraud detection precision. Therefore, in this work, we propose a novel generative fraud detection framework called GSRGNN. In particular, we first design a generative structure to obtain augmented node features. Then we prioritize the nodes with high degrees to create a more stable graph structure on local distributions. Finally, we pass the enhanced features and structure with the input ones in pairs through GNN layers, and create synthetic representations with abundant information and sufficient resistance to perturbations for subsequent fraud detection. In addition, we also design a novel black-box attack algorithm to realistically imitate the perturbations conducted by fraudsters on graph features and structure. Experiments on the world's leading electronic trading platform and public anti-fraud datasets demonstrate the outstanding performance of our proposed method compared with those state-of-the-art models, showing its superiority in precision and robustness on financial fraud detection missions.",IEEE Transactions on Knowledge and Data Engineering,c6840156-ee10-4d78-8832-7f8909811576
ab83f170958e1d10c12fccfc311b7166fbd27cde,ExplainHM++: Explainable Harmful Meme Detection With Retrieval-Augmented Debate Between Large Multimodal Models,2026,"Hongzhan Lin, Wei Gao, Jing Ma, Yang Deng, Ziyang Luo, Bo Wang, Ruichao Yang, Tat-Seng Chua","Identifying harmful memes is challenging due to their implicit meanings, which are not always evident from texts and images alone. Existing solutions often lack clear explanations to justify their decisions. To address this gap, we propose an explainable approach, ExplainHM++, which detects harmful memes by reasoning over competing rationales from both harmful and harmless perspectives. First, inspired by the capabilities of Large Multimodal Models (LMMs) in text generation and multimodal reasoning, we develop ExplainHM, a one-stage multimodal debate in which LMMs generate explanations through contradictory arguments. Second, we fine-tune a small language model to serve as a judge in the debate, improving the integration of harmfulness rationales with the multimodal content of memes. However, we observe that a naive multimodal debate remains vulnerable, as it heavily depends on the inherent reasoning ability of LMMs to understand the memes. Given the evolving and noisy nature of memes, we further introduce a meme sample retrieval mechanism and a retrieval-augmented debate paradigm to strengthen and refine LMM-generated explanations. Extensive experiments on three public meme datasets demonstrate that ExplainHM++ not only outperforms state-of-the-art methods but also provides superior, interpretable explanations for harmful meme detection.",IEEE Transactions on Knowledge and Data Engineering,c6840156-ee10-4d78-8832-7f8909811576
2a1f72caaee9e4aa38872e96d4e1be539924aa90,Lithium-ion battery state of health estimation using a hybrid model with multiple health indicators,2026,"Yuxi Yang, Qihui Xia, Lexi Yuan, Peng Zhao, Liang Xuan, Jiyuan Liang",,Journal of Energy Storage,9c4d2d6f-3684-4677-b2ec-700ebb73e760
432fbe08e9eeb5bc378f795d7220ebeb299aa655,MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments,2026,"Weipeng Jing, Peilun Kang, Donglin Di, Jian Wang, Yang Song, Chao Li, Lei Fan",,Isprs Journal of Photogrammetry and Remote Sensing,227fb221-5e57-477c-b756-e39dd8ffd538
1ac2196cde3e64b40ac12dbdaac3afce94f95fd6,Reinforcement learning for cumulative error correction in prediction the state-of-health of battery,2026,"Yongyi Shi, Hequn Wu, Jianshe Wu, Feng Zheng",,Journal of Energy Storage,9c4d2d6f-3684-4677-b2ec-700ebb73e760
ef5e8e82b0cba3a84dea0e138fcfcd263edf0cc6,SwinULoc: Pre-Trained Swin Transformer U-Net With ToF Offset Correction for Resource-Efficient WiFi Indoor Localization,2026,"Wenhao Zhang, Xingfa Shen, Sicong Xia, Zhibo Wang","The ubiquity of WiFi infrastructure has motivated significant research into WiFi-based indoor positioning systems as practical alternatives to GNSS. While deep learning approaches show promise, existing models face three critical limitations: (1) inadequate modeling of long-range feature dependencies, (2) difficulty in correcting time-of-flight (ToF) offsets induced by device clock asynchrony, and (3) prohibitive computational costs for environmental adaptation through model retraining. This paper introduces SwinULoc, a novel U-shaped indoor positioning framework that synergizes Swin Transformer blocks with 2D CSI heatmap processing. Our architecture uniquely addresses these challenges through three key innovations: First, the integration of shifted window attention mechanisms enables effective learning of long-range signal correlations. Second, a multi-access-point fusion strategy enhanced with skip connections achieves precise ToF offset correction through cross-device pattern analysis and multi-scale feature integration. Third, a transfer learning paradigm reduces retraining costs by 75% compared to conventional approaches. Extensive evaluations demonstrate SwinULoc’s superiority, achieving 70% higher positioning accuracy than state-of-the-art baselines while requiring only 1/4 of the training resources for new environments.",IEEE Transactions on Mobile Computing,4e46790b-e240-4236-9b8d-a70ed74f900a
66d5c0c71b52855a0d5809fc0dfd499c7e73366f,FullPerception: Network-Level Collaborative Perception for Eliminating Vehicular Blind Spots,2026,"Lin Liang, Guiyang Luo, Yijing Lin, Lei Deng, Nan Cheng, Quan Yuan, Jinglin Li, Dusit Niyato","Collaborative perception can significantly enhance the perceptual capabilities of autonomous vehicles by sharing sensing information through vehicular communications. However, large-scale sharing of sensing information often results in unsustainable network loads, making it challenging to maximize perception performance with limited communication resources in complex environments. To address this challenge, we propose FullPerception, an innovative cooperative perception framework that jointly orchestrates sensing information sharing and communication resource allocation at the network level. FullPerception advocates for the sharing of semantic information (neural network features) within critical areas, i.e., blind spots. With limited communication resources, FullPerception strategically eliminates these blind spots to maximize the accumulated perception performance. We formulate this strategy as a weighted optimization problem and prove its NP-hardness. We propose a simple yet effective algorithm, Proactive Conflict-free Scheduling (PCS), which guarantees a good performance ratio by considering broader contexts. PCS is meticulously combined with recursive structure, accounting for both the overall and future contexts to determine link scheduling and resource allocation. We demonstrate that FullPerception improves perception accuracy by 20% relative to single-vehicle systems and by 10% compared to existing scheduling methods through large-scale comprehensive joint simulation experiments.",IEEE Transactions on Mobile Computing,4e46790b-e240-4236-9b8d-a70ed74f900a
d8c178e47911baf0be81724ec7ecc93452431991,"Long-term air quality forecasting in Korba, India (2025–2047): A hybrid model using 44-year satellite data",2026,"Abhimanyu Kumar Gond, Aarif Jamal, T. Verma",,Journal of Atmospheric and Solar-Terrestrial Physics,6015c636-10c6-4fd7-94ca-943dceb51354
3211f1fb5f6dce247c3708c8d50ac5e77958d72c,PrivGuardInfer: Channel-Level End-Edge Collaborative Inference Strategy Protecting Original Inputs and Sensitive Attributes,2026,"Yunhao Yao, Zhiqiang Wang, Puhan Luo, Yihang Cheng, Jiahui Hou, Xiangyang Li","End-edge collaborative inference improves computational efficiency by dividing a deep neural network into two parts, executed across the end device and the edge node in parallel. However, adversaries like malicious edge nodes can exploit transmitted data to reconstruct original inputs or infer sensitive attributes. Existing collaborative inference strategies upload the majority of input features to the edge node, significantly increasing the risk of privacy leakage, even without input reconstruction. Therefore, we propose PrivGuardInfer, a channel-level DNN end-edge collaborative inference strategy that optimizes intra-layer partition to simultaneously protect original inputs and sensitive attributes while ensuring latency constraints, supported by three key designs. First, the privacy measurements oriented both layer depth and channel count, jointly quantify the difficulty of reconstructing original inputs using varying numbers of feature maps across different layers. After assessing each channel’s contribution, the information offset further measures the difficulty of inferring sensitive attributes. Finally, PrivGuardInfer models the privacy-optimal intra-layer partition under latency constraints as a grouped knapsack problem, mapping attack difficulty to item values and inference latency to item weights. Experimental results reveal that PrivGuardInfer achieves an average improvement of 80.54% in defending against model inversion attacks and 63.34% against attribute inference attacks compared to existing end-edge partition strategies. Moreover, it outperforms current privacy protection methods by an average of 69.37% and 49.75% in mitigating these two types of attacks.",IEEE Transactions on Mobile Computing,4e46790b-e240-4236-9b8d-a70ed74f900a
063405d2802198f54b014b3387f445d33b7f1030,EOC-Tracking: An Environmental Obstacles Constrained Adaptive Wi-Fi Tracking Framework,2026,"Jinwei Gao, Qixuan Cai, Mengjie Yu, Xinyu Tong, Tony Xiao Han, Xiulong Liu, Xin Xie, Wenyu Qu","Wi-Fi device-free tracking enables the inference of user behaviors without physical contact, which is crucial for intelligent indoor location-based services. Nevertheless, the practical implementation of current tracking systems is constrained by several critical limitations: 1) The low-quality sensing signals in complex scenarios lead to increased tracking errors; 2) Existing methods inadequately adjust to dynamic environments, necessitating additional data collection or retraining processes. To address these challenges, this paper introduces EOC-Tracking, a device-free Wi-Fi tracking system that dynamically incorporates environmental information. Our key innovation involves leveraging obstacles to correct illogical users’ trajectories and facilitate adjustment to varying environments. This significantly improves the accuracy of the follow-up in complex and changing environments. The EOC-Tracking system is built upon three fundamental design principles: 1) A lightweight dual-branch neural network architecture that effectively fuses environmental data with Wi-Fi signal characteristics; 2) An autonomous map updating mechanism that facilitates real-time adaptation to environmental layout modifications without human intervention; 3) A sophisticated data-driven, phased training paradigm that optimizes the model’s ability to learn and apply obstacle constraints. We implement EOC-Tracking using commercial Wi-Fi devices and deploy it on low-power embedded systems such as the MCU. Experimental results demonstrate that EOC-Tracking can reduce tracking errors by at most 49.48% compared to data-driven methods and 62.21% compared to model-based methods in various complex scenarios.",IEEE Transactions on Mobile Computing,4e46790b-e240-4236-9b8d-a70ed74f900a
bdcdf1f6495cedfaad77c746b39f6c6c8e799312,Mapping land uses following tropical deforestation with location-aware deep learning,2026,"Jan Pišl, Gencer Sumbul, G. Lenczner, Camilo Zamora, Martin Herold, Jan Dirk Wegner, D. Tuia",,Isprs Journal of Photogrammetry and Remote Sensing,227fb221-5e57-477c-b756-e39dd8ffd538
3f549b3cb6c244cbffe29f95f69a6974d2f23a4f,Behavioral modeling of microwave components in constrained domains using recurrent neural networks with attention layers,2026,"Sławomir Kozieł, Kaustab C. Sahu, A. Pietrenko‐Dabrowska",,AEU - International Journal of Electronics and Communications,
1894b2802282ddf72834a0cf6170d932c0df9e43,CAFTrans: Content-Aware Fusion Transformer for ground-based remote sensing cloud detection,2026,"Shuang Liu, Zeyu Yu, Ying Liu, Zhong Zhang, Chaojun Shi, Baihua Xiao",,International Journal of Applied Earth Observation and Geoinformation,4502a19c-ac5e-45a7-9302-2f7b3bcdc0b7
9434a5a01a1380b37a3eef314156a7ad29e5c7ab,Lithium-ion battery temperature prediction based on multi-scale feature fusion using temporal convolutional network and informer,2026,"Liang Zeng, Yongyao Zhong, Shanshan Wang",,Journal of Energy Storage,9c4d2d6f-3684-4677-b2ec-700ebb73e760
4ae8219cffd75f6248b8cb18176c0b1190ca506c,Large Kernel Information-interaction Network for single image super-resolution,2026,"Yi Wu, Weiwei Wang, Yujie Wang, Kaige Cui",,Computer Vision and Image Understanding,5fbb417b-d7a5-44e6-856d-993f0624ed9c
bc75c1722af1d7b227447af8074f6f4e76b540ad,A multi-variable driven dual-stage modal-decoupling framework integrating deterministic–uncertainty modeling for wind power forecasting with feature interpretability analysis,2026,"Wenchao Jia, Aiming An, Bin Gong, Yaoke Shi, Zheming Yan",,Energy,619f1d85-7b3f-451d-b795-3f40e3019fd4
babbd7077481c499a1fb2959adfe66f453927e89,Residential-ECG: AI-driven one-step generation of low-energy residential floor plans,2026,"Pengyu Zeng, Yujian Huang, Jun Yin, Yi Zhang, Shuai Lu",,Sustainable cities and society,96999166-7d47-4d87-b4f8-e11d11c7c45a
faeb4ddfbac94b7d9b39800255125afd111414f6,Safety-aware predictive motion planning for close-range human-UAV collaboration in construction,2026,"Tianyu Ren, Houtan Jebelli",,Automation in Construction,cbe2e2e0-f4d3-4923-8b48-a02259e5f89c
886c5333980e361077a645cd4d3bfbc812920a84,"Full-waveform CNN–transformer neural network for regional coseismic landslide susceptibility modeling: A case study of the 2022 Luding earthquake, China",2026,"Xiaolong Zhang, Shuai Huang, Binghai Gao",,Engineering Geology,1aa91ec4-2e1e-4767-894b-34a0d6def7e9
e95838b4d96b8be97ac1233deffc5c110f3c81e7,"Long-Term Time Series Forecasting: The Good, the Bad, and the Ugly",2026,"Lorenzo Epifani, Alessandro Falcetta, Manuel Roveri","Long-term time series forecasting (LTF) aims to predict time series over extended time horizons, offering significant cross-domain advantages and competitive insights. However, there is no universal solution to this task, and different methods may prove to be effective depending on the specific context. This survey provides a critical investigation of the state-of-the-art and the current research landscape in LTF. To structure this analysis, the paper is built around three key research questions: (1) Why are long-term forecasts more intriguing and valuable than short-term ones? (2) Why are specialised models essential for accurate long-term forecasting? (3) What are the typical challenges, pitfalls and critical points that must be addressed in this field of research? By addressing these questions, this work critically examines existing approaches, highlighting weak and strong points of current research trends. This discussion is further enriched by an overview of the benchmark datasets, the evaluation metrics, a taxonomy for model classification, as well as an analysis of recent methodological advances.",IEEE Computational Intelligence Magazine,ee372de7-efda-4907-a03f-359292ea27f6
210f9b5f28e1f7996f71553e0fcf9204c78b0413,Predicting transfer times across production lines using data pooling,2026,"Seohyun Choi, Young Ha Joo, Hoonseok Park, Younkook Kang, Ri Choe, Jae-Yoon Jung",,Journal of manufacturing systems,dc56537e-0e06-44dd-8fe9-50131609f0c2
a03710046b64af2e14290b789a544a976c162443,QuaFT: Quality-guided semantic fault intelligence under corrupted industrial data,2026,"Lu Ning, Rui Wang, Xiaoran Wei, Hao Huang, Zonghan Zhang, Zehua Chen, Xiaolu Liu, Bo Zhang, Yao Liu, Hao Gong, Kai Song",,Chemical engineering research & design,f4425a16-9b37-42ae-b9e3-a9359627f3b9
0ffe731ca9bd617e419e3f1a41346059468ef23c,UAV-based quantitative crack measurement for bridges integrating four-point laser metric calibration and mamba segmentation,2026,"Jinghuan Zhang, Wang Chen, Jian Zhang",,Automation in Construction,cbe2e2e0-f4d3-4923-8b48-a02259e5f89c
750de49eed75260bd54548a97a0c76abd5b2732c,Privacy-preserving image captioning using virtual photon-limited imaging and federated learning,2026,"Antoinette Deborah Martin, Inkyu Moon",,Results in Optics,bffbb20b-740f-4079-a11a-0ebc8c035852
3fb7d1d4e0717ae751c0f04862e15a69c636f16a,A data-driven framework for distributed dynamic load identification incorporating physics-based temporal causality constraints,2026,"Rutong Chen, Jinhui Jiang, Yiyuan Guan",,Mechanical systems and signal processing,dc4b3846-1e31-4c19-a196-e8b1d091037f
62fa1ac4c116ba3d56e0eb5dfadd0e3a3b72219d,A survey of features used for representing black-box single-objective continuous optimization,2026,"Gjorgjina Cenikj, Ana Nikolikj, G. Petelin, Niki van Stein, Carola Doerr, T. Eftimov",,Swarm and Evolutionary Computation,b712be20-2e99-48d0-a5dd-e62e6ffc3fdb
4e7efbb0e005033caf8062b7fa9a33b38c6dcb58,VDDFormer: A Variable Dependency Discrepancy-Based Transformer for Multivariate Time Series Anomaly Detection,2026,"Bo Liu, Lingling Tao, Xiaodan Chen, Zhijun Li","The dynamics of multivariate time series (MTS) data are jointly characterized by its nonlinear temporal dependencies and complex variable dependencies, making unsupervised time series anomaly detection a challenging task. Existing methods primarily rely on prediction or reconstruction errors, neglecting the valuable information within the variable dependencies. In this paper, we propose a variable dependency discrepancy-based Transformer (VDDFormer) for unsupervised MTS anomaly detection. VDDFormer comprises a variable correlation encoder, a temporal dependency encoder, and a reconstruction decoder. The variable correlation encoder capitalizes on a variable dependency attention mechanism, which employs self-attention to learn the global variable dependencies; meanwhile, the local variable dependencies are captured by the adaptive correlation matrix. The global and local variable dependencies are then used to compute the variable dependency discrepancy as a new intrinsic property to distinguish between normal and abnormal patterns. By integrating this new discrepancy with the reconstruction error, the model effectively enhances its anomaly differentiation capability. Extensive experiments on five real-world anomaly detection datasets demonstrate that VDDFormer effectively and robustly detects group anomaly patterns by leveraging the variable dependency discrepancy and achieves state-of-the-art performance on four out of the five datasets.",IEEE Transactions on Big Data,9cc701cf-1f49-4121-8ea9-11bac24d61ad
478d4faffd10faa3ff78b43dd62485a759bbdcb8,TAG: Triple Alignment With Rationale Generation for Knowledge-Based Visual Question Answering,2026,"Sihang Cai, Xuan Lin, Wenqiang Xu, Jingtong Wu, Tao Jin, Zhou Zhao, Fei Wu, Jun Yu","Knowledge-based Visual Question Answering (VQA) involves answering questions based not only on the given image, but also on external knowledge. Existing methods for knowledge-based VQA can be classified into two main categories: those that rely on external knowledge bases, and those that use Large Language Models (LLMs) as implicit knowledge engines. However, the former approach heavily relies on the quality of information retrieval, introducing additional information bias to the entire system. And the latter approach suffers from the extremely high computational cost and the loss of image information. To address these issues, we propose a novel framework called TAG that reformulates knowledge-based VQA as a contrastive learning problem. We innovatively propose a triple asymmetric paradigm, which aligns a lightweight text encoder to the image space with an extremely low training cost (0.0152B trainable parameters), and enhance its understanding ability on semantic granularity. TAG is both computation-efficient and effective, and we evaluate it on the knowledge-based VQA datasets, A-OKVQA, OK-VQA and VCR. The results show that TAG (0.387B) achieves the state-of-the-art performance when compared to methods using less than 1B parameters. Besides, TAG still shows competitive performance when compared to methods with LLM.",IEEE Transactions on Big Data,9cc701cf-1f49-4121-8ea9-11bac24d61ad
05aec74efe3860883d56f334621512047c757da1,Behavior Habits Enhanced Intention Learning for Session Based Recommendation,2026,"Zhida Qin, Wenhao Xue, Haotian He, Haoyao Zhang, Shixiao Yang, Enjun Du, Tianyu Huang, John C. S. Lui","Multi-behavior Session Based Recommendations (MBSBRs) have achieved remarkable results due to considering behavioral heterogeneity in sessions. Yet most existing works only consider binary or continuous behavior dependencies and aim to predict the next item under the target behavior, neglecting users’ inherent behavior habits, resulting in learning inaccurate intentions. To tackle the above issues, we propose a novel Behavior Habits Enhanced Intention Learning framework for Session Based Recommendation (BHSBR). Specifically, we focus on the next item recommendation and design a global item transition graph to learn the behavior-aware semantic relationships between items, in order to mine the underlying similarity between items beyond the session. In addition, we construct a hypergraph to extract the diverse behavior habits of users and break through the limitations of temporal relationships in the session. Compared to the existing works, our behavior habit learning method learns behavior dependencies at the user level, which could capture the user’s more accurate long-term intentions and reduce the impact of noise behaviors. Extensive experiments on three datasets demonstrate that the performance of our proposed BHSBR is superior to SOTA. Further ablation experiments fully illustrate the effectiveness of our various modules.",IEEE Transactions on Big Data,9cc701cf-1f49-4121-8ea9-11bac24d61ad
f89d50d82f30b02eebef7dd22bb5f8e78cf6b955,FPGA Implementation of Binary-Weighted Transformer for Prognostics and Health Management of Permanent Magnet Synchronous Motors Using Current Sensors,2026,"Soongyu Kang, Yongchul Jung, Sewoon Oh, Yunho Jung","In this letter, we propose a prognostics and health management (PHM) method for permanent magnet synchronous motors (PMSMs) in urban air mobility. The method uses a Transformer and three-phase current sensors. The short-time Fourier transform is employed for current signals to capture fault-related information. The Transformer architecture effectively extracts both local and global features from time–frequency representations, enabling high classification performance. However, its high computational cost and large model size hinder deployment in practical industrial applications. To address this challenge, we designed a lightweight binary-weighted Transformer (BWT) for PMSM PHM, reducing the model size to 5.5% of the baseline. The proposed BWT achieves 99.81% classification accuracy across four classes. We also developed a hardware accelerator for matrix multiplication—the most time-consuming operation in BWT—and implemented it on a field-programmable gate array. The proposed SW/HW co-design achieved an 85.55× speedup over the software-only implementation on the ARM microprocessor unit.",IEEE Sensors Letters,3e034454-c398-4b3f-a197-7f7dbf3f9eb7
010e95944061e30926ce2327377b776038dceaf2,Large Foundation Model Empowered Region-aware Underwater Image Captioning,2026,"Huanyu Li, Li Li, Hao Wang, Weibo Zhang, Peng Ren",,International Journal of Computer Vision,939ee07c-6009-43f8-b884-69238b40659e
078b706f7acc5f6bfb56776afb65ab43f8bc104f,Heterogeneous GNN-driven and meta-learned architecture for transparent multimodal string similarity estimation,2026,"Shaik Asha, S. T. Krishna",,Knowledge and Information Systems,1f55639d-134e-44ae-b050-ccf2a6676bc5
c591be5beb88866142a7c8d6c34b399eac00ba19,Contactless 12-lead electrocardiogram via deep computational radar,2026,"Zhi Lu, Jinbo Chen, Dongheng Zhang, Haoyu Wang, Pengcheng Huang, Fang Zhou, Yang Hu, Qibin Sun, Min Gao, Yan Chen",,npj Biomedical Innovations,
c4c7daf880d0bdd217711ee9fa19851640d077db,Fusion of multi-scale geometric features and frequency domain decomposition for stereo matching network,2026,"Hua Hou, Diancheng Wang, Jinqian Xu, Yan Wang","In learning-based stereo matching methods, a feature information-rich and concise cost volume is crucial for achieving high-precision and high-efficiency stereo matching. Aiming at the problem that the cost volume lacks global geometric information, which leads to confusing foreground and background disparity estimation and blurring at edges and details, this paper proposes a fusion of multi-scale geometric features and frequency domain decomposition stereo matching network. Firstly, the initial cost volume is processed by the multi-scale geometric extraction module, which achieves an effective conversion from local correlation to global geometric information understanding, and significantly enhances the perception of scene boundaries and occluded regions. In the cost aggregation stage, we introduce an adaptive guidance mechanism based on channel attention, which not only improves the cost aggregation efficiency but also reduces the time overhead. In the disparity refinement stage, we not only use the iterative update of disparity based on multi-scale GRU, but also introduce the high and low-frequency separation of disparity reconstruction network, which reconstructs the disparity by decomposing the high and low-frequency errors, and is able to obtain a finer full-resolution disparity map. Our method achieves state-of-the-art performance on benchmark tests across multiple datasets, including Scene mFlow, KITTI2012, KITTI2015, ETH3D, and Middlebury. Compared to mainstream approaches, our method demonstrates excellent results on the KITTI2015 test set, attaining error rates of 1.39% in the background region (D1-bg) and 2.54% in the foreground region (D1-fg), while maintaining real-time inference capabilities.",PLoS ONE,0aed7a40-85f3-4c66-9e1b-c1556c57001b
e5e3619305e5d234ce6eab1b6d7119aa6f169db8,Track Optimization and Matching With Learnable Modules for LMB-Based Distributed Resolvable Group Target Tracking,2026,"Yue Yu, Mei Liu, Yuguan Hou","This article integrates two data-driven learnable modules into the conventional distributed multisensor resolvable group target tracking (RGTT) framework based on the labeled multi-Bernoulli (LMB) filter to improve tracking performance. First, in RGTT scenarios with densely distributed targets, the LMB filter is prone to track fragmentation and label switching, which reduces accuracy and complicates multisensor data association. While some studies have attempted to address this, their methods typically overlook the group structure and are often offline and computationally intensive. To overcome this, we propose a track optimization module based on an attention mechanism that alleviates these issues efficiently in an online setting. Second, we replace traditional data association algorithms, which rely on fixed similarity metrics, with a track matching module based on a message passing network (MPN). This module leverages historical track data and learns to infer latent similarities across sensors, significantly improving matching accuracy. Simulation results demonstrate the effectiveness of the proposed modules.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
567d00adfb01dacefa796d73577ac7ba36c2bf47,A Lightweight Two-Stage Attention-Gating Network for Efficient Multimodal Sentiment Analysis,2026,"Yibing Wang, Wupeng Xie, Zhutian Yang, Congan Xu, Yun Lin, Yushi Chen","Multimodal sentiment analysis (MSA) has become a crucial research area for interpreting human affective states by integrating textual, visual, and acoustic signals. Despite rapid advances, current MSA models face two persistent challenges: insufficient feature discrimination and effective handling of modality importance. While existing solutions improve performance, they often rely on increasingly complex architectures that limit efficiency and practical deployment. To address these issues, we propose a lightweight two-stage attention-gating network (LTAG-Net), an efficient architecture designed to balance competitive performance with low computational cost. LTAG-Net first employs discriminative feature re-extractors (DFREs), long short-term memory (LSTM)-FC for text and audio, convolutional neural network (CNN)-FC for vision, to enhance unimodal representations, making them more robust and sentiment-discriminative. It then introduces a lightweight residual self-cross attention (RSCA) mechanism to model both intramodal context and intermodal interactions without the computational burden of deep Transformer stacks. Finally, a dynamic modality gating network (DMGN) adaptively prioritizes informative modalities, ensuring balanced and effective fusion. An extensive experiments on CMU-MOSEI and CMU-MOSI demonstrate that LTAG-Net achieves competitive performance compared to baseline models, while significantly reducing computational complexity, offering a more practical solution for real-world applications.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
540cd0cf98ccfd13e37e11f8a257b7e730999882,DecisionLLM: Large Language Models for Long Sequence Decision Exploration,2026,"Xiaowei Lv, Zhilin Zhang, Yijun Li, Yusen Huo, Siyuan Ju, Xuyan Li, Chunxiang Hong, Tianyu Wang, Yongcai Wang, Peng Sun, Chuan Yu, Jian Xu, Bo Zheng","Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs'inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.",Unknown,
660339796cdb13701b6e064e00ae26fdbb4ffd52,End-to-End Shipping Container Code Spotting Based on Visual Sensors for Intelligent Inspection,2026,"Honghui Chen, Xinyang Piao, Suo You, Mengxi Jiang, Chengbin Chen, Pingping Chen","Real-time shipping container code spotting (CCS) plays a vital role in enabling intelligent inspection to optimize smart port logistics. Recent methods suffer from error accumulation and have complex pipelines, resulting in high computational complexity that limits the generalization ability for multidirectional, including horizontal, vertical, and multiline (HVM) codes. Moreover, fixing the parameters of visual sensors (e.g., angle, position, and focal length) to avoid noise interference limits deployment on mobile devices. To address them, we propose an end-toend framework for HVM code spotting. First, an inter-domain feature fusion network that includes the self-spatial enhancement module (SSEM) and self-channel enhancement module (SCEM) is proposed to reduce noise interference and improve the efficiency of the lightweight model. Next, a transformerbased branch with character contrastive earning (CCL) loss is designed to enhance the representation of character features. Experimental results indicate that the proposed method achieves state-of-the-art performance, in which the F-measure and recognition accuracy reach 91.2% and 93.2%, respectively, in real-time.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
fb57713e8e62c770d387980e70fa6b3bc4da0f87,AIoT-Powered Real-Time Sensing and Calibration of Low-Cost Particulate Matter Sensors Using the TCM Network,2026,"Godwin Msigwa, Minji An, Ester Ntambala, Jaeseok Yun","The artificial intelligence of things (AIoT) integrates artificial intelligence (AI) and the Internet of Things (IoT) to create smart systems capable of environmental sensing and real-time interaction. Low-cost sensors (LCSs) offer scalable and continuous air quality monitoring, but their accuracy is often limited due to inherent biases, even after factory calibration. Real-world calibration is therefore essential for reliable measurements. This study presents an AIoT-based solution called the real-time particulate matter sensing and calibration (RPM-SC) system, which uses customized PMS7003 sensors equipped with externally controlled pulsewidth modulation (PWM) fans to improve airflow and measurement consistency. The sensors are integrated into a oneM2M-compliant Internet of Things (IoT) platform for standardized data collection and communication. To enhance measurement accuracy, we propose a novel calibration model: the trans-convolutional fusion memory-aware network (TCM-Net). This model combines transformer networks, temporal convolutional networks (TCNs), and a memory-aware module to capture temporal dependencies and correct sensor bias effectively. TCM-Net was trained on real-world datasets from RPM-SC systems co-located with a certified particulate matter (PM) reference instrument. It achieved a root mean squared error (RMSE) of $0.325~\mu $ g/m3 and a coefficient of determination ( $R^{2}$ ) of 0.999, outperforming conventional models. The results confirm the effectiveness of the proposed AIoT architecture and calibration model in providing accurate and reliable PM2.5 data from LCSs.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
b73390d9a806fd56c03d1749253a8ef29307cc47,ActiFormer: Sign-Aware Linear Attention for Sensor-Based Human Activity Recognition,2026,"Qifan Sun, Zenan Fu, Lei Zhang, Guangjie Chen, Wenbo Huang, Hao Wu, Aiguo Song","Human activity recognition (HAR) plays a pivotal role in ubiquitous computing. However, it remains constrained by the challenge of balancing fine-grained temporal modeling with real-time efficiency on resource-limited devices. While Transformer-based models excel at capturing long-range dependencies, they suffer from high computational costs, limiting their applicability on resource-constrained devices. Linear attention mechanisms improve efficiency but often discard negative signals and produce overly smooth, high-entropy attention distributions, impairing the extraction of fine-grained patterns and degrading classification accuracy in complex scenarios. In this work, we present ActiFormer, a novel sign-aware linear attention framework tailored for sensor-based HAR to overcome these limitations. To preserve bidirectional signal dynamics, we introduce sign-aware attention, which explicitly models both same-sign and cross-sign interactions between queries and keys, effectively retaining negative-signals crucial for accurate recognition. Furthermore, we propose a learnable entropy-scaling function that compensates for the exponential scaling effect lost in linear attention, originally provided by softmax, solving the high-entropy attention weight issue by amplifying the importance of critical temporal points. Extensive experiments on four benchmark HAR datasets demonstrate that ActiFormer consistently outperforms CNNs, standard Transformers, and state-of-the-art (SOTA) linear attention models, both in accuracy and efficiency. Its lightweight design supports real-time inference on edge devices such as the Raspberry Pi 5, highlighting its practical deployability in real-world applications.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
180f5d3771880cb2d68ab0b7e43c1ac7ac1e058c,TransSIL: A Silhouette Cue-Aware Image Classification Framework for Bird Ecological Monitoring Systems,2026,"Hai Liu, Yu Song, Tingting Liu, Lin Chen, Zhaoli Zhang, Xiaolan Yang, Neal N. Xiong","How to automatically recognize the bird species has caused concerns in ecological systems and intelligent ecological monitoring system due to the increasing threat to bird ecological society and bird species diversity. However, traditional monitoring systems are susceptible to specific challenges when operating in complex environments, such as complex environments, multifarious postures, and backlight scenarios. To effectively address these challenges, we present a novel bird ecological intelligent detection system (TransSIL) for fine-grained bird image classification (FBIC) in diverse ecological to learn discriminative features by explicitly incorporating silhouette structural information alongside critical visual cues. Specifically, the approach begins with a silhouette token construction module to estimate the bird silhouette and extract silhouette tokens. Then, a silhouette relationship mining module is developed to fuse visual and silhouette tokens and capture long-range dependencies between them. In addition, to learn bird distinctive features at multiple levels, a critical cues awareness module is embedded within TransSIL. The performance of TransSIL was evaluated on two bird datasets, such as CUB200-2011 and NABirds. The framework demonstrates significant improvements over existing ecological intelligent surveillance methods. By utilizing silhouette and visual dependencies, we anticipate that our approach will ultimately contribute to the conservation of avian ecological societies.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
43e53f42d0ce80b2b492dd92f03d60cc772dbfd1,Design of Tree-Shaped Decoupling Structure Based on Generative Nonadversarial Networks for Antennas in IoT Applications,2026,"Hui Li, Zihan Zhang, Jiapeng Zhang, Zhaolin Li, Tianxi Feng","The miniaturization of Internet of Things (IoT) devices requires multiple antennas in a limited space, which can lead to strong mutual coupling. Traditional decoupling methods usually rely on experience-driven design or application-specific structures, which limits their effectiveness in different antenna configurations. In this article, a generalized tree-shaped structure with high flexibility is proposed for antenna decoupling in terminal devices in IoT systems. The specific geometries of the decoupling structure are optimized using generative nonadversarial networks (GNANs) according to different requirements. As the prior knowledge, quarter-wavelength branches are initialized to improve the optimization efficiency. To meet the design requirements with a smaller dataset, an evolutionary method is used. At the same time, attention mechanism is introduced into the discriminator network, which largely improves the training speed. Following the proposed method, different tree-shaped decoupling structures are optimized and applied to three different multiple-input multiple-output (MIMO) antenna systems, including an ultrawideband (UWB) antenna array, a narrowband microstrip patch antenna array, and an antenna array with different antenna pairs. As a result, isolations have been proved by 20, 53, and 45 dB, respectively. Hence, the flexible tree-shaped structure is validated to be generally effective for closely placed MIMO antennas in IoT systems. Prototypes of the transparent UWB antenna array have been fabricated and measured, with the measurement results agreeing well with the simulated ones.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
e388a4e0c96c2c0c37c629d4c8ea60638eed76e6,Distformer: Time–Frequency Feature Distribution-Aware Transformer for Long-Term Time Series Prediction,2026,"Ruiyun Yu, Yanyan Zhang, Haoyuan Li, Mingda Chen","Long-term time series prediction plays a crucial role in many practical applications. However, time series in the real world often exhibit complex and dynamic temporal patterns, making it challenging to accurately capture their complex time–frequency characteristics. Existing methods struggle to effectively integrate frequency-domain information and time-domain information, leading to poor information interdependence and frequent information loss when introducing frequency-domain components. These limitations compromise the accuracy and robustness of the predictions. Additionally, the lack of diversity in feature extraction restricts the model’s ability to comprehensively capture complex sequence features. To address these problems, we propose an innovative long-term series prediction method called the time–frequency feature distribution-aware Transformer (Distformer). First, Distformer introduces a time–frequency dependency module within the Transformer architecture to enhance the complementarity and synergy between frequency and time information. To preserve information integrity and mitigate potential loss when incorporating frequency-domain data, we design a strategy to supplement the original time-domain information, further improving cross-domain interaction. Second, we develop a feature distribution query module that employs multiple auxiliary encoders to extract features from various perspectives, thereby enriching the diversity of input features. Extensive experiments on multiple datasets demonstrate that our method achieves significant performance improvements over existing prediction models across various benchmarks, offering an effective solution for long-term series prediction. The experimental code for the core method proposed in this article has been open-sourced, and the repository address is https://github.com/bjtdxxn/Dist",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
d5fd719b04c36f0bb999d3383d922468ac0a22a0,Enhancing Generation Quality of Multivehicle Interaction Scenarios for Connected and Automated Vehicles’ Testing: A Novel Physical Constraint-Based Time-Series Generative Adversarial Network,2026,"Ye Li, Lin Huang, Lu Xing, Rui Zhou","Connected and automated vehicles (CAVs) play a crucial role in improving traffic efficiency, safety, and environmental sustainability. Before the widespread deployment of CAVs, extensive scenario-based testing and evaluation are required to ensure their safety and functionality. Currently, most research on test scenarios focuses on generating diverse scenarios and searching for safety-critical scenarios, while insufficient attention has been paid to validating the reasonability and realism of these scenarios. To address this gap, this study proposes an improved deep generative network to enhance the generation quality of multivehicle interaction scenarios. Real vehicle trajectories from urban expressway sections are first extracted from the CitySim dataset. Then, a novel physical constraint-based time-series generative adversarial network (PhysTimeGAN) is proposed. The PhysTimeGAN integrates three key enhancements: a self-attention mechanism to better capture long-term temporal dependencies, a differentiable sorting module to enforce temporal order consistency, and an acceleration anomaly penalty embedded in the loss function to explicitly constrain the generated trajectories within feasible physical limits. These modifications collectively guide the model to generate more realistic and dynamically coherent traffic scenarios. Experimental results show that PhysTimeGAN completely eliminates the trajectory reversal phenomena observed in traditional generative models. Furthermore, the proportion of trajectories satisfying physically reasonable acceleration constraints rises significantly, reaching 94.7%. Compared to both the original time-series generative adversarial network (TimeGAN) and other commonly used generative models, PhysTimeGAN not only enhances the physical reasonability of generated scenarios but also preserves a high degree of similarity to real-world data. Findings of this study provide more reliable support for the testing and evaluation of CAVs.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
8e3b6057c947fd66aa712c284c895e71944b5f26,Multiperson Pose Estimation Using Velocity-Dependent Enhanced mmWave Radar Point Clouds,2026,"Yuexuan Feng, Qifei Zhang, Zhao Wang, Xiaonan Hui, Yulin Zhou","The application of radio frequency (RF) sensors in human-centric perception has attracted considerable attention. Human pose estimation (HPE) using millimeter-wave (mmWave) radar holds promise for use in private and sensitive environments, such as medical monitoring and domestic rehabilitation assessment. Recent methods that utilize mmWave point clouds for pose estimation rely on raw 3-D coordinate input data, failing to effectively utilize key velocity information. In addition, existing research primarily focuses on reflection point clouds from a single person in a fixed position, making it difficult to apply in practical multiperson environments with interference. In this article, we introduce the first multiperson mmWave radar point cloud dataset (mmVPose) focused on rehabilitation training. To effectively leverage velocity information for enriching point cloud features, we propose a velocity-dependent heatmap generation method, which represents input point clouds and predicted joint distributions as heatmaps in 3-D voxel space. Furthermore, to preserve the complete latent human body structure while reducing computational complexity, we design a sparse attention mechanism for voxelized point clouds. Extensive experiments demonstrate that our method mmVPE achieves state-of-the-art (SOTA) performance in multiperson scenarios.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
dddff1108ad62678e0e0da84914e977f785378e4,Attention-Enhanced Transferable Task Offloading via Auxiliary Learning in Mobile Edge Computing,2026,"Rui Zhang, Junjie Hu, Yicheng Di, Jiayu Bao, Jiansong Fan, Yuan Liu","The rapid development of the Internet of Things (IoT) and cyberphysical systems (CPSs) has led to the rise of mobile edge computing (MEC), enabling low-latency task offloading in dynamic environments. However, existing offloading strategies struggle to generalize across diverse and evolving network topologies, often requiring retraining or fine-tuning when deployed in new scenarios. To address these challenges, we propose AtALT, a transferable task-offloading framework that achieves zero-shot transferability. The acronym AtALT is derived from the key components of our method: attention-auxiliary learning-transferable task offloading. AtALT integrates an attention-based encoder and an auxiliary learning module. The attention-based encoder dynamically computes topology-agnostic compatibility scores, allowing for flexible task-offloading decisions across different network configurations. The auxiliary learning module predicts node states, regularizing the policy learning process and enhancing generalization. Experimental results demonstrate that AtALT outperforms existing methods in transferability and efficiency, making it suitable for deployment in previously unseen environments without the need for further training.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
c16df8367857bc3af695ec0d66fe28b54d8fa026,Noncontact Sleep Apnea–Hypopnea Syndrome Detection Using Variational Mode Decomposition-Based Respiratory Signals and Deep Neural Networks,2026,"Saman Faridsoltani, Zahra Rahmani, S. Chamaani, Fatemeh Kashaninasab, Mahboobe Khoozan, Mir Farhad Ghalebandi, Mansour Vali","Abstract-Sleep apnea-hypopnea syndrome (SAHS) is a common sleep disorder that impacts quality of life and often goes undiagnosed. The gold standard for SAHS detection, polysomnography (PSG), is expensive and inconvenient for long-term monitoring. As a result, the research has increasingly shifted toward alternative approaches, such as radar sensors. Feature extraction is crucial for SAHS detection using radar-based respiratory signals. It relies not only on the expertise of specialists and their prior knowledge of respiratory patterns, but also on the methodologies used to extract respiration from the radar. This study presents an automated feature extraction method that utilizes radar respiratory signals processed through variational mode decomposition (VMD) to better preserve the morphological structure of breathing patterns, followed by a hybrid convolutional neural network (CNN)-transformer for classifying SAHS events from normal segments. Then, the apnea-hypopnea index (AHI) is applied to discriminate SAHS subjects from healthy ones. Our study involved 20 PSG recordings and impulse-radio ultrawideband (IR-UWB) radar data across all sleeping positions. The proposed method achieved per-segment classification accuracy of 93.85 %(88.28 %), sensitivity of 87.86 %(75.52 %), specificity of 93.97 %(95.74 %), F1-score of 90.6 %(82.62 %), and Cohen's kappa of 0.763(0.724) using leave-one-out cross-validation (LOOCV) for two (three)-class classifications. AHI estimation analysis showed a strong correlation between the estimated and reference AHI, with a Pearson's correlation coefficient of 0.996. In addition, the SAHS diagnosis achieved an average accuracy of 98.33 % and a Cohen's kappa of 0.96 for AHI thresholds of $\geq 5,15$ , and 30 events/h.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
21c04111cfe74e764a5c5d9d6ca12b77a75c76d5,CoSimR: Similarity Cues-Aware Consanguinity Relationship Mining Framework for IoT-Based Bird Monitoring Systems,2026,"Tingting Liu, Zhibing Liu, Qiang Chen, Hai Liu, Zhaoli Zhang, Neal N. Xiong","The Internet of Things (IoT)-based bird monitoring system faces challenges from occlusion, arbitrary postures, and similar species. To address these, we exploit two key properties of bird images: self-correlation within individual birds and appearance similarity across species, revealing self-correlation and consanguinity relationships. We propose a similarity cues-aware consanguinity relationship mining (CoSimR) framework, which leverages these relationships for robust classification. CoSimR comprises two modules: consanguinity relationship mining (CRM) and cross-species avian prediction (CAP). CRM can capture skeletal structures by generating the correlation tokens, while consanguinity tokens encode $p$ information across five taxonomic levels (class, order, family, genus, and species). CAP leverages a consanguinity-driven multiloss function, including homogeneity loss for intraspecies consistency and affinity loss for cross-species similarity, to guide discriminative feature learning. Experiments conducted on two fine-grained bird image classification (FBIC) datasets demonstrate that the CoSimR model achieves better performance compared with state-of-the-art methods. It highlights the effectiveness of integrating biological hierarchy with visual features, paving the way for similarity cues-aware approaches in fine-grained classification tasks.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
2a92c7bfad4e602260ce76262d44a31a6cca5602,VIB-CTFN: A Multimodal Data Fusion Method Based on Variational Information Bottleneck Optimization for Radar Deceptive Jamming Discrimination,2026,"Yinuo Wu, Jieyi Liu, Yu Zhou, Maoguo Gong, Yongkang Wang, Jiaxi Guo","Existing radar anti-jamming methods are difficult to effectively integrate and utilize multimodal input data, resulting in limited jamming discrimination performance in complex deceptive jamming environments. In order to achieve more sufficient information utilization, VIB-CTFN, a convolutional neural network-Transformer fusion network based on variational information bottleneck optimization is proposed, which introduces an information-theoretic approach to solve the issue of information redundancy in current fusion strategies. The network leverages complementary feature extraction. The convolutional neural network is utilized to extract the microscopic electromagnetic features of signal modality, while Transformer captures the macroscopic long-range kinematic dependencies of data modality, and the multimodal representations are fused with the feature-level multi-head attention mechanism. During alternate training, variational information bottleneck compresses redundant and irrelevant information through variational approximation, optimizes the target critical feature representation, enhancing the generalization ability and robustness of the model. Simulation experiments show that the accuracy of VIB-CTFN in complex deceptive jamming scenarios reaches 99.79%, which is 9.47% and 3.33% higher than that of convolutional neural network and Transformer without variational information bottleneck optimization, while 18.48% and 16.41% higher than that of the traditional methods signal and data fusion-based traditional interference model, demonstrating that the method has a robust and efficient jamming discrimination capability in complex electromagnetic environments.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
b2cfe3b9af80b746ecc8e9068d38653f6e9de232,From Voice to Shell: A SLM-Based Assistant for IoT Maintenance Tasks on the Edge,2026,"Ángela Gijón Flores, Cristina Bolaños Peño, Henry Llumiguano Solano, Jesús Fernández-Bermejo Ruiz, Félix Jesús Villanueva Molina, Fernando Rincón Calle","Internet of Things (IoT) deployment relies on complex command-line interfaces that handle multiple commands, difficult to master, especially for nonexpert or novel technicians and maintainers. Spoken dialog arises as the most natural interaction for providing assistance to humans. Therefore, this study presents a privacy-preserving multilingual voice assistant, completely run on edge devices, that transforms natural language instructions into executable commands for heterogeneous IoT deployments. Two contexts are considered: Docker commands, whose options and arguments are publicly accessible, and an emergency luminaires system commands, a proprietary deployment whose information is not previously available to pretrained models. Multiple datasets for both contexts have been drafted to fine-tune and evaluate proposed models. A benchmark and its results for comparing proposed models are exposed, as well as a functional implementation of the voice assistant, composed of models from the comparison. The proposed system achieves a 3.15 word error rate (WER) and 98.57% semantic similarity for transcribing voice data, and takes only 2.8 s to generate a rightful command with 81.36% accuracy and not needing GPU acceleration, demonstrating reasonable performance on tested edge devices. This study’s findings show that carefully fine-tuned small language models (SLMs) deliver high efficiency for speech-driven IoT device management at a fraction of the computational cost, eliminating cloud dependence and protecting sensitive audio data.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
f038d617246f9d36b8462262c807765073a2b721,DMAW: A Dynamic Multimodal Measurement Fusion Network With Attention for Reliable Welding Process Monitoring Under Harsh Industrial Environments,2026,"Jiawei Fan, Ting Yuan, Haonan Zhang, Songlin Li, U. Hanebeck, Zhuguo Li, Edmond Q. Wu, Jiuchao Qian","In laser welding, harsh environmental conditions such as arc light interference, spatter, and strong background noise pose significant challenges to accurate process monitoring and measurement. Traditional single-modality sensing methods are often inadequate for a comprehensive characterization of welding states and remain highly vulnerable to noise contamination. To overcome the limitations, a dynamic multimodal attention-based weighting (DMAW) network is proposed, which integrates visual and acoustic information to achieve more comprehensive welding state characterization. First, modality-specific feature extractors are trained using unsupervised knowledge distillation (KD) to learn welding-relevant semantic representations. Next, the extracted features are passed through a cross-attention module to enable intermodal interaction and suppress noise. Finally, a dynamic reliability-weighted fusion is proposed that adaptively adjusts modality contributions, thereby reducing measurement uncertainty and enhancing robustness under varying conditions. Experimental validation on a dedicated laser welding platform demonstrates that the proposed framework achieves superior accuracy and resilience in welding state monitoring, highlighting its potential as a reliable solution for intelligent industrial welding systems.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
f509d20f1dd7a8bf4fa94b1f6723f26f13e7cd45,Informer-Based Joint Tracking and Trajectory Prediction of Maneuvering Radar Targets,2026,"Shihuaxiao Zhao, Cao Zeng, Minti Liu","Tracking radar targets over extended time horizons remains challenging due to highly nonlinear and maneuverable motion patterns that lead to cumulative prediction errors. This work addresses the joint estimation of current states and future trajectory prediction for radar targets under incomplete and noisy measurements. We develop a rigorous error-theoretic framework that formally analyzes error propagation mechanisms in sequential prediction, demonstrating that incremental state prediction effectively suppresses systematic bias and cumulative error growth compared to absolute prediction. Guided by these theoretical insights, we propose principled design strategies including feature augmentation with Doppler and kinematic information and increased network capacity. We instantiate this theory-guided approach within the informer architecture, leveraging its efficient sequence-to-sequence capability to mitigate error accumulation while achieving O(L log L) complexity, and propose this informer-based radar maneuvering target tracking (MTT) and predicting framework (InforMTP). A synthetic dataset with randomized motion patterns and multimode dynamics is constructed to validate the framework. Experimental results demonstrate that the proposed method consistently achieves tighter error bounds and substantially outperforms the model-based and hybrid baselines in both prediction accuracy and convergence speed, particularly under complex maneuvering scenarios. The source code is available at: https://github.com/poem-flower/InforMTP.",IEEE Sensors Journal,b210fd3d-11d7-478e-a0aa-7e3d2a4f482d
ea69a6fa5793b5bb84951ad50b65c0d5d9b709da,A Survey on Binary and Ternary Neural Networks and Their Realization in Compute-in-Memory for Edge Intelligence,2026,"Dahoon Park, Hyungdong Park, Inguk Yeo, Jiyun Kim, Sunghyun Lee, Suhak Lee, Hyunseob Shin, S. Pae, Deliang Fan, Jaeha Kung, K. Kwon","Deep learning has achieved remarkable success across a wide range of applications, such as language modeling, computer vision, recommendation systems, and robotics. However, the growing size of models and their increasing computational demands pose significant challenges, particularly for resource-constrained devices. A promising approach to address these challenges is extreme quantization, exemplified by binary and ternary neural networks. These techniques significantly reduce model size by quantizing weights and activations to 1 or 1.58 bit, while simplifying computation, making them well-suited for efficient deployment in resource-limited environments. This article presents a comprehensive review of extreme quantization techniques, organized into three key areas: 1) a comparative analysis of quantizing only the weights [e.g., binary weight networks (BWNs) and ternary weight networks (TWNs)] versus quantizing both weights and activations (e.g., binary neural networks and ternary neural networks), along with a discussion of the progress and tradeoffs of their approaches; 2) an examination of how extreme quantization, initially applied to convolutional neural networks (CNNs), has been extended to Transformer architectures; and 3) an overview of compute-in-memory architectures optimized for binarization and ternarization, including designs based on advanced bit-cell technologies.",IEEE Internet of Things Journal,228761ec-c40a-479b-8309-9dcbe9851bcd
c8abd467396ead2bb488b4b84c6fe5ea5e0df020,LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries,2026,"Xuancheng Ren, Shijing Hu, Zhihui Lu, Jiangqi Huang, Qiang Duan","In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.",Unknown,
daf36986c786b6d2831a0cd9e2e666a3f983895e,LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers,2026,Aryan Karmore,"Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $\rho>0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.",Unknown,
1264a84528238ceb7d0d4d514dfca9502ac7f9ec,Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models,2026,"Andrea Melis, Andrea Piroddi, Roberto Girau","Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.",Unknown,
899020711fa4c2b7bbd68d709044f380fef1e39c,Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition,2026,"Yijin Zhou, Fu Li, Yi Niu, Boxun Fu, Huaning Wang, Lijian Zhang","Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a dimension-as-token formulation. Extensive experiments demonstrate that Neuro-HGLN achieves state-of-the-art performance on multiple benchmarks, providing enhanced interpretability grounded in neurophysiological structure. These results highlight the efficacy of unifying local topological learning with cross-region dependency modeling for robust EEG emotion recognition.",Unknown,
ac581f85e70c777b23ce8accddc4010c4b0d12db,"Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends",2026,"Ye Wang, Jiaxing Chen, Hongjiang Xiao","In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.",Unknown,
1ae308abb1a9e507a609e390da1d36e9843ec9ca,Process-Guided Concept Bottleneck Model,2026,"R. M. Asiyabi, Seosaw Partnership, Steven Hancock Casey Ryan School of GeoSciences, U. Edinburgh, Uk, UK National Centre for Earth Observation","Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.",Unknown,
f7f4a00d740e33bafc29b0d5c0b15756c96711b2,Development of Ontological Knowledge Bases by Leveraging Large Language Models,2026,"Le Ngoc Luyen, Marie-Hélène Abel, Philippe Gouspillou","Ontological Knowledge Bases (OKBs) play a vital role in structuring domain-specific knowledge and serve as a foundation for effective knowledge management systems. However, their traditional manual development poses significant challenges related to scalability, consistency, and adaptability. Recent advancements in Generative AI, particularly Large Language Models (LLMs), offer promising solutions for automating and enhancing OKB development. This paper introduces a structured, iterative methodology leveraging LLMs to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles. We demonstrate this approach through a detailed case study focused on developing a user context profile ontology within the vehicle sales domain. Key contributions include significantly accelerated ontology construction processes, improved ontological consistency, effective bias mitigation, and enhanced transparency in the ontology engineering process. Our findings highlight the transformative potential of integrating LLMs into ontology development, notably improving scalability, integration capabilities, and overall efficiency in knowledge management systems.",Unknown,
f91a3d1409383b79c1bff926b7eba0997cf52588,ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition,2026,"Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri","Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",Unknown,
c1645c9c8dde15f56448595fb85d4025d4ea489f,FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data,2026,"Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu","The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.",Unknown,
951f9c895610d0ff79b398fe89b43929ca8864da,TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction,2026,"Mihai Nadǎş, Laura Dioşan, Andreea Tomescu, Andrei Piscoran","Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.",Unknown,
cc6b39ccf1607371d317ed391dcf24da4af9d313,Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models,2026,"Zirui Ren, Ziming Liu","Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b)""Grokking""dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM""guesses""the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be""guessing""instead of""reasoning"". Leveraging this""guessing""picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models""reason"".",Unknown,
7003b69885435f737669e86871e0552cbb02fd7d,Meta Dynamic Graph for Traffic Flow Prediction,2026,"Yiqing Zou, Hanning Yuan, Qianyu Yang, Ziqiang Yuan, Shuliang Wang, Sijie Ruan","Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.",Unknown,
574f0053cee537d040a58ab363b5c1f5c817bd6a,DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction,2026,Zhancun Mu,"We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.",Unknown,
3718cfdb2012713c6981fcd81ba1b31221e21cb6,Unlabeled Data Can Provably Enhance In-Context Learning of Transformers,2026,"Renpu Liu, Jing Yang","Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.",Unknown,
974b85e18917b698a2d713ff2992927e3ddb5397,In-Context Source and Channel Coding,2026,"Ziqiong Wang, Tianqi Ren, Rongpeng Li, Zhifeng Zhao, Honggang Zhang","Separate Source-Channel Coding (SSCC) remains attractive for text transmission due to its modularity and compatibility with mature entropy coders and powerful channel codes. However, SSCC often suffers from a pronounced cliff effect in low Signal-to-Noise Ratio (SNR) regimes, where residual bit errors after channel decoding can catastrophically break lossless source decoding, especially for Arithmetic Coding (AC) driven by Large Language Models (LLMs). This paper proposes a receiver-side In-Context Decoding (ICD) framework that enhances SSCC robustness without modifying the transmitter. ICD leverages an Error Correction Code Transformer (ECCT) to obtain bit-wise reliability for the decoded information bits. Based on the context-consistent bitstream, ICD constructs a confidence-ranked candidate pool via reliability-guided bit flipping, samples a compact yet diverse subset of candidates, and applies an LLM-based arithmetic decoder to obtain both reconstructions and sequence-level log-likelihoods. A reliability-likelihood fusion rule then selects the final output. We further provide theoretical guarantees on the stability and convergence of the proposed sampling procedure. Extensive experiments over Additive White Gaussian Noise (AWGN) and Rayleigh fading channels demonstrate consistent gains compared with conventional SSCC baselines and representative Joint Source-Channel Coding (JSCC) schemes.",Unknown,
997aec55a175f67fcceef568d5a640819d4c7f1f,Enhancing Visual In-Context Learning by Multi-Faceted Fusion,2026,"Wenwen Liao, Jianbo Yu, Yuansong Wang, Qingchao Jiang, Xiaofeng Yang","Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant""retrieve-then-prompt""approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.",Unknown,
ea409c2672136d2fbfd1892e9c8c132a7dc8708e,Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL,2026,"Wenwen Liao, Jianbo Yu, Yuansong Wang, Shifu Yan, Xiaofeng Yang","Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements. We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.",Unknown,
c9260cbcc99e166ddfb83d148f03476ac869035a,SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery,2026,"Chong Liu, Luxuan Fu, Yang Jia, Zhen Dong, Bisheng Yang","The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.",Unknown,
e998beb80be4d806684caaf1de79a62ab505c35d,On the origin of neural scaling laws: from random graphs to natural language,2026,"M. Barkeshli, Alberto Alfarano, Andrey Gromov","Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd\""os-Renyi and scale-free Barab\'asi-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.",Unknown,
95e83e49e47aed17b4e1402c6c729d726d6a05f8,Generative AI collective behavior needs an interactionist paradigm,2026,"Laura Ferrarotti, G. Campedelli, Roberto Dessì, Andrea Baronchelli, Giovanni Iacca, K. Carley, A. Pentland, Joel Z. Leibo, James Evans, Bruno Lepri","In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",Unknown,
fe6b9f648f6f4767328625dd8f86d9bdcda32928,Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD,2026,"Murat Bilgehan Ertan, Marten van Dijk","Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $\kappa$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $\kappa$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $\sigma$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy $\sigma \ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $\kappa \ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4\pi\ln M}}\right)$, and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.",Unknown,
32e4bc510ff857e03979ac23da2cfba82502eb78,CtD: Composition through Decomposition in Emergent Communication,2026,"Boaz Carmeli, Ron Meir, Yonatan Belinkov","Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed""Composition through Decomposition"", involves two sequential training steps. In the'Decompose'step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the'Compose'step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose'step is achieved zero-shot, without the need for additional training.",International Conference on Learning Representations,939c6e1d-0d17-4d6e-8a82-66d960df0e40
2c613ede62a995af17a9f0af634976e2e3c6a3de,XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs,2026,"Xinyang Chen, Huidong Jin, Yu Huang, Zaiwen Feng","Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.",Unknown,
8fd7d92aabebb37a3f0a31931d6203e3748e2958,Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?,2026,"David Reid, Ognjen Arandjelovíc","Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.",Unknown,
d809dc5bfcc23bd296154871f7ef46d359c172d8,DPTMS: data-physics coupling thermo-mechanical simulation method of quenching process,2026,"Yongjia Xu, Ze Zhao, Jinhui Yan",,Engineering computations,47ba6266-85d0-494a-a55a-927f8c599119
08077c508b3457edbab44bb763fe0320e1753ac2,Video Joint-Embedding Predictive Architectures for Facial Expression Recognition,2026,"Lennart Eing, Cristina Luna-Jim'enez, Silvan Mertes, Elisabeth Andr'e","This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.",Unknown,
973f1c76700aea17c06b6e8ed20ad5aa882b7919,Exploring Reliable Spatiotemporal Dependencies for Efficient Visual Tracking,2026,"Junze Shi, Yang Yu, Jian Shi, Haibo Luo","Recent advances in transformer-based lightweight object tracking have established new standards across benchmarks, leveraging the global receptive field and powerful feature extraction capabilities of attention mechanisms. Despite these achievements, existing methods universally employ sparse sampling during training--utilizing only one template and one search image per sequence--which fails to comprehensively explore spatiotemporal information in videos. This limitation constrains performance and cause the gap between lightweight and high-performance trackers. To bridge this divide while maintaining real-time efficiency, we propose STDTrack, a framework that pioneers the integration of reliable spatiotemporal dependencies into lightweight trackers. Our approach implements dense video sampling to maximize spatiotemporal information utilization. We introduce a temporally propagating spatiotemporal token to guide per-frame feature extraction. To ensure comprehensive target state representation, we disign the Multi-frame Information Fusion Module (MFIFM), which augments current dependencies using historical context. The MFIFM operates on features stored in our constructed Spatiotemporal Token Maintainer (STM), where a quality-based update mechanism ensures information reliability. Considering the scale variation among tracking targets, we develop a multi-scale prediction head to dynamically adapt to objects of different sizes. Extensive experiments demonstrate state-of-the-art results across six benchmarks. Notably, on GOT-10k, STDTrack rivals certain high-performance non-real-time trackers (e.g., MixFormer) while operating at 192 FPS(GPU) and 41 FPS(CPU).",Unknown,
4a47cf546e57562ec04f0477749dee9672350d01,Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment,2026,"Chen-Wei Liang, Bin Guo, Zhen-Yuan Wei, Mujiangshan Wang","Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\% cross-jurisdictional performance retention versus 76.2\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.",Unknown,
d5afe9f9175902ee180aa628d1cb1a3d03e7842e,"Lean Clients, Full Accuracy: Hybrid Zeroth- and First-Order Split Federated Learning",2026,"Zhoubin Kou, Zihan Chen, Jing Yang, Cong Shen","Split Federated Learning (SFL) enables collaborative training between resource-constrained edge devices and a compute-rich server. Communication overhead is a central issue in SFL and can be mitigated with auxiliary networks. Yet, the fundamental client-side computation challenge remains, as back-propagation requires substantial memory and computation costs, severely limiting the scale of models that edge devices can support. To enable more resource-efficient client computation and reduce the client-server communication, we propose HERON-SFL, a novel hybrid optimization framework that integrates zeroth-order (ZO) optimization for local client training while retaining first-order (FO) optimization on the server. With the assistance of auxiliary networks, ZO updates enable clients to approximate local gradients using perturbed forward-only evaluations per step, eliminating memory-intensive activation caching and avoiding explicit gradient computation in the traditional training process. Leveraging the low effective rank assumption, we theoretically prove that HERON-SFL's convergence rate is independent of model dimensionality, addressing a key scalability concern common to ZO algorithms. Empirically, on ResNet training and language model (LM) fine-tuning tasks, HERON-SFL matches benchmark accuracy while reducing client peak memory by up to 64% and client-side compute cost by up to 33% per step, substantially expanding the range of models that can be trained or adapted on resource-limited devices.",Unknown,
ed55b9c0683079ad66de57d5bc7083dabc534459,Towards Realistic Synthetic Data for Automatic Drum Transcription,2026,"Pierfrancesco Melucci, P. Merialdo, Taketo Akama","Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",Unknown,
5c1fdfa9d15b4d1892a5338ce4c0412608dc90f2,Parallelizable memory recurrent units,2026,"Florent De Geeter, Gaspard Lambrechts, Damien Ernst, G. Drion","With the emergence of massively parallel processing units, parallelization has become a desirable property for new sequence models. The ability to parallelize the processing of sequences with respect to the sequence length during training is one of the main factors behind the uprising of the Transformer architecture. However, Transformers lack efficiency at sequence generation, as they need to reprocess all past timesteps at every generation step. Recently, state-space models (SSMs) emerged as a more efficient alternative. These new kinds of recurrent neural networks (RNNs) keep the efficient update of the RNNs while gaining parallelization by getting rid of nonlinear dynamics (or recurrence). SSMs can reach state-of-the art performance through the efficient training of potentially very large networks, but still suffer from limited representation capabilities. In particular, SSMs cannot exhibit persistent memory, or the capacity of retaining information for an infinite duration, because of their monostability. In this paper, we introduce a new family of RNNs, the memory recurrent units (MRUs), that combine the persistent memory capabilities of nonlinear RNNs with the parallelizable computations of SSMs. These units leverage multistability as a source of persistent memory, while getting rid of transient dynamics for efficient computations. We then derive a specific implementation as proof-of-concept: the bistable memory recurrent unit (BMRU). This new RNN is compatible with the parallel scan algorithm. We show that BMRU achieves good results in tasks with long-term dependencies, and can be combined with state-space models to create hybrid networks that are parallelizable and have transient dynamics as well as persistent memory.",Unknown,
3b464ea37058a7ff3a87edd40805adf96f4f8805,Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models,2026,"Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou","In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.",Unknown,
d423d7b53134999aceb09f1f40e7e21e90235b2b,FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks,2026,"Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang Hou, Enyan Dai, Feng Xia","Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.",Unknown,
2e58081437d608dc01445f190236a8a155411f15,LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models,2026,"Haoyan Gong, Hongbin Liu","Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing""restoration-then-recognition""two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.",Unknown,
173c44e3bf71f278e888591d1423ef33a6e20148,Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability,2026,"Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang","Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs. We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model. Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.",Unknown,
c401b487e7edb8d84c0c7b776edd0c2ed64d7a74,WiFo-E: A Scalable Wireless Foundation Model for End-to-End FDD Precoding in Communication Networks,2026,"Weibo Wen, Shijian Gao, Haotian Zhang, Xiang Cheng, Liuqing Yang","Accurate precoding in massive multiple-input multiple-output (MIMO) frequency-division duplexing (FDD) systems relies on efficient channel state information (CSI) acquisition. End-to-end learning frameworks improve performance by jointly optimizing this process, but they lack scalability and fail to generalize across different system configurations, such as varying numbers of antennas and users. To overcome this limitation, we introduce WiFo-E, a wireless foundation model designed for scalable end-to-end precoding. WiFo-E employs multi-task pretraining on a diverse set of configurations to learn transferable representations of underlying wireless principles. Central to the model is a sparse Mixture-of-Experts (MoE) Transformer architecture, which mitigates task interference and enhances training efficiency by activating specialized parameter subsets adaptively. Extensive simulations demonstrate that WiFo-E outperforms conventional per-configuration training and shows strong generalization to unseen system configurations, providing a flexible and efficient foundation for adaptive massive MIMO precoding.",Unknown,
2f284647acdf09ce8ed09e6aa56146ec9c240b58,GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis,2026,"Manning Gao, Leheng Zhang, Shiqin Han, Haifeng Hu, Yuncheng Jiang, Sijie Mai","Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.",Unknown,
f743ec74984074b9f1d221aa249f823161db1b9f,TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding,2026,"Xiangqian Wang, Yifan Jia, Yang Xiang, Yumin Zhang, Yanbin Wang, Ke Liu","Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.",Unknown,
8922d39260c2f6d76346ebf08d5b533f78aa7b56,Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting,2026,"Tianye Li, Qi Liu, Hao Li, Lei Chen, Wencong Cheng, Fei Zheng, Xiangao Xia, Ya Wang, Gang Huang, Weiwei Wang, Xuan Tong, Z. Zu, Yi Fang, Shenming Fu, Jiang Jiang, Haochen Li, Mingxin Li, Jiangjiang Xia","Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.",Unknown,
eaeab4d5c08c3a5de571796785eb8f45bffebe9a,Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy,2026,"Qiang Hu, Qimei Wang, Yingjie Guo, Qiang Li, Zhiwei Wang","White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.",Unknown,
04abdcdd066d707b7ce828642c8907488d183364,Point Tracking as a Temporal Cue for Robust Myocardial Segmentation in Echocardiography Videos,2026,"B. Khodabakhshian, Nima Hashemi, Armin Saadat, Zahra Gholami, In-Chang Hwang, Samira Sojoudi, Christina Luong, P. Abolmaesumi, Teresa Tsang","Purpose: Myocardium segmentation in echocardiography videos is a challenging task due to low contrast, noise, and anatomical variability. Traditional deep learning models either process frames independently, ignoring temporal information, or rely on memory-based feature propagation, which accumulates error over time. Methods: We propose Point-Seg, a transformer-based segmentation framework that integrates point tracking as a temporal cue to ensure stable and consistent segmentation of myocardium across frames. Our method leverages a point-tracking module trained on a synthetic echocardiography dataset to track key anatomical landmarks across video sequences. These tracked trajectories provide an explicit motion-aware signal that guides segmentation, reducing drift and eliminating the need for memory-based feature accumulation. Additionally, we incorporate a temporal smoothing loss to further enhance temporal consistency across frames. Results: We evaluate our approach on both public and private echocardiography datasets. Experimental results demonstrate that Point-Seg has statistically similar accuracy in terms of Dice to state-of-the-art segmentation models in high quality echo data, while it achieves better segmentation accuracy in lower quality echo with improved temporal stability. Furthermore, Point-Seg has the key advantage of pixel-level myocardium motion information as opposed to other segmentation methods. Such information is essential in the computation of other downstream tasks such as myocardial strain measurement and regional wall motion abnormality detection. Conclusion: Point-Seg demonstrates that point tracking can serve as an effective temporal cue for consistent video segmentation, offering a reliable and generalizable approach for myocardium segmentation in echocardiography videos. The code is available at https://github.com/DeepRCL/PointSeg.",Unknown,
913be6872c219fc5db6aeb2362881929a002e0d8,Understanding or Memorizing? A Case Study of German Definite Articles in Language Models,2026,"Jonathan Drechsel, Erisa Bytyqi, Steffen Herbold","Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.",Unknown,
00170d59d70b5b9840e3efbb07a1e9ecf3f25f89,Energy-Entropy Regularization: The True Power of Minimal Looped Transformers,2026,Wai-Lun Lam,"Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.",Unknown,
3b7578c41adada6b7b4f0f3cad26aecc0ae2a992,"Function Calling in Large Language Models: Industrial Practices, Challenges, and Future Directions",2026,"Maolin Wang, Yingyi Zhang, Bowen Yu, Bingguang Hao, Cunyin Peng, Yicheng Chen, Wei Zhou, Jinjie Gu, Chenyi Zhuang, Ruocheng Guo, Wanyu Wang, Xiangyu Zhao","The swift evolution of Large Language Models (LLMs) like the GPT family, LLaMA, ChatGLM, and Qwen represents significant progress in artificial intelligence research. Despite their remarkable capabilities in generating content, these models encounter substantial challenges when producing structured outputs and engaging in dynamic interactions, particularly when they need to retrieve external information in real time. To address these limitations, researchers have developed the ”Function Calling” paradigm. This approach enables language models to analyze user inquiries and engage with defined functions, thereby facilitating precise responses through connections to external sources, including databases, programming interfaces, and live data streams. This functionality has been successfully implemented across numerous sectors such as finance analytics, healthcare systems, and service operations. The implementation of function calling comprises three essential phases: preparation, execution, and processing. The preparation phase encompasses query analysis and function identification. During execution, the system evaluates whether a function is necessary, extracts relevant parameters, and oversees the operation. The processing phase concentrates on analyzing outcomes and crafting appropriate responses. Each phase presents unique difficulties, ranging from accurately selecting functions to managing complex parameter extraction and ensuring reliable execution. Researchers have established various evaluation frameworks and metrics to assess function calling performance, including success rates, computational efficiency, parameter extraction accuracy, and response quality indicators such as ROUGE-L evaluation scores. This survey systematically reviews the current landscape of function calling in LLMs, analyzing technical challenges, examining existing solutions, and discussing evaluation methodologies. We particularly focus on practical implementations and industrial applications, providing insights into both current achievements and future directions in this rapidly evolving field. For a comprehensive collection of related research papers and the Appendix file, please refer to our repository at GitHub.",ACM Computing Surveys,7b2adce0-d53f-49d6-8784-b0645604fe62
3c58483bc544cef1e4e80e94f5027b580be2cc3f,RaptScore: a large language model-based algorithm for versatile aptamer evaluation,2026,"Akira Kimura-Yamazaki, Tatsuo Adachi, Shigetaka Nakamura, Yoshikazu Nakamura, M. Hamada","Abstract RNA aptamers are a high-potency tool in the life sciences, offering promising applications in drug discovery and beyond. They are typically obtained through systematic evolution of ligands by exponential enrichment (SELEX), which imposes constraints on sequence length and diversity. Several metrics, such as frequency and enrichment, have been developed to identify high-activity aptamers from SELEX. However, existing evaluation metrics are limited to sequences that appear within SELEX and cannot assess sequences of varying lengths, limiting their utility in optimizing aptamer design. To overcome these limitations, we developed RaptScore, a novel binding activity evaluation metric leveraging large language models. RaptScore enables the assessment of arbitrary sequences, including those absent from SELEX, and accommodates variations in sequence length. RaptScore exhibited a strong correlation with binding activity, allowing the identification of shorter aptamers with enhanced binding properties. By integrating RaptScore with in silico maturation, we achieved a 10-nucleotide truncation while maintaining binding efficiency. Furthermore, we demonstrated improved aptamer discovery efficiency by combining RaptScore with RaptGen, a variational autoencoder-based aptamer discovery tool. By enabling efficient sequence evaluation and optimization, RaptScore provides a powerful tool for aptamer research, facilitating the discovery of high-activity candidates while reducing experimental effort.",Nucleic Acids Research,da96c220-cd39-4894-bae6-d2d77c8e7aa9
967f0a64f4222ba4b381b0cbf199b25a4d2012c7,The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit,2026,"Faruk Alpay, Bilge Senturk","We prove that the Transformer self-attention mechanism in the high-confidence regime ($\beta \to \infty$, where $\beta$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.",Unknown,
287e1f2ec9dd45b75e75c896ac6f37dcd033e66e,LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities,2026,"Yongjian Tang, Thomas Runkler","Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.",Unknown,
78de9a82b137e4a12912caecc99040a76f7cb844,TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models,2026,"Khalid Oublal, Quentin Bouniot, Qi Gan, Stephan Cl'emenccon, Zeynep Akata","As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.",Unknown,
4d2b90739ed6918eeb1635835491f23a183a8e4b,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,2026,"Phuong Minh Nguyen, Dang Huu-Tien, Naoya Inoue","Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.",Unknown,
be3a4518693f8254478cdc68e77ca0254109f0d0,Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction,2026,"Kai Zhang, Zhengzhong Yi, Shaojun Guo, Linghang Kong, Situ Wang, Xiaoyu Zhan, Tan He, Weiping Lin, Tao Jiang, Dongxin Gao, Yiming Zhang, Fangming Liu, Fang Zhang, Zhengfeng Ji, Fusheng Chen, Jianxin Chen","Fast, reliable decoders are pivotal components for enabling fault-tolerant quantum computation (FTQC). Neural network decoders like AlphaQubit have demonstrated potential, achieving higher accuracy than traditional human-designed decoding algorithms. However, existing implementations of neural network decoders lack the parallelism required to decode the syndrome stream generated by a superconducting logical qubit in real time. Moreover, integrating AlphaQubit with sliding window-based parallel decoding schemes presents non-trivial challenges: AlphaQubit is trained solely to output a single bit corresponding to the global logical correction for an entire memory experiment, rather than local physical corrections that can be easily integrated. We address this issue by training a recurrent, transformer-based neural network specifically tailored for parallel window decoding. While it still outputs a single bit, we derive training labels from a consistent set of local corrections and train on various types of decoding windows simultaneously. This approach enables the network to self-coordinate across neighboring windows, facilitating high-accuracy parallel decoding of arbitrarily long memory experiments. As a result, we overcome the throughput bottleneck that previously precluded the use of AlphaQubit-type decoders in FTQC. Our work presents the first scalable, neural-network-based parallel decoding framework that simultaneously achieves SOTA accuracy and the stringent throughput required for real-time quantum error correction. Using an end-to-end experimental workflow, we benchmark our decoder on the Zuchongzhi 3.2 superconducting quantum processor on surface codes with distances up to 7, demonstrating its superior accuracy. Moreover, we demonstrate that, using our approach, a single TPU v6e is capable of decoding surface codes with distances up to 25 within 1us per decoding round.",Unknown,
b317d7193dfb5d1000ac3057406839713c115894,Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP,2026,"Anant Mehta, Xiyuan Wei, Xingyu Chen, Tianbao Yang","CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.",Unknown,
10feb5b4a6c48ffaddc708ee7c59fd0085c9120a,Deep learning-enabled hybrid systems for accurate recognition of text in seal images,2026,"Keke Zhang, Mingyu Guan, Chao Wu, Yutong Li, Qingguo Lü, Yi Liu, Yi Wang, Wei Wang, Wei Zhang","Chinese seals are widely used in various fields within Chinese society as a tool for certifying legal documents. However, recognizing text on these seals presents challenges due to background text, high noise levels, and minimalistic image features. This paper introduces a hybrid model to address these difficulties in Chinese seal text recognition. Our model integrates preprocessing techniques tailored for real seals, a deep learning-based position correction model, a circular text unwrapping model, and OCR text recognition. First, we apply a color-based method to effectively remove the black background text on seals, eliminating redundant information while retaining crucial features for further analysis. Next, we introduce an innovative image denoising algorithm to significantly improve the system's robustness in processing noisy seal images. Additionally, we develop a deep learning-based angle prediction network and create synthetic datasets that mimic real seal scenes, enabling optimal seal image positioning for enhanced text flattening and recognition, thus boosting overall system performance. Finally, polar coordinate transformation is employed to convert the circular seal into a rectangular image for more efficient text recognition. Experimental results indicate that our proposed methods effectively enhance the accuracy of seal text recognition.",Frontiers in Big Data,165fa1b5-e07f-4b6e-9203-04493f6a7c5c
fa5b6b4988d4bb7894bf474177dab29475dcef94,"Toward real-time emotion recognition in fog computing-based systems: leveraging interpretable PCA_CNN, YOLO with self-attention mechanism",2026,"Nora El Rashidy, Eman Allogmani, Esraa Hassan, Khaled Alnowaiser, Hela Elmannai, Zainab H. Ali","Emotion estimation from face expression analysis has been extensively examined in computer science. In contrast, classifying expressions depends on appropriate facial features and their dynamics. Despite the promising accuracy results in handled and favorable conditions, processing faces acquired at a distance, entailing low-quality images, still needs an influential performance reduction. The primary objective of this study is to introduce a Real-Time Emotion Recognition system-based Fog Technique, which was developed to track and observe human emotional states in real time. This paper provides a comprehensive integration of PCA-based feature selection with a specific version of YOLO (YOLOv8), in addition to spatial attention for real-time recognition. The developed system demonstrates superiority in edge deployment capabilities compared to existing approaches. The proposed model is compared with the CNN_PCA hybrid model. First, Principal Component Analysis (PCA) is employed as a dimension-reduction tool, focusing on the most informative characteristics during training, and then CNN as classification layer. The proposed system's performance is assessed via a dataset of 35,888 facial photos classified into seven classes: anger, fear, happiness, neutral, sadness, surprise, and disgust. The constructed model surpasses established pre-trained models, such as VGG, ResNet, and MobileNet, with different evaluation metrics. First, the PCA_CNN model achieved superior accuracy, precision, recall, and Area Under the Curve (AUC) scores of 0.936, 0.971, 0.843, 0.871, and 0.943.YOLO v8 aith attention model achieved 0.986, 0.902, 0.941, and 0.952. Additionally, the model exhibits significantly faster processing time, completing computations in just 610 seconds than other pre-trained models. To validate the model's superiority, extensive testing on additional datasets consistently yields promising performance results, further validating the efficiency and effectiveness of our developed model in real-time emotion recognition for advancing affective computing applications.",Frontiers of Computer Science,
d314868b1b0fc7caaaa1e0f7732373732284212c,Adaptive Machine Learning Framework for Real-time Optical Coherence Tomography Artefact Correction in Retinal Detachment Surgery: A Hybrid Convolutional Neural Network–Long Short-term Memory–Transformer Approach with Unsupervised Domain Adaptation,2026,"Nora Mubarak Alghareeb, Mohammed Youssif Al-Nasser, Jana Hasan Alzahrani, S. A. Alghamdi, Eyad Ghurmullah Alzahrani, Linda Saad Alghamdi, S. A. Alnassri, Shahad Abdulrahman Alzahrani, H. A. Morfeq","Intraoperative optical coherence tomography (OCT) provides high-resolution cross-sectional imaging during retinal detachment surgery, but motion artefacts, speckle noise and dynamic retinal changes often limit its use. These artefacts degrade image quality and complicate surgical decision-making, emphasizing the need for reliable real-time correction to improve outcomes.        We developed a hybrid adaptive machine-learning framework for real-time OCT artefact correction. The system integrates convolutional neural networks (CNNs) for spatial analysis, long short-term memory (LSTM) networks for temporal sequence consistency and a lightweight transformer module for efficient feature prioritization. To overcome the lack of labelled intraoperative datasets, unsupervised domain adaptation transfers knowledge from pre-operative to intraoperative conditions. The framework was implemented with FPGA acceleration, achieving sub-200 ms latency suitable for surgical workflows.          The proposed method outperformed baseline approaches, achieving a peak signal-to-noise ratio of 32.7 dB and a structural similarity index of 0.921. It preserved retinal layer continuity and achieved higher surgical relevance scores compared with CNN-LSTM and transformer-based methods. In a prospective clinical study of 20 retinal detachment surgeries, its use reduced unplanned manoeuvres by 23%, shortened procedure times by 18% and achieved higher reattachment rates (94% vs. 82%,  P  < 0.05). Surgeons also reported greater confidence in identifying residual traction and vitreous remnants.          This hybrid adaptive framework provides robust real-time OCT artefact correction, enhances intraoperative visualization and improves anatomical and functional outcomes in retinal detachment surgery. Its modular and adaptive design supports broader applications in ophthalmic surgery and other intraoperative imaging modalities.",Journal of Advanced Trends in Medical Research,
bd79187679ff7ea1cb41410f89c9586de8006f62,Artificial intelligence in aortic CT angiography: current applications and future perspectives,2026,"Jingkai Xu, Jinjin Liu, Guoquan Cao","Artificial intelligence (AI) is revolutionizing cardiovascular imaging, with aortic computed tomography angiography (CTA) emerging as a prominent area of application. CTA imaging is essential for the diagnosis, risk stratification, and treatment planning of aortic diseases. However, conventional CTA techniques face limitations such as radiation exposure, contrast agent risks, and reliance on manual interpretation. The integration of AI into aortic CTA offers innovative solutions across multiple domains. AI can enhance image quality, automate anatomical segmentation, improve diagnostic accuracy for aortic emergencies, and provide quantitative tools for prognostic evaluation following interventions like endovascular aortic repair. Furthermore, this review provides the analysis of emerging techniques, including advanced image synthesis methods, Vision Transformer architectures, multi-task learning, weakly supervised learning, and the paradigm shift introduced by Foundation Models, emphasizing their potential for clinical application. This work comprehensively summarizes the current applications and nascent technological paradigms of AI in aortic CTA, along with existing challenges and future research directions.",Frontiers in Cardiovascular Medicine,3bc0e661-dc2a-454a-9ff5-6515430ce9ff
81eb60229958f80b76070287d5dafb5000ca75a1,LGSTA-GNN: A Local-Global Spatiotemporal Attention Graph Neural Network for Bridge Structural Damage Detection,2026,"Die Liu, Jianxi Yang, Jianming Li, Jingyuan Shen, Youjia Zhang, Lihua Chen, Lei Zhou","Accurate detection of structural damage is essential for ensuring the safety and reliability of bridges. However, traditional vibration-based approaches often struggle to capture rich feature representations and adequately model spatial dependencies among sensors. This study proposes a novel bridge damage detection framework, LGSTA-GNN, which integrates local–global spatiotemporal learning with graph neural networks. The framework first extracts multi-scale temporal–frequency features using a multi-scale feature extraction module. A local graph feature extraction module then models intrinsic spatial relationships through graph convolutions, while a global graph attention module adaptively captures inter-sensor dependencies by emphasizing structurally informative nodes. A benchmark dataset generated from a scaled bridge model under progressive damage states is used to evaluate the proposed method. Extensive experiments demonstrate that LGSTA-GNN outperforms multiple graph neural network variants and conventional deep learning techniques, achieving superior accuracy, precision, recall, and F1-score. The confusion matrix and t-SNE visualization further verify its enhanced discriminative capability and robustness. Ablation studies confirm the contribution of each module, highlighting the effectiveness of global attention in identifying subtle structural deterioration. Overall, LGSTA-GNN provides an effective and interpretable solution for intelligent bridge damage detection, with strong potential for practical structural health monitoring and real-time safety assessment.",Buildings,f2c78b1b-6bcc-4700-859b-af9401c02941
0ae85883d96d0150bc3c002f6756acd4f76c49be,Epilepsy detection based on spatiotemporal feature interaction fusion of EEG signals,2026,"Zhencai Xu, Hongfeng Ge, Weiwei Huang, Hongwei Lu","In recent years, with the development of machine learning and deep learning technologies, an increasing number of research works have begun using these technologies for automatic seizure detection in EEG signals. However, existing automatic seizure detection algorithms primarily focus on the features of individual EEG channels and pay less attention to the inter-channel relationships. This results in insufficient extraction of spatiotemporal information from multi-channel EEG data, affecting the final seizure detection performance.        Therefore, this paper proposes an automatic seizure detection method based on the combination of Graph Attention Networks (GAT) and Transformer networks. Specifically, GAT is used as the front end for extracting spatial features, fully leveraging the topological structure of different EEG channels. Meanwhile, the Transformer network is used as the back end to explore temporal relationships and make final decisions based on the states before and after the current moment.        Experiments were conducted on the CHB-MIT and TUH datasets with ten-fold cross-validation. The final seizure detection accuracies on the two datasets were 98.62 and 98.12%, respectively, with the model’s performance surpassing or being comparable to current state-of-the-art models.        The proposed hybrid algorithm combines the advantages of two deep learning models, fully exploring the spatiotemporal correlations between EEG channels. Experiments on public datasets demonstrate the effectiveness of this method, significantly advancing the development of automatic seizure detection.",Frontiers in Neurology,3513ce18-3757-4358-b15f-7060af98e6d0
56a0d3ced3188fe0dcae844b5858c7f619f3edcd,Unifying phylogenetic traversal and deep learning to guide tree exploration,2026,"Lena Collienne, Harry Richman, David H. Rich, Mary Barker, Chris Jennings-Shaffer, F. Matsen",,bioRxiv,027ffd21-ebb0-4af8-baf5-911124292fd0
103331a471a3f9f01ecec22fc4908fe4713dc783,NEuRT: A Transformer-Based Model for Explainable Neuronal Activity Analysis,2026,"Georgii Raev, Daniil Baev, Evgenii Gerasimov, V. Chukanov, E. Pchitskaya",,bioRxiv,027ffd21-ebb0-4af8-baf5-911124292fd0
1a4b2b79343ce5c546f8fc06c4c629a26e310c36,A quantum-inspired attention integrated scalar long short-term memory model for accurate and stable groundwater contaminant source inversion,2026,"Liuzhi Zhu, Wenxi Lu",,Environmental Monitoring & Assessment,2ed4b0ca-e2ee-4190-bd44-39b9ac8ab3fd
99038cc0527ef4d4e7f727e60f275c1cb2c3bf19,Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition,2026,"Zheng Zhou, Isabella McEvoy, Camilo E. Valderrama","Subject-independent EEG emotion recognition is challenged by pronounced inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. To address this, we propose a fusion framework that integrates (i) local, channel-wise descriptors and (ii) global, trial-level descriptors, improving cross-subject generalization on the SEED-VII dataset. Local representations are formed per channel by concatenating differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics at the trial level. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, with samples filtered by an intensity threshold. Experiments under a leave-one-subject-out protocol demonstrate that the proposed method consistently outperforms single-view and classical baselines, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition. The code has been released at https://github.com/Danielz-z/LGF-EEG-Emotion.",Unknown,
5f83545bfc09217b87ea6b95bd28861e1411e57e,LLMs in Code Vulnerability Analysis: A Proof of Concept,2026,"Shaznin Sultana, Sadia Afreen, Nasir U. Eisty","Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.",Unknown,
5a6f2e574289e4bd1754d7bd1534bbb2d7e4ee5e,Markovian Pre-Trained Transformer for Next-Item Recommendation,2026,"Cong Xu, Guoliang Li, Jun Wang, Wei Zhang","We introduce the Markovian Pre-trained Transformer (MPT) for next-item recommendation, a transferable model fully pre-trained on synthetic Markov chains, yet capable of achieving state-of-the-art performance by fine-tuning a lightweight adaptor. This counterintuitive success stems from the observation of the `Markovian'nature: advanced sequential recommenders coincidentally rely on the latest interaction to make predictions, while the historical interactions serve mainly as auxiliary cues for inferring the user's general, non-sequential identity. This characteristic necessitates the capabilities of a universal recommendation model to effectively summarize the user sequence, with particular emphasis on the latest interaction. MPT inherently has the potential to be universal and transferable. On the one hand, when trained to predict the next state of Markov chains, it acquires the capabilities to estimate transition probabilities from the context (one adaptive manner for summarizing sequences) and attend to the last state to ensure accurate state transitions. On the other hand, unlike the heterogeneous interaction data, an unlimited amount of controllable Markov chains is available to boost the model capacity. We conduct extensive experiments on five public datasets from three distinct platforms to validate the superiority of Markovian pre-training over traditional recommendation pre-training and recent language pre-training paradigms.",Unknown,
958e7c3d38c3b8b63ed3119c1c04e2ed6cedc886,Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation,2026,"Yizhan Feng, H. Snoussi, Yuhang Wang, Jing Teng, A. Cherouat, Tian Wang","With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs",Unknown,
e14a8e99c4e2e6abb432ca85f8d4de78e71f0ba0,Second-order Gaussian directional derivative representations for image high-resolution corner detection,2026,"Dongbo Xie, Junjie Qiu, Changming Sun, Weichuan Zhang","Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction. Our research indicates that there are theoretical flaws in Zhang et al.'s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other. In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e. END-type and L-type models). Then, the SOGDD representations of these two corner models were derived separately, and many characteristics of high-resolution corners were discovered, which enabled us to demonstrate how to select Gaussian filtering scales to obtain intensity variation information from images, accurately depicting adjacent corners. In addition, a new high-resolution corner detection method for images has been proposed for the first time, which can accurately detect adjacent corner points. The experimental results have verified that the proposed method outperforms state-of-the-art methods in terms of localization error, robustness to image blur transformation, image matching, and 3D reconstruction.",Unknown,
72d2f8ffebec663243392ea26229b56c5900df79,AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding,2026,"Chong Zhang, Victor Klemm, Fan Yang, Marco Hutter","Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.",Unknown,
35d77f8933fcc35e0e6dab8ebdf2387ce9350dcb,Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering,2026,"Lavanya Prahallad, Sai Utkarsh Choudarypally, Pragna Prahallad, Pranathi Prahallad","Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.",Unknown,
1cb062c631bcf175bbd72b477c80cf1199c84bfc,Hyperbolic Heterogeneous Graph Transformer,2026,"Jongmin Park, Seunghoon Han, Hyewon Lee, Won-Yong Shin, Sungsu Lim","In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.",Unknown,
9637859da727d030cb60034398f38c5442997210,An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3,2026,"Daesuk Kwon, Won-Gi Paeng","General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.",Unknown,
043b2c57a6f7926af7a3b6ff4da9b18a905b13cd,Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs,2026,Manideep Reddy Chinthareddy,"Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal. Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",Unknown,
cba47deb39330fa61c5eb0ba80ef9616e79dec9c,Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures,2026,"Sucheta Ghosh, Zahra Monfared, Felix Dietrich","We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.",Unknown,
3d63c92444e07519969b798c99d7ea824061d8d3,Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints,2026,"Seng Pei Liew, K. Shinzato, Yuyang Dong","Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$). Moreover, $n_{exp}$ and $n_{topk}$ do not""cancel out""within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.",Unknown,
581c3ffe5decf99a582832f68220b109252024ec,Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables,2026,"Valérie Zermatten, Chiara Vanalli, Gencer Sumbul, Diego Marcos, D. Tuia","Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.",Unknown,
7f88e8b9e960e31aedaf6091efc3b2a244ca8e3c,ReCo-KD: Region- and Context-Aware Knowledge Distillation for Efficient 3D Medical Image Segmentation,2026,"Qizhen Lan, Yu-Chun Hsu, Nida Saddaf Khan, Xiaoqian Jiang","Accurate 3D medical image segmentation is vital for diagnosis and treatment planning, but state-of-the-art models are often too large for clinics with limited computing resources. Lightweight architectures typically suffer significant performance loss. To address these deployment and speed constraints, we propose Region- and Context-aware Knowledge Distillation (ReCo-KD), a training-only framework that transfers both fine-grained anatomical detail and long-range contextual information from a high-capacity teacher to a compact student network. The framework integrates Multi-Scale Structure-Aware Region Distillation (MS-SARD), which applies class-aware masks and scale-normalized weighting to emphasize small but clinically important regions, and Multi-Scale Context Alignment (MS-CA), which aligns teacher-student affinity patterns across feature levels. Implemented on nnU-Net in a backbone-agnostic manner, ReCo-KD requires no custom student design and is easily adapted to other architectures. Experiments on multiple public 3D medical segmentation datasets and a challenging aggregated dataset show that the distilled lightweight model attains accuracy close to the teacher while markedly reducing parameters and inference latency, underscoring its practicality for clinical deployment.",Unknown,
7066f1b14cf32bb7f268e275bf6d3e1f9ae96f42,ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms,2026,"Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed M. Saifullah, Ali Jannesari","Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.",Unknown,
6aa88dc32e65da7e530d8ba274fac6e13edfce9c,Ministral 3,2026,"Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault, Abhinav Rastogi, Adrien Sad'e, Alan Jeffares, Albert Q. Jiang, Alexandre Cahill, Alexandre Gavaudan, Alexandre Sablayrolles, Am'elie H'eliou, Amos You, Andy Ehrenberg, Andy Lo, Anton Eliseev, Antonia Calvi, Avinash Sooriyarachchi, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Clémence Lanfranchi, Corentin Barreau, Cyprien Courtot, Daniele Grattarola, Darius Dabert, Diego de Las Casas, Elliot Chane-Sane, Faruk Ahmed, Gabrielle Berrada, Gaetan Ecrepont, Gauthier Guinet, Georgii Sergeevich Novikov, Guillaume Kunsch, Guillaume Lample, Guillaume Martin, Gunshi Gupta, Jan Ludziejewski, Jason Rute, J. Studnia, Jonas Amar, Joséphine Delas, Josselin Somerville Roberts, Karmesh Yadav, K. Chandu, Kush Jain, Laurence Aitchison, Laurent Fainsin, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, M. Buyl, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poir'ee, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mia Chiquier, Michel Schimpf, Nathan Grinsztajn, Neha Gupta, N. Raghuraman, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Patrick von Platen, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Quentin Torroba, Romain Sauvestre, Roman Soletskyi, Rupert Menneer, S. Vaze, Samuel Barry, Sanchit Gandhi, Siddhant Waghjale, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Théo Cachet, Theo Simon Sorg, Thibaut Lavril, Thiziri Nait Saada, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Edwards, Umar Jamil, Umberto Tomasini, Valeriia Nemychnikova, Van Phung, V. Maladiere, Virgile Richard, Wassim Bouaziz, Wen-Ding Li, William Marshall, Xinghui Li, Xinyu Yang, Yassine El Ouahidi, Yihan Wang, Yunhao Tang, Zaccharie Ramzi","We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",Unknown,
cec411773a4d5135ccd5ee0f182df3b32e0548e7,Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots,2026,"Francesco Dettori, Matteo Forasassi, Lorenzo Veronese, Livia Lestingi, Vincenzo Scotti, Matteo G. Rossi","Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.",Unknown,
3b44e2463c46cdb06617d944f8c47db675e90eba,MobiDiary: Autoregressive Action Captioning with Wearable Devices and Wireless Signals,2026,"Fei Deng, Yinghui He, Chuntong Chu, Ge Wang, Han Ding, Jinsong Han, Fei Wang","Human Activity Recognition (HAR) in smart homes is critical for health monitoring and assistive living. While vision-based systems are common, they face privacy concerns and environmental limitations (e.g., occlusion). In this work, we present MobiDiary, a framework that generates natural language descriptions of daily activities directly from heterogeneous physical signals (specifically IMU and Wi-Fi). Unlike conventional approaches that restrict outputs to pre-defined labels, MobiDiary produces expressive, human-readable summaries. To bridge the semantic gap between continuous, noisy physical signals and discrete linguistic descriptions, we propose a unified sensor encoder. Instead of relying on modality-specific engineering, we exploit the shared inductive biases of motion-induced signals--where both inertial and wireless data reflect underlying kinematic dynamics. Specifically, our encoder utilizes a patch-based mechanism to capture local temporal correlations and integrates heterogeneous placement embedding to unify spatial contexts across different sensors. These unified signal tokens are then fed into a Transformer-based decoder, which employs an autoregressive mechanism to generate coherent action descriptions word-by-word. We comprehensively evaluate our approach on multiple public benchmarks (XRF V2, UWash, and WiFiTAD). Experimental results demonstrate that MobiDiary effectively generalizes across modalities, achieving state-of-the-art performance on captioning metrics (e.g., BLEU@4, CIDEr, RMC) and outperforming specialized baselines in continuous action understanding.",Unknown,
f51fb3bfef9464123009a92929afd42a52e95d26,Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training,2026,"Muhammad Taimoor Hassan, Jawad Ahmed, Muhammad Awais","Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.",Unknown,
201b768dceeab35686652eaae47233f1e75982bd,Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation,2026,"Runfeng Qu, Ole Hall, Pia Bideau, Julie Ouerfelli-Ethier, Martin Rolfs, Klaus Obermayer, Olaf Hellwich","Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision",Unknown,
c237a467412511c057a7b70f39bc7477f4f0c129,REVNET: Rotation-Equivariant Point Cloud Completion via Vector Neuron Anchor Transformer,2026,"Zhifan Ni, Eckehard Steinbach","Incomplete point clouds captured by 3D sensors often result in the loss of both geometric and semantic information. Most existing point cloud completion methods are built on rotation-variant frameworks trained with data in canonical poses, limiting their applicability in real-world scenarios. While data augmentation with random rotations can partially mitigate this issue, it significantly increases the learning burden and still fails to guarantee robust performance under arbitrary poses. To address this challenge, we propose the Rotation-Equivariant Anchor Transformer (REVNET), a novel framework built upon the Vector Neuron (VN) network for robust point cloud completion under arbitrary rotations. To preserve local details, we represent partial point clouds as sets of equivariant anchors and design a VN Missing Anchor Transformer to predict the positions and features of missing anchors. Furthermore, we extend VN networks with a rotation-equivariant bias formulation and a ZCA-based layer normalization to improve feature expressiveness. Leveraging the flexible conversion between equivariant and invariant VN features, our model can generate point coordinates with greater stability. Experimental results show that our method outperforms state-of-the-art approaches on the synthetic MVP dataset in the equivariant setting. On the real-world KITTI dataset, REVNET delivers competitive results compared to non-equivariant networks, without requiring input pose alignment. The source code will be released on GitHub under URL: https://github.com/nizhf/REVNET.",Unknown,
0e24130327af7c1a476d17e4432b3a69755e83b1,DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion,2026,"Chenxu Han, Sean Bin Yang, Jilin Hu","Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.",Unknown,
f806c4d4a2bfdc7e51134b767afec1209006fa18,Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style,2026,"Mengqi Wu, Yongheng Sun, Qianqian Wang, Pew-Thian Yap, Mingxia Liu","Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.",Unknown,
5f81efd7727d2d2cde427efdb55ace64b71a47cc,Baiting AI: Deceptive Adversary Against AI-Protected Industrial Infrastructures,2026,"A. Pasikhani, P. Gope, Yang Yang, Shagufta Mehnaz, Biplab Sikdar","This paper explores a new cyber-attack vector targeting Industrial Control Systems (ICS), particularly focusing on water treatment facilities. Developing a new multi-agent Deep Reinforcement Learning (DRL) approach, adversaries craft stealthy, strategically timed, wear-out attacks designed to subtly degrade product quality and reduce the lifespan of field actuators. This sophisticated method leverages DRL methodology not only to execute precise and detrimental impacts on targeted infrastructure but also to evade detection by contemporary AI-driven defence systems. By developing and implementing tailored policies, the attackers ensure their hostile actions blend seamlessly with normal operational patterns, circumventing integrated security measures. Our research reveals the robustness of this attack strategy, shedding light on the potential for DRL models to be manipulated for adversarial purposes. Our research has been validated through testing and analysis in an industry-level setup. For reproducibility and further study, all related materials, including datasets and documentation, are publicly accessible.",IEEE Transactions on Dependable and Secure Computing,d286fdd0-3b6c-433c-afee-87228d8e9f93
39eeb99b4bae455c20d1cd94dbc1ad6acf494ec8,Attention Projection Mixing and Exogenous Anchors,2026,Jonathan Su,"Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. Through a unified normalized mixing framework (studying different coefficient granularities: elementwise, headwise, scalar) across all attention pathways (queries, keys, values, and gate logits), ExoFormer variants consistently outperform their internal-anchor counterparts. Moreover, the dynamic variant achieves a 2.13-point increase in downstream accuracy over the baseline and demonstrates superior data efficiency, matching baseline validation loss with 1.84x fewer tokens. ExoFormer also achieves a 2x reduction in attention sink compared to standard Gated Attention. Paradoxically, all ExoFormer variants exhibit signs of representation collapse. We explain this via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in computational refinement. We release codes and models to facilitate future research.",Unknown,
3c3eb094c96706c183c8c5c02daf888bd7e33283,Efficient Maintenance of Leiden Communities in Large Dynamic Graphs,2026,"Chunxu Lin, Yumao Xie, Yixiang Fang, Yongmin Hu, Yingqian Hu, Chen Cheng","As a well-known community detection algorithm, Leiden has been widely used in various scenarios such as large language model generation (e.g., Graph-RAG), anomaly detection, and biological analysis. In these scenarios, the graphs are often large and dynamic, where vertices and edges are inserted and deleted frequently, so it is costly to obtain the updated communities by Leiden from scratch when the graph has changed. Recently, one work has attempted to study how to maintain Leiden communities in the dynamic graph, but it lacks a detailed theoretical analysis, and its algorithms are inefficient for large graphs. To address these issues, in this paper, we first theoretically show that the existing algorithms are relatively unbounded via the boundedness analysis (a powerful tool for analyzing incremental algorithms on dynamic graphs), and also analyze the memberships of vertices in communities when the graph changes. Based on theoretical analysis, we develop a novel efficient maintenance algorithm, called Hierarchical Incremental Tree Leiden (HIT-Leiden), which effectively reduces the range of affected vertices by maintaining the connected components and hierarchical community structures. Comprehensive experiments in various datasets demonstrate the superior performance of HIT-Leiden. In particular, it achieves speedups of up to five orders of magnitude over existing methods.",Unknown,
4beb72c18e050a6d48cc1880f63f36f39925d1a7,Single-shot matrix-matrix photonic processor based on spatial-spectral hypermultiplexed parallel diffraction,2026,"Chao Luan, Ronald Davis III, Zaijun Chen, D. Englund, Ryan Hamerly","The ever-increasing data demand craves advancements in high-speed and energy-efficient computing hardware. Analog optical neural network (ONN) processors have emerged as a promising solution, offering benefits in bandwidth and energy consumption. However, existing ONN processors exhibit limited computational parallelism, and while certain architectures achieve high parallelism, they encounter serious scaling up roadblocks for large-scale implementation. Here, we introduce a spatial-wavelength-temporal hyper-multiplexed ONN processor, which is based on parallel diffractive beam routing. The architecture supports high three-dimensional data, high O(N3) computing parallelism, and is feasible for large-scale implementation. A 16 × 16 parallel diffractive beam routing is demonstrated, enabling a large-scale (16 × 16 − by − 16 × 16), high-parallelism (4096 multiply-and-accumulates/shot (MACs/shot)), high-speed (2 Gsa/s), single-shot matrix-matrix multiplication (MMM) optical tensor processor. It accelerates convolutional neural networks (CNNs) and deep neural networks (DNNs) through parallel matrix multiplication. We demonstrate benchmark image recognition using a CNN and a subsequently fully connected DNN in the optical domain. The network works with an ultra-low optical energy of ≈ 20 attojoules (aJ)/MAC at 96.4% classification accuracy. The ONN system supports broad spectral and spatial bandwidths and is capable for large-scale scaling up, paving the way for highly efficient large-scale optical computing for next-generation deep learning. Optical neural network processors offering benefits in bandwidth and energy consumption but problems in scaling and parallelism. The author presenting a novel optical tensor processor capable of optically performing large-scale, high-speed matrix-matrix multiplication in a single step.",Nature Communications,43b3f0f9-489a-4566-8164-02fafde3cd98
eb8f5eb330f99c5f27d215a31164f8e0f8986fa1,Recent approaches for aspect based sentiment analysis: a review,2026,"G. Chauhan, Ravi Nahta, Abhishek Upadhyay, Y. Meena, Dinesh Gopalani, Akash Saxena",,Cluster Computing,f1d0ef3d-4e90-41e9-b454-f589a933654f
87d1e49c2507c423067688d3b70a757ad9a2f96f,Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models,2026,Andrew Kiruluta,"We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure. Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.",Unknown,
68827568c466ef7096bf3b1b4b94c3edb58aee65,Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers,2026,"Annalisa Belloni, Lorenzo Noci, Antonio Orvieto","The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.",Unknown,
ed5ec649c323cf1856b8013c685d2aef873c9c0c,Layer-Parallel Training for Transformers,2026,"Shuai Jiang, Marc Salvado, Eric C. Cyr, Alena Kopanivc'akov'a, Rolf Krause, Jacob B. Schroder","We present a new training methodology for transformers using a multilevel, layer-parallel approach. Through a neural ODE formulation of transformers, our application of a multilevel parallel-in-time algorithm for the forward and backpropagation phases of training achieves parallel acceleration over the layer dimension. This dramatically enhances parallel scalability as the network depth increases, which is particularly useful for increasingly large foundational models. However, achieving this introduces errors that cause systematic bias in the gradients, which in turn reduces convergence when closer to the minima. We develop an algorithm to detect this critical transition and either switch to serial training or systematically increase the accuracy of layer-parallel training. Results, including BERT, GPT2, ViT, and machine translation architectures, demonstrate parallel-acceleration as well as accuracy commensurate with serial pre-training while fine-tuning is unaffected.",Unknown,
